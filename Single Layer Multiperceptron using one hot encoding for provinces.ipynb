{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "train_df = pd.read_csv('data/features.csv')\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.drop(columns=['precio'])\n",
    "train_y = train_df.precio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 164176 samples, validate on 41044 samples\n",
      "Epoch 1/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 6647014399558.5059 - mae: 1753711.7500 - val_loss: 3404545369212.8438 - val_mae: 1228316.7500\n",
      "Epoch 2/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 3053293452547.1685 - mae: 1260207.7500 - val_loss: 2907511004624.3477 - val_mae: 1242380.0000\n",
      "Epoch 3/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 2724394484722.7275 - mae: 1200425.2500 - val_loss: 2596275457161.4180 - val_mae: 1156639.1250\n",
      "Epoch 4/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 2433930482474.7373 - mae: 1114338.1250 - val_loss: 2322893232634.3613 - val_mae: 1067823.0000\n",
      "Epoch 5/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 2194025620256.7578 - mae: 1031623.7500 - val_loss: 2113710577793.5344 - val_mae: 991992.6250\n",
      "Epoch 6/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 2025712526125.5315 - mae: 970170.5000 - val_loss: 1978397805946.5237 - val_mae: 945876.5000\n",
      "Epoch 7/1000\n",
      "164176/164176 [==============================] - 26s 155us/step - loss: 1920692654889.1406 - mae: 936174.5000 - val_loss: 1895853160212.6826 - val_mae: 927607.6875\n",
      "Epoch 8/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1855234409262.3301 - mae: 918187.5000 - val_loss: 1842054447619.7424 - val_mae: 912213.6250\n",
      "Epoch 9/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1810532145580.1221 - mae: 906705.6250 - val_loss: 1804316852049.9570 - val_mae: 900174.3125\n",
      "Epoch 10/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1778568949720.4810 - mae: 898047.8125 - val_loss: 1776568533257.0564 - val_mae: 892072.2500\n",
      "Epoch 11/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1754247516625.8447 - mae: 890958.9375 - val_loss: 1754650324781.6313 - val_mae: 885956.1875\n",
      "Epoch 12/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1734444040378.8169 - mae: 884372.2500 - val_loss: 1736955210786.4294 - val_mae: 881633.4375\n",
      "Epoch 13/1000\n",
      "164176/164176 [==============================] - 26s 155us/step - loss: 1718393484602.4551 - mae: 879264.7500 - val_loss: 1722504476960.4084 - val_mae: 871874.6875\n",
      "Epoch 14/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1704945259420.1047 - mae: 873779.1875 - val_loss: 1709657183884.1626 - val_mae: 871350.1250\n",
      "Epoch 15/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1693013554484.0681 - mae: 869597.2500 - val_loss: 1698462944775.0356 - val_mae: 862191.1250\n",
      "Epoch 16/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1681927161502.1257 - mae: 863891.2500 - val_loss: 1687271919152.5505 - val_mae: 861666.8125\n",
      "Epoch 17/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1670252557034.0701 - mae: 857872.0000 - val_loss: 1674350511170.5635 - val_mae: 851414.9375\n",
      "Epoch 18/1000\n",
      "164176/164176 [==============================] - 26s 155us/step - loss: 1656873020426.4785 - mae: 849276.9375 - val_loss: 1660179728716.7178 - val_mae: 841149.8750\n",
      "Epoch 19/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1643022840419.9451 - mae: 839935.5625 - val_loss: 1646628034606.5046 - val_mae: 830258.5000\n",
      "Epoch 20/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1630515428235.9380 - mae: 831658.0000 - val_loss: 1635091840720.6221 - val_mae: 823703.4375\n",
      "Epoch 21/1000\n",
      "164176/164176 [==============================] - 26s 155us/step - loss: 1620208692031.6943 - mae: 825271.2500 - val_loss: 1625431374923.5452 - val_mae: 817964.3750\n",
      "Epoch 22/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1611500696740.3628 - mae: 820049.9375 - val_loss: 1617487506309.9502 - val_mae: 813856.1250\n",
      "Epoch 23/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1604306564875.1023 - mae: 815674.7500 - val_loss: 1610972319251.8093 - val_mae: 809991.0625\n",
      "Epoch 24/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1598328382221.5972 - mae: 811846.0625 - val_loss: 1605529479424.8733 - val_mae: 807867.5000\n",
      "Epoch 25/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1593263896581.7881 - mae: 809010.8125 - val_loss: 1601083936018.7366 - val_mae: 801871.0625\n",
      "Epoch 26/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1588995760737.1509 - mae: 805936.0625 - val_loss: 1597241860899.9512 - val_mae: 799480.9375\n",
      "Epoch 27/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1585334689708.1719 - mae: 803346.6875 - val_loss: 1593826929508.4189 - val_mae: 799244.6875\n",
      "Epoch 28/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1582201852922.3115 - mae: 801102.1250 - val_loss: 1591619349923.2402 - val_mae: 800254.0000\n",
      "Epoch 29/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1579517896860.2795 - mae: 799375.3125 - val_loss: 1588729624297.5710 - val_mae: 795803.0625\n",
      "Epoch 30/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1577107159705.2358 - mae: 797662.9375 - val_loss: 1586876933776.5535 - val_mae: 790648.3750\n",
      "Epoch 31/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1574991891922.3435 - mae: 795682.6250 - val_loss: 1584705419442.2344 - val_mae: 790805.2500\n",
      "Epoch 32/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1573126034170.8354 - mae: 794314.1250 - val_loss: 1582938220374.5476 - val_mae: 789806.6875\n",
      "Epoch 33/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1571479146661.6604 - mae: 793147.1250 - val_loss: 1581691240658.7678 - val_mae: 787290.5625\n",
      "Epoch 34/1000\n",
      "164176/164176 [==============================] - 26s 158us/step - loss: 1569895370138.4583 - mae: 791683.7500 - val_loss: 1580501223191.4768 - val_mae: 790726.8750\n",
      "Epoch 35/1000\n",
      "164176/164176 [==============================] - 26s 158us/step - loss: 1568580933609.8455 - mae: 791019.9375 - val_loss: 1579123204304.3726 - val_mae: 785681.1250\n",
      "Epoch 36/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1567307977958.6270 - mae: 789757.4375 - val_loss: 1577949361103.9985 - val_mae: 786587.5625\n",
      "Epoch 37/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1566100303798.9497 - mae: 788966.4375 - val_loss: 1576966842735.0474 - val_mae: 783692.8750\n",
      "Epoch 38/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1564965968389.5386 - mae: 787681.5625 - val_loss: 1576584945800.5200 - val_mae: 788729.3750\n",
      "Epoch 39/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1563988831927.4739 - mae: 787414.6875 - val_loss: 1574957803615.6040 - val_mae: 784529.6250\n",
      "Epoch 40/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1562968014645.2158 - mae: 786517.7500 - val_loss: 1574164227010.0271 - val_mae: 783745.8750\n",
      "Epoch 41/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1562006256887.9915 - mae: 785561.6250 - val_loss: 1573937514967.6328 - val_mae: 786489.5625\n",
      "Epoch 42/1000\n",
      "164176/164176 [==============================] - 26s 161us/step - loss: 1561136113100.1562 - mae: 785473.7500 - val_loss: 1572963811059.7500 - val_mae: 778521.3125\n",
      "Epoch 43/1000\n",
      "164176/164176 [==============================] - 26s 160us/step - loss: 1560307314755.4617 - mae: 784100.3125 - val_loss: 1572380895088.0952 - val_mae: 785121.1250\n",
      "Epoch 44/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1559498388131.8140 - mae: 784232.1875 - val_loss: 1571368583029.0850 - val_mae: 777813.8125\n",
      "Epoch 45/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1558641814578.3967 - mae: 783244.1875 - val_loss: 1570650832763.4717 - val_mae: 777900.4375\n",
      "Epoch 46/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1558026641747.3042 - mae: 782681.6250 - val_loss: 1569582336846.8633 - val_mae: 779781.5625\n",
      "Epoch 47/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1557291468431.7551 - mae: 782231.0000 - val_loss: 1568951925605.6167 - val_mae: 779651.8750\n",
      "Epoch 48/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1556627342501.5605 - mae: 781680.8125 - val_loss: 1568197379242.3506 - val_mae: 778452.4375\n",
      "Epoch 49/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1555997937430.4788 - mae: 781148.7500 - val_loss: 1567727153192.3174 - val_mae: 779603.4375\n",
      "Epoch 50/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1555356410080.3398 - mae: 780836.6250 - val_loss: 1567126297626.8450 - val_mae: 778297.6875\n",
      "Epoch 51/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1554712230831.0662 - mae: 780257.7500 - val_loss: 1566855396073.0720 - val_mae: 779601.0625\n",
      "Epoch 52/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1554236139510.2200 - mae: 780128.5000 - val_loss: 1565958896530.3250 - val_mae: 776186.7500\n",
      "Epoch 53/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1553681231208.1614 - mae: 779404.1875 - val_loss: 1565468546170.0498 - val_mae: 776043.5625\n",
      "Epoch 54/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1553018809261.8684 - mae: 779164.8125 - val_loss: 1565016312840.0835 - val_mae: 774398.7500\n",
      "Epoch 55/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1552605700141.5068 - mae: 778684.3125 - val_loss: 1564818810156.4839 - val_mae: 773312.0000\n",
      "Epoch 56/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1552175246424.7180 - mae: 778420.6875 - val_loss: 1564068922329.1797 - val_mae: 773557.8750\n",
      "Epoch 57/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1551621154065.3396 - mae: 777878.8750 - val_loss: 1563950170477.6501 - val_mae: 777901.8125\n",
      "Epoch 58/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1551276668328.6292 - mae: 777670.6250 - val_loss: 1563225656387.1624 - val_mae: 776581.0625\n",
      "Epoch 59/1000\n",
      "164176/164176 [==============================] - 25s 149us/step - loss: 1550805920994.0364 - mae: 777637.0625 - val_loss: 1562690399847.6375 - val_mae: 773646.2500\n",
      "Epoch 60/1000\n",
      "164176/164176 [==============================] - 24s 149us/step - loss: 1550411826605.0203 - mae: 777098.0625 - val_loss: 1562282553058.5852 - val_mae: 774328.0625\n",
      "Epoch 61/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1549951927641.9905 - mae: 776968.9375 - val_loss: 1561823548494.9382 - val_mae: 773593.1250\n",
      "Epoch 62/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1549483301514.0669 - mae: 776640.6250 - val_loss: 1561879411602.3250 - val_mae: 770790.6250\n",
      "Epoch 63/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1549172187805.8264 - mae: 776106.3125 - val_loss: 1561194984901.3701 - val_mae: 773611.9375\n",
      "Epoch 64/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1548821213232.3010 - mae: 776194.8750 - val_loss: 1560746753386.9556 - val_mae: 772956.4375\n",
      "Epoch 65/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1548324420206.1243 - mae: 775775.6250 - val_loss: 1560531424866.6477 - val_mae: 774589.7500\n",
      "Epoch 66/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1548047102734.7947 - mae: 775722.7500 - val_loss: 1559900704550.4460 - val_mae: 771572.6875\n",
      "Epoch 67/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1547570260790.1140 - mae: 775366.5625 - val_loss: 1559542076945.2146 - val_mae: 772866.4375\n",
      "Epoch 68/1000\n",
      "164176/164176 [==============================] - 27s 166us/step - loss: 1547212177888.6143 - mae: 775343.5000 - val_loss: 1559932978753.5156 - val_mae: 768469.3750\n",
      "Epoch 69/1000\n",
      "164176/164176 [==============================] - 27s 163us/step - loss: 1546897713710.6543 - mae: 774833.7500 - val_loss: 1558877047380.1775 - val_mae: 771579.1875\n",
      "Epoch 70/1000\n",
      "164176/164176 [==============================] - 27s 162us/step - loss: 1546502373558.0266 - mae: 774694.8750 - val_loss: 1558486357961.9109 - val_mae: 771764.2500\n",
      "Epoch 71/1000\n",
      "164176/164176 [==============================] - 26s 159us/step - loss: 1546170630897.6545 - mae: 774560.0625 - val_loss: 1558227374309.5293 - val_mae: 771436.5625\n",
      "Epoch 72/1000\n",
      "164176/164176 [==============================] - 26s 160us/step - loss: 1545788916456.8723 - mae: 774348.9375 - val_loss: 1557818878170.5518 - val_mae: 771463.8750\n",
      "Epoch 73/1000\n",
      "164176/164176 [==============================] - 27s 162us/step - loss: 1545458067003.9270 - mae: 774186.6875 - val_loss: 1557442091939.0906 - val_mae: 770432.7500\n",
      "Epoch 74/1000\n",
      "164176/164176 [==============================] - 27s 166us/step - loss: 1545045563115.9661 - mae: 774105.2500 - val_loss: 1557386144524.5991 - val_mae: 769091.1875\n",
      "Epoch 75/1000\n",
      "164176/164176 [==============================] - 27s 163us/step - loss: 1544718623335.5376 - mae: 773902.8750 - val_loss: 1557276617754.4458 - val_mae: 767797.1875\n",
      "Epoch 76/1000\n",
      "164176/164176 [==============================] - 27s 162us/step - loss: 1544450805748.4236 - mae: 773411.0625 - val_loss: 1556640878955.6543 - val_mae: 772281.6875\n",
      "Epoch 77/1000\n",
      "164176/164176 [==============================] - 27s 164us/step - loss: 1544059265061.5232 - mae: 773741.6875 - val_loss: 1556417613355.7603 - val_mae: 768373.7500\n",
      "Epoch 78/1000\n",
      "164176/164176 [==============================] - 27s 162us/step - loss: 1543698085445.1084 - mae: 773196.0625 - val_loss: 1555899238279.2476 - val_mae: 770774.0000\n",
      "Epoch 79/1000\n",
      "164176/164176 [==============================] - 27s 162us/step - loss: 1543434657735.3164 - mae: 773314.7500 - val_loss: 1555561866890.1667 - val_mae: 769215.6875\n",
      "Epoch 80/1000\n",
      "164176/164176 [==============================] - 27s 162us/step - loss: 1542994103637.7991 - mae: 772833.9375 - val_loss: 1555398623133.0029 - val_mae: 771571.3750\n",
      "Epoch 81/1000\n",
      "164176/164176 [==============================] - 27s 163us/step - loss: 1542741971646.1599 - mae: 772919.8750 - val_loss: 1555252689578.7998 - val_mae: 771535.3125\n",
      "Epoch 82/1000\n",
      "164176/164176 [==============================] - 27s 162us/step - loss: 1542517199417.6318 - mae: 772943.8750 - val_loss: 1554803781065.5615 - val_mae: 767660.1875\n",
      "Epoch 83/1000\n",
      "164176/164176 [==============================] - 26s 159us/step - loss: 1542289567955.1670 - mae: 772518.6875 - val_loss: 1554468156832.6455 - val_mae: 768019.5000\n",
      "Epoch 84/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1541959636403.0081 - mae: 772505.1250 - val_loss: 1554118993434.3958 - val_mae: 768201.5625\n",
      "Epoch 85/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1541585124298.2102 - mae: 772114.7500 - val_loss: 1554296866383.8862 - val_mae: 772481.3125\n",
      "Epoch 86/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1541364353620.1772 - mae: 772397.0000 - val_loss: 1553508044283.8584 - val_mae: 768860.5000\n",
      "Epoch 87/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1541133418184.6384 - mae: 772055.2500 - val_loss: 1553395558743.0964 - val_mae: 770530.1875\n",
      "Epoch 88/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1540884284472.9832 - mae: 772105.2500 - val_loss: 1552983518806.0735 - val_mae: 768538.1250\n",
      "Epoch 89/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1540590896681.5647 - mae: 771752.3750 - val_loss: 1552658241309.4646 - val_mae: 769573.1250\n",
      "Epoch 90/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1540157642250.8276 - mae: 771927.8750 - val_loss: 1552608614711.5610 - val_mae: 766171.1250\n",
      "Epoch 91/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1540043589814.6255 - mae: 771538.5000 - val_loss: 1552112324826.3523 - val_mae: 769917.4375\n",
      "Epoch 92/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1539803802417.3240 - mae: 771546.4375 - val_loss: 1551967745518.8850 - val_mae: 770039.7500\n",
      "Epoch 93/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1539536315489.6997 - mae: 771432.2500 - val_loss: 1551578426831.9485 - val_mae: 769219.6250\n",
      "Epoch 94/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1539194884215.3552 - mae: 771442.1250 - val_loss: 1551353148080.5879 - val_mae: 767525.0000\n",
      "Epoch 95/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1539012357227.8787 - mae: 771117.8750 - val_loss: 1551085063195.2441 - val_mae: 768933.9375\n",
      "Epoch 96/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1538697479167.5010 - mae: 771260.8125 - val_loss: 1550835836321.0447 - val_mae: 767421.5000\n",
      "Epoch 97/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1538465356496.3228 - mae: 771042.6250 - val_loss: 1550835303027.5132 - val_mae: 769784.8125\n",
      "Epoch 98/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1538237622204.7378 - mae: 771034.4375 - val_loss: 1550414066552.1785 - val_mae: 768612.3750\n",
      "Epoch 99/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1537926569718.1453 - mae: 770945.1250 - val_loss: 1550069282243.7734 - val_mae: 768193.6250\n",
      "Epoch 100/1000\n",
      "164176/164176 [==============================] - 25s 149us/step - loss: 1537602633592.2783 - mae: 770720.7500 - val_loss: 1549777477388.4993 - val_mae: 768538.3750\n",
      "Epoch 101/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1537476710428.3418 - mae: 770841.6875 - val_loss: 1549728178119.3162 - val_mae: 765933.6875\n",
      "Epoch 102/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1537172794627.5676 - mae: 770735.3750 - val_loss: 1549653196249.1296 - val_mae: 765100.4375\n",
      "Epoch 103/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1537016919474.2095 - mae: 770497.3750 - val_loss: 1549291555373.8560 - val_mae: 764875.6875\n",
      "Epoch 104/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1536690507162.4583 - mae: 770342.9375 - val_loss: 1548820039425.1228 - val_mae: 767012.1250\n",
      "Epoch 105/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1536482700464.5378 - mae: 770374.7500 - val_loss: 1548937285513.4431 - val_mae: 769655.6250\n",
      "Epoch 106/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1536311580178.8113 - mae: 770333.2500 - val_loss: 1548299463358.3596 - val_mae: 767504.5000\n",
      "Epoch 107/1000\n",
      "164176/164176 [==============================] - 25s 149us/step - loss: 1536063288334.2708 - mae: 770233.8125 - val_loss: 1548164092615.7405 - val_mae: 768080.1875\n",
      "Epoch 108/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1535825642599.2883 - mae: 770288.6875 - val_loss: 1547824273657.2888 - val_mae: 766603.9375\n",
      "Epoch 109/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1535593171487.4856 - mae: 770155.3125 - val_loss: 1547816118548.0339 - val_mae: 765421.5625\n",
      "Epoch 110/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1535258835259.5530 - mae: 769925.7500 - val_loss: 1548195427082.0044 - val_mae: 770550.7500\n",
      "Epoch 111/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1535149577917.6611 - mae: 770122.1250 - val_loss: 1547258931099.2068 - val_mae: 764851.0625\n",
      "Epoch 112/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1534984353464.0725 - mae: 769707.4375 - val_loss: 1546912987663.9172 - val_mae: 767693.0000\n",
      "Epoch 113/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1534798403296.7888 - mae: 769993.0000 - val_loss: 1546587032178.5151 - val_mae: 766160.5000\n",
      "Epoch 114/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1534524188636.6724 - mae: 769775.6875 - val_loss: 1546426486574.4297 - val_mae: 766548.0000\n",
      "Epoch 115/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1534361038339.0439 - mae: 769665.3750 - val_loss: 1546166654920.3142 - val_mae: 766948.3125\n",
      "Epoch 116/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1534025803022.1460 - mae: 769486.9375 - val_loss: 1546393723383.7668 - val_mae: 769467.3125\n",
      "Epoch 117/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1533842001113.2546 - mae: 769736.6875 - val_loss: 1545625485227.3735 - val_mae: 766052.6250\n",
      "Epoch 118/1000\n",
      "164176/164176 [==============================] - 28s 168us/step - loss: 1533682280799.0801 - mae: 769530.4375 - val_loss: 1545499675121.1804 - val_mae: 765178.0625\n",
      "Epoch 119/1000\n",
      "164176/164176 [==============================] - 27s 167us/step - loss: 1533459535770.0093 - mae: 769357.1875 - val_loss: 1545425479606.3511 - val_mae: 767843.1250\n",
      "Epoch 120/1000\n",
      "164176/164176 [==============================] - 27s 164us/step - loss: 1533168890632.4077 - mae: 769501.5000 - val_loss: 1545060251738.4146 - val_mae: 765397.3125\n",
      "Epoch 121/1000\n",
      "164176/164176 [==============================] - 27s 166us/step - loss: 1533015410382.0276 - mae: 769433.4375 - val_loss: 1545109249750.2107 - val_mae: 763442.8750\n",
      "Epoch 122/1000\n",
      "164176/164176 [==============================] - 28s 168us/step - loss: 1532803386323.3916 - mae: 769082.7500 - val_loss: 1544875152545.9678 - val_mae: 768319.3125\n",
      "Epoch 123/1000\n",
      "164176/164176 [==============================] - 28s 168us/step - loss: 1532605063871.3577 - mae: 769385.3125 - val_loss: 1544284704060.2515 - val_mae: 766032.5625\n",
      "Epoch 124/1000\n",
      "164176/164176 [==============================] - 27s 163us/step - loss: 1532425819497.5586 - mae: 769206.0000 - val_loss: 1544041771235.1343 - val_mae: 765238.5000\n",
      "Epoch 125/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1532184497238.3230 - mae: 769002.8750 - val_loss: 1544246560767.0020 - val_mae: 768020.4375\n",
      "Epoch 126/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1531859221264.9900 - mae: 769212.9375 - val_loss: 1544018990395.2534 - val_mae: 763205.0000\n",
      "Epoch 127/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1531799376630.0454 - mae: 768775.6875 - val_loss: 1544084955001.0767 - val_mae: 768919.1875\n",
      "Epoch 128/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1531593303081.8142 - mae: 769223.3750 - val_loss: 1543439901340.2295 - val_mae: 763660.1250\n",
      "Epoch 129/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1531376188074.7996 - mae: 768774.7500 - val_loss: 1543157600584.5264 - val_mae: 764081.0000\n",
      "Epoch 130/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1531159048066.2578 - mae: 768850.8750 - val_loss: 1542856718217.2437 - val_mae: 765121.8750\n",
      "Epoch 131/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1530995227591.0168 - mae: 768877.1250 - val_loss: 1542616040434.7273 - val_mae: 764797.1875\n",
      "Epoch 132/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1530799295733.9956 - mae: 768599.0625 - val_loss: 1542565798311.9307 - val_mae: 767206.6875\n",
      "Epoch 133/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1530579967764.4829 - mae: 768710.3125 - val_loss: 1542227254313.2156 - val_mae: 766523.3750\n",
      "Epoch 134/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1530369975085.5317 - mae: 768704.1250 - val_loss: 1542512458484.2490 - val_mae: 768246.3750\n",
      "Epoch 135/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1530165477918.8865 - mae: 768683.6250 - val_loss: 1541808363082.6968 - val_mae: 765292.1250\n",
      "Epoch 136/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1530021161239.2275 - mae: 768539.8125 - val_loss: 1541651452832.8950 - val_mae: 764073.8125\n",
      "Epoch 137/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1529871456778.3289 - mae: 768360.8125 - val_loss: 1541357552109.3882 - val_mae: 765875.7500\n",
      "Epoch 138/1000\n",
      "164176/164176 [==============================] - 25s 150us/step - loss: 1529634743776.9136 - mae: 768491.3125 - val_loss: 1541037076713.5210 - val_mae: 765456.5000\n",
      "Epoch 139/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1529320768299.1367 - mae: 768576.5000 - val_loss: 1541269231935.5447 - val_mae: 762128.6250\n",
      "Epoch 140/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1529255527457.0325 - mae: 768202.5000 - val_loss: 1540733375531.8101 - val_mae: 765129.9375\n",
      "Epoch 141/1000\n",
      "164176/164176 [==============================] - 26s 155us/step - loss: 1529031630620.4666 - mae: 768322.1250 - val_loss: 1540656854469.3701 - val_mae: 765427.0000\n",
      "Epoch 142/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1528686461897.2124 - mae: 768153.5000 - val_loss: 1540362741617.6919 - val_mae: 766200.2500\n",
      "Epoch 143/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1528706161101.6533 - mae: 768275.6875 - val_loss: 1540019396706.1987 - val_mae: 765220.0000\n",
      "Epoch 144/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1528437645627.3533 - mae: 768192.9375 - val_loss: 1539826211287.2336 - val_mae: 763454.8750\n",
      "Epoch 145/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1528263763309.9495 - mae: 767948.1875 - val_loss: 1540143164000.1528 - val_mae: 767856.7500\n",
      "Epoch 146/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1528098355135.5322 - mae: 768156.0625 - val_loss: 1539409599382.3167 - val_mae: 764858.5000\n",
      "Epoch 147/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1527930092517.3547 - mae: 768070.2500 - val_loss: 1539291885823.8752 - val_mae: 763193.8750\n",
      "Epoch 148/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1527758839882.2478 - mae: 767863.5000 - val_loss: 1539012144746.8308 - val_mae: 764112.1875\n",
      "Epoch 149/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1527502440379.9397 - mae: 768007.1875 - val_loss: 1538947520815.5774 - val_mae: 762883.5000\n",
      "Epoch 150/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1527322071984.8623 - mae: 767612.3125 - val_loss: 1539222867059.0640 - val_mae: 768317.2500\n",
      "Epoch 151/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1527218484586.3569 - mae: 768049.1250 - val_loss: 1538424667281.8010 - val_mae: 763702.8750\n",
      "Epoch 152/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1527014612600.0039 - mae: 767724.8125 - val_loss: 1538206121749.2812 - val_mae: 764656.1250\n",
      "Epoch 153/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1526818944630.6067 - mae: 767689.1875 - val_loss: 1538081841619.0422 - val_mae: 762807.3750\n",
      "Epoch 154/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1526556841636.1135 - mae: 767555.4375 - val_loss: 1537877891025.8945 - val_mae: 765675.1250\n",
      "Epoch 155/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1526341788728.1848 - mae: 767788.0000 - val_loss: 1537959644231.9524 - val_mae: 761480.2500\n",
      "Epoch 156/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1526299350000.7312 - mae: 767558.0625 - val_loss: 1537398252030.1538 - val_mae: 762642.0000\n",
      "Epoch 157/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1525968185255.9805 - mae: 767600.3125 - val_loss: 1537509259922.8489 - val_mae: 762131.4375\n",
      "Epoch 158/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1525819064008.6384 - mae: 767411.8750 - val_loss: 1537051092503.2024 - val_mae: 764899.5000\n",
      "Epoch 159/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1525729427641.2202 - mae: 767462.3750 - val_loss: 1537017514452.4395 - val_mae: 765122.6250\n",
      "Epoch 160/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1525523897714.1409 - mae: 767516.1875 - val_loss: 1536641146997.3594 - val_mae: 762879.5000\n",
      "Epoch 161/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1525320456361.4526 - mae: 767340.3750 - val_loss: 1536482383083.4172 - val_mae: 763407.6875\n",
      "Epoch 162/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1525047417338.3616 - mae: 767299.8750 - val_loss: 1536261462466.4761 - val_mae: 764946.0000\n",
      "Epoch 163/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1524969376851.1296 - mae: 767320.9375 - val_loss: 1536082656473.6536 - val_mae: 763842.5000\n",
      "Epoch 164/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1524806881236.4893 - mae: 767242.3125 - val_loss: 1535892742112.8638 - val_mae: 762407.1250\n",
      "Epoch 165/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1524617853888.6299 - mae: 767142.6250 - val_loss: 1535635912085.4685 - val_mae: 764356.8125\n",
      "Epoch 166/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1524383064567.7668 - mae: 767244.6875 - val_loss: 1535537565657.2793 - val_mae: 762157.6875\n",
      "Epoch 167/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1524266409593.6006 - mae: 767067.0625 - val_loss: 1535347724043.3018 - val_mae: 761653.1250\n",
      "Epoch 168/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1524071293797.4170 - mae: 767036.8125 - val_loss: 1535085826430.8149 - val_mae: 763888.3125\n",
      "Epoch 169/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1523813141080.9675 - mae: 766978.8125 - val_loss: 1534916170403.0156 - val_mae: 763928.3750\n",
      "Epoch 170/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1523718803305.5088 - mae: 766928.3750 - val_loss: 1534850330686.6714 - val_mae: 765378.5000\n",
      "Epoch 171/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1523559719439.7178 - mae: 766948.8125 - val_loss: 1534382790303.3232 - val_mae: 763503.0000\n",
      "Epoch 172/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1523334845882.9915 - mae: 766847.8750 - val_loss: 1534613731126.2139 - val_mae: 766158.7500\n",
      "Epoch 173/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1523172098927.7957 - mae: 766988.1875 - val_loss: 1534144655274.1760 - val_mae: 761782.6875\n",
      "Epoch 174/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1522967695675.7524 - mae: 766866.8125 - val_loss: 1534390882814.5530 - val_mae: 760155.1875\n",
      "Epoch 175/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1522816516971.0056 - mae: 766636.5000 - val_loss: 1533649497052.1736 - val_mae: 763393.6250\n",
      "Epoch 176/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1522587280668.5164 - mae: 766576.9375 - val_loss: 1533591128969.9421 - val_mae: 764645.2500\n",
      "Epoch 177/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1522504120242.7583 - mae: 766716.1875 - val_loss: 1533414426771.4976 - val_mae: 764296.3125\n",
      "Epoch 178/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1522336245780.5579 - mae: 766686.8125 - val_loss: 1533204285699.2683 - val_mae: 761982.1875\n",
      "Epoch 179/1000\n",
      "164176/164176 [==============================] - 26s 155us/step - loss: 1522122411424.8450 - mae: 766665.8750 - val_loss: 1533181772726.0518 - val_mae: 760610.6875\n",
      "Epoch 180/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1521947863090.8955 - mae: 766485.1875 - val_loss: 1532825742098.9861 - val_mae: 761643.1875\n",
      "Epoch 181/1000\n",
      "164176/164176 [==============================] - 26s 155us/step - loss: 1521748662126.8975 - mae: 766367.4375 - val_loss: 1532533405029.5667 - val_mae: 763299.3125\n",
      "Epoch 182/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1521536783788.6208 - mae: 766495.5000 - val_loss: 1532351414840.4343 - val_mae: 762879.0000\n",
      "Epoch 183/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1521347831141.9658 - mae: 766516.0625 - val_loss: 1532507435988.7886 - val_mae: 760336.4375\n",
      "Epoch 184/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1521199822627.0532 - mae: 766083.2500 - val_loss: 1532135004889.9031 - val_mae: 764489.3125\n",
      "Epoch 185/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1521073323137.8337 - mae: 766386.3750 - val_loss: 1531818502172.5415 - val_mae: 762732.8125\n",
      "Epoch 186/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1520856164125.0654 - mae: 766369.1875 - val_loss: 1531726586422.0391 - val_mae: 760954.0000\n",
      "Epoch 187/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1520756946820.4534 - mae: 766029.5625 - val_loss: 1531548349776.8093 - val_mae: 764101.0000\n",
      "Epoch 188/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1520469509731.0469 - mae: 766340.6875 - val_loss: 1531658361344.7485 - val_mae: 759627.0000\n",
      "Epoch 189/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1520371365748.4861 - mae: 766073.8750 - val_loss: 1531065703131.4998 - val_mae: 761460.2500\n",
      "Epoch 190/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1520219409764.1697 - mae: 765989.2500 - val_loss: 1530808476847.9392 - val_mae: 763331.6875\n",
      "Epoch 191/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1520084865176.8865 - mae: 765979.4375 - val_loss: 1530886825803.3704 - val_mae: 764764.5625\n",
      "Epoch 192/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1519874213420.7581 - mae: 766073.1875 - val_loss: 1530581753788.1392 - val_mae: 763719.7500\n",
      "Epoch 193/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1519585576845.6345 - mae: 765898.8750 - val_loss: 1530341257840.7188 - val_mae: 762677.5625\n",
      "Epoch 194/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1519499106791.2009 - mae: 766051.3125 - val_loss: 1530441199116.6240 - val_mae: 760073.6875\n",
      "Epoch 195/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1519400623756.7612 - mae: 765851.6250 - val_loss: 1529927470019.4243 - val_mae: 762461.3750\n",
      "Epoch 196/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1519099485252.3599 - mae: 765870.9375 - val_loss: 1529841389176.4031 - val_mae: 760738.0000\n",
      "Epoch 197/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1519022203230.4812 - mae: 765726.1875 - val_loss: 1529537824131.5054 - val_mae: 762459.4375\n",
      "Epoch 198/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1518791729454.5793 - mae: 765645.5000 - val_loss: 1529532144815.3403 - val_mae: 764088.8750\n",
      "Epoch 199/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1518741855744.2495 - mae: 765816.0000 - val_loss: 1529309015939.8545 - val_mae: 763315.6250\n",
      "Epoch 200/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1518506989397.3501 - mae: 765821.3125 - val_loss: 1529128990214.8359 - val_mae: 760575.7500\n",
      "Epoch 201/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1518310099591.2727 - mae: 765404.6250 - val_loss: 1529338357929.0532 - val_mae: 764974.8750\n",
      "Epoch 202/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1518082859754.4690 - mae: 765883.9375 - val_loss: 1529973792443.0664 - val_mae: 757173.3750\n",
      "Epoch 203/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1518087107254.9746 - mae: 765324.5000 - val_loss: 1528466822665.2310 - val_mae: 761680.4375\n",
      "Epoch 204/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1517796953250.9658 - mae: 765459.5000 - val_loss: 1528280732960.1091 - val_mae: 761857.5000\n",
      "Epoch 205/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1517703133440.4739 - mae: 765556.4375 - val_loss: 1528264390513.2927 - val_mae: 760517.0625\n",
      "Epoch 206/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1517478096490.8308 - mae: 765368.8750 - val_loss: 1528147987042.5479 - val_mae: 760409.6875\n",
      "Epoch 207/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1517152055625.3247 - mae: 765312.6875 - val_loss: 1527789775805.8357 - val_mae: 761181.6875\n",
      "Epoch 208/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1517164389266.1252 - mae: 765262.4375 - val_loss: 1527645561771.6729 - val_mae: 762575.0625\n",
      "Epoch 209/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1516891507581.1685 - mae: 765178.5000 - val_loss: 1527409422700.4526 - val_mae: 762298.1875\n",
      "Epoch 210/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1516772969678.0774 - mae: 765344.3125 - val_loss: 1527395285325.1169 - val_mae: 760951.3750\n",
      "Epoch 211/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1516597756215.3616 - mae: 765384.4375 - val_loss: 1528222535159.0684 - val_mae: 757266.5000\n",
      "Epoch 212/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1516458227794.4309 - mae: 764998.3125 - val_loss: 1526853823565.5410 - val_mae: 760413.3125\n",
      "Epoch 213/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1516324620740.4719 - mae: 765086.9375 - val_loss: 1526795052623.0879 - val_mae: 759891.9375\n",
      "Epoch 214/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1516092276718.4360 - mae: 764923.4375 - val_loss: 1526780365099.1865 - val_mae: 763531.2500\n",
      "Epoch 215/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1515934452480.6238 - mae: 765113.6250 - val_loss: 1526310023701.7056 - val_mae: 760900.1250\n",
      "Epoch 216/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1515687199596.6023 - mae: 764972.4375 - val_loss: 1526582073639.5938 - val_mae: 758693.6875\n",
      "Epoch 217/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1515518998484.2896 - mae: 764896.0625 - val_loss: 1526015788451.4399 - val_mae: 760470.5625\n",
      "Epoch 218/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1515393025769.1719 - mae: 764781.0000 - val_loss: 1526020523042.6289 - val_mae: 763168.0625\n",
      "Epoch 219/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1515220974975.7131 - mae: 764841.3125 - val_loss: 1525677542095.1252 - val_mae: 761982.5000\n",
      "Epoch 220/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1515050364546.7817 - mae: 764797.0625 - val_loss: 1525398827297.4065 - val_mae: 760526.0625\n",
      "Epoch 221/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1514827253363.0142 - mae: 764800.9375 - val_loss: 1525293008130.1707 - val_mae: 760789.6250\n",
      "Epoch 222/1000\n",
      "164176/164176 [==============================] - 26s 157us/step - loss: 1514680424470.5537 - mae: 764822.4375 - val_loss: 1525467613776.9839 - val_mae: 758413.1875\n",
      "Epoch 223/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1514548643454.1912 - mae: 764558.6875 - val_loss: 1525329075118.2676 - val_mae: 758256.0625\n",
      "Epoch 224/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1514379271597.2197 - mae: 764400.3125 - val_loss: 1524745507324.9563 - val_mae: 761079.1250\n",
      "Epoch 225/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1514249028692.1274 - mae: 764564.9375 - val_loss: 1524602139140.1416 - val_mae: 760531.2500\n",
      "Epoch 226/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1513960772672.5676 - mae: 764346.1250 - val_loss: 1524538682249.1438 - val_mae: 762250.2500\n",
      "Epoch 227/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1513842797447.5471 - mae: 764492.5625 - val_loss: 1524309402646.6536 - val_mae: 760953.6875\n",
      "Epoch 228/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1513621414150.6614 - mae: 764270.6250 - val_loss: 1524670291247.0784 - val_mae: 764376.3750\n",
      "Epoch 229/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1513493206899.6877 - mae: 764531.5625 - val_loss: 1523891628293.6633 - val_mae: 759756.7500\n",
      "Epoch 230/1000\n",
      "164176/164176 [==============================] - 26s 156us/step - loss: 1513318114887.1042 - mae: 764180.6875 - val_loss: 1523723728510.5903 - val_mae: 761367.8750\n",
      "Epoch 231/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1513085847932.6194 - mae: 764400.8125 - val_loss: 1523610648802.7351 - val_mae: 758615.8750\n",
      "Epoch 232/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1512925885369.6443 - mae: 764270.3750 - val_loss: 1523926129775.4714 - val_mae: 757363.2500\n",
      "Epoch 233/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1512732961280.3494 - mae: 763979.8125 - val_loss: 1523103840105.3091 - val_mae: 760007.0625\n",
      "Epoch 234/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1512480538333.3960 - mae: 763905.3750 - val_loss: 1523484871039.8130 - val_mae: 763646.3125\n",
      "Epoch 235/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1512398460956.9407 - mae: 764147.0625 - val_loss: 1522905254381.9871 - val_mae: 762170.4375\n",
      "Epoch 236/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1512205372334.6667 - mae: 764048.5000 - val_loss: 1522607543360.5676 - val_mae: 761098.8750\n",
      "Epoch 237/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1511949850565.1208 - mae: 763873.9375 - val_loss: 1522566858208.1155 - val_mae: 762106.8750\n",
      "Epoch 238/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1511790342222.6387 - mae: 763869.0000 - val_loss: 1522852680248.0352 - val_mae: 764151.1875\n",
      "Epoch 239/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1511643511516.2981 - mae: 764041.5625 - val_loss: 1522038353594.6672 - val_mae: 760951.3750\n",
      "Epoch 240/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1511498638021.7444 - mae: 763697.5000 - val_loss: 1521922022659.9668 - val_mae: 761761.8125\n",
      "Epoch 241/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1511152696087.3772 - mae: 763836.0625 - val_loss: 1521614479910.0720 - val_mae: 760588.3750\n",
      "Epoch 242/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1511064662126.5732 - mae: 763753.2500 - val_loss: 1521461934328.4905 - val_mae: 760528.7500\n",
      "Epoch 243/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1510906569252.4751 - mae: 763548.4375 - val_loss: 1521257177733.8755 - val_mae: 761070.1250\n",
      "Epoch 244/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1510746599948.1250 - mae: 763656.9375 - val_loss: 1521147564676.6780 - val_mae: 760503.0000\n",
      "Epoch 245/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1510580967170.3203 - mae: 763675.3125 - val_loss: 1520933733878.4695 - val_mae: 758737.3750\n",
      "Epoch 246/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1510259101416.1738 - mae: 763484.6875 - val_loss: 1520969406877.7515 - val_mae: 757574.2500\n",
      "Epoch 247/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1510139944973.5723 - mae: 763435.9375 - val_loss: 1520460658918.9265 - val_mae: 759717.3125\n",
      "Epoch 248/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1509926328663.4956 - mae: 763195.3125 - val_loss: 1520602617029.0957 - val_mae: 762545.0625\n",
      "Epoch 249/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1509729526517.6462 - mae: 763528.0000 - val_loss: 1520304164085.9956 - val_mae: 758092.2500\n",
      "Epoch 250/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1509534261523.4353 - mae: 763443.8125 - val_loss: 1520226843709.1746 - val_mae: 757030.8125\n",
      "Epoch 251/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1509378004870.8484 - mae: 763079.8125 - val_loss: 1519762695601.0120 - val_mae: 761580.3125\n",
      "Epoch 252/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1509075808178.1597 - mae: 763247.5625 - val_loss: 1519501361568.7454 - val_mae: 761021.0000\n",
      "Epoch 253/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1508989298291.8125 - mae: 763234.3125 - val_loss: 1519285911090.5464 - val_mae: 758672.0000\n",
      "Epoch 254/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1508781982465.6218 - mae: 763121.5625 - val_loss: 1519033627310.9912 - val_mae: 758793.1250\n",
      "Epoch 255/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1508552116797.8232 - mae: 762854.1250 - val_loss: 1519255866267.5061 - val_mae: 762216.1875\n",
      "Epoch 256/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1508382047160.6462 - mae: 763023.1875 - val_loss: 1518825774384.6753 - val_mae: 761269.6250\n",
      "Epoch 257/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1508169326032.3477 - mae: 763058.6250 - val_loss: 1518568396763.4749 - val_mae: 757761.5625\n",
      "Epoch 258/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1508023788383.0303 - mae: 762693.8125 - val_loss: 1518283078854.4929 - val_mae: 759991.4375\n",
      "Epoch 259/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1507805213956.1665 - mae: 762903.6250 - val_loss: 1518102617740.7612 - val_mae: 759829.1250\n",
      "Epoch 260/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1507442653501.1497 - mae: 762735.6250 - val_loss: 1518001411615.5852 - val_mae: 758245.3125\n",
      "Epoch 261/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1507382254866.3374 - mae: 762729.8750 - val_loss: 1517692110181.1675 - val_mae: 759912.3750\n",
      "Epoch 262/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1507157248037.3235 - mae: 762679.8125 - val_loss: 1517414414643.5693 - val_mae: 759054.6250\n",
      "Epoch 263/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1506898726608.2229 - mae: 762623.2500 - val_loss: 1517336686820.7310 - val_mae: 758360.6875\n",
      "Epoch 264/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1506743270199.3115 - mae: 762518.6250 - val_loss: 1517103102931.7908 - val_mae: 758581.9375\n",
      "Epoch 265/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1506543976938.1946 - mae: 762502.0625 - val_loss: 1516897351098.1931 - val_mae: 759665.3125\n",
      "Epoch 266/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1506426118141.6050 - mae: 762519.1250 - val_loss: 1516696542343.4224 - val_mae: 758494.1250\n",
      "Epoch 267/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1506101513077.1848 - mae: 762218.5625 - val_loss: 1516485018021.5356 - val_mae: 759104.3750\n",
      "Epoch 268/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1505990312446.9521 - mae: 762379.0000 - val_loss: 1516163532358.3059 - val_mae: 758536.8125\n",
      "Epoch 269/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1505738625951.2983 - mae: 762318.7500 - val_loss: 1516027982520.5715 - val_mae: 757530.8750\n",
      "Epoch 270/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1505491036308.2959 - mae: 761896.4375 - val_loss: 1517251343013.4109 - val_mae: 764418.3750\n",
      "Epoch 271/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1505331676818.9485 - mae: 762296.8750 - val_loss: 1515594622543.0879 - val_mae: 759249.1250\n",
      "Epoch 272/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1505151749097.7456 - mae: 762030.3750 - val_loss: 1515403986743.4114 - val_mae: 759717.5625\n",
      "Epoch 273/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1504897441918.9397 - mae: 761976.5625 - val_loss: 1515331325055.8379 - val_mae: 760559.4375\n",
      "Epoch 274/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1504662098253.3164 - mae: 762162.0625 - val_loss: 1515177024807.0947 - val_mae: 756156.6875\n",
      "Epoch 275/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1504504400045.0452 - mae: 761836.6875 - val_loss: 1514775143671.7917 - val_mae: 758431.6875\n",
      "Epoch 276/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1504179202256.9714 - mae: 761858.8125 - val_loss: 1514705903505.0276 - val_mae: 756835.6250\n",
      "Epoch 277/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1504034412887.4956 - mae: 761780.4375 - val_loss: 1514747695871.6257 - val_mae: 755409.5625\n",
      "Epoch 278/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1503753812446.4189 - mae: 761556.7500 - val_loss: 1513971931920.1919 - val_mae: 758468.3125\n",
      "Epoch 279/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1503572559530.8994 - mae: 761734.2500 - val_loss: 1513828745583.2468 - val_mae: 757966.3750\n",
      "Epoch 280/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1503422134539.6511 - mae: 761555.3125 - val_loss: 1513611400577.5095 - val_mae: 757899.9375\n",
      "Epoch 281/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1503146330965.2502 - mae: 761482.1250 - val_loss: 1513350052142.7791 - val_mae: 758835.6250\n",
      "Epoch 282/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1502868236310.1545 - mae: 761482.8750 - val_loss: 1513180278653.4678 - val_mae: 757262.5625\n",
      "Epoch 283/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1502705116832.5208 - mae: 761492.8125 - val_loss: 1512929145640.3423 - val_mae: 757112.9375\n",
      "Epoch 284/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1502423572780.9827 - mae: 761272.8750 - val_loss: 1512718053251.8545 - val_mae: 757132.8125\n",
      "Epoch 285/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1502245314672.5691 - mae: 761300.4375 - val_loss: 1512484380738.2642 - val_mae: 757202.8750\n",
      "Epoch 286/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1502030488917.9985 - mae: 761106.6250 - val_loss: 1512288041269.1660 - val_mae: 758474.1875\n",
      "Epoch 287/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1501754845218.9282 - mae: 761158.6875 - val_loss: 1512108698374.2122 - val_mae: 758380.5625\n",
      "Epoch 288/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1501608698560.9541 - mae: 761019.3750 - val_loss: 1511858952462.0461 - val_mae: 759045.0625\n",
      "Epoch 289/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1501400489128.9534 - mae: 760958.9375 - val_loss: 1511568212258.6040 - val_mae: 758355.5000\n",
      "Epoch 290/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1501196614833.7356 - mae: 761011.8125 - val_loss: 1511324780910.2490 - val_mae: 757659.3125\n",
      "Epoch 291/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1500998614482.7427 - mae: 760884.7500 - val_loss: 1511030268960.2339 - val_mae: 757764.2500\n",
      "Epoch 292/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1500693748534.5132 - mae: 760709.3125 - val_loss: 1511075716052.5891 - val_mae: 758975.6250\n",
      "Epoch 293/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1500566546438.1873 - mae: 760945.0625 - val_loss: 1510794101722.7764 - val_mae: 755900.1250\n",
      "Epoch 294/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1500322341897.8796 - mae: 760527.7500 - val_loss: 1510665040396.1252 - val_mae: 759259.5000\n",
      "Epoch 295/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1500039508407.4988 - mae: 760569.5625 - val_loss: 1510347645217.8057 - val_mae: 758775.8750\n",
      "Epoch 296/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1499735051661.6846 - mae: 760792.0625 - val_loss: 1510148684040.4578 - val_mae: 754932.3750\n",
      "Epoch 297/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1499652567353.3574 - mae: 760321.8750 - val_loss: 1509683233266.1785 - val_mae: 757245.2500\n",
      "Epoch 298/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1499252656178.0972 - mae: 760495.0000 - val_loss: 1509541343917.3943 - val_mae: 755902.3125\n",
      "Epoch 299/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1499170014097.3271 - mae: 760356.6875 - val_loss: 1509195217459.0454 - val_mae: 756316.2500\n",
      "Epoch 300/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1498963654808.6868 - mae: 760154.4375 - val_loss: 1509343129317.7788 - val_mae: 759524.8125\n",
      "Epoch 301/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1498724416304.9246 - mae: 760235.8750 - val_loss: 1508874279708.9656 - val_mae: 757298.4375\n",
      "Epoch 302/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1498510907141.5137 - mae: 760264.6875 - val_loss: 1508647534965.6338 - val_mae: 755609.6250\n",
      "Epoch 303/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1498301557380.8774 - mae: 759956.9375 - val_loss: 1508849199878.2122 - val_mae: 759708.1250\n",
      "Epoch 304/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1497985869169.1431 - mae: 760012.3125 - val_loss: 1508507105924.4783 - val_mae: 759362.5625\n",
      "Epoch 305/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1497833346641.0837 - mae: 760062.2500 - val_loss: 1507851744474.6516 - val_mae: 757075.6875\n",
      "Epoch 306/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1497574379355.1382 - mae: 759965.8125 - val_loss: 1507892702820.4441 - val_mae: 754295.5000\n",
      "Epoch 307/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1497372442628.7900 - mae: 759722.0625 - val_loss: 1507646966330.6299 - val_mae: 758125.6250\n",
      "Epoch 308/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1497179942601.0378 - mae: 759876.1250 - val_loss: 1507209125162.2883 - val_mae: 755880.9375\n",
      "Epoch 309/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1496883586803.5505 - mae: 759795.7500 - val_loss: 1507471972903.0698 - val_mae: 753391.5625\n",
      "Epoch 310/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1496616191141.5605 - mae: 759355.1875 - val_loss: 1506928288737.9617 - val_mae: 758515.3125\n",
      "Epoch 311/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1496401117292.5774 - mae: 759738.5000 - val_loss: 1506836291816.3235 - val_mae: 753263.8750\n",
      "Epoch 312/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1496238328225.3442 - mae: 759372.3125 - val_loss: 1506257664527.9172 - val_mae: 754923.6875\n",
      "Epoch 313/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1495942866012.9094 - mae: 759265.7500 - val_loss: 1505891511804.6567 - val_mae: 756382.4375\n",
      "Epoch 314/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1495698728372.2053 - mae: 759322.1250 - val_loss: 1505892331141.9753 - val_mae: 757648.5000\n",
      "Epoch 315/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1495436020198.0034 - mae: 759147.6250 - val_loss: 1505822881484.8298 - val_mae: 758747.5000\n",
      "Epoch 316/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1495163017982.6277 - mae: 759210.1875 - val_loss: 1505299130112.3242 - val_mae: 757235.6875\n",
      "Epoch 317/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1494944122412.7581 - mae: 759051.1250 - val_loss: 1505021786461.0840 - val_mae: 757143.0000\n",
      "Epoch 318/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1494778215055.6555 - mae: 759074.9375 - val_loss: 1504730154888.8445 - val_mae: 756146.3125\n",
      "Epoch 319/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1494474267103.2173 - mae: 758949.4375 - val_loss: 1504521530565.2954 - val_mae: 754732.1250\n",
      "Epoch 320/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1494070448118.0205 - mae: 758622.0000 - val_loss: 1504344618167.5237 - val_mae: 756728.1250\n",
      "Epoch 321/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1494023576136.6011 - mae: 758908.3750 - val_loss: 1504159284064.8264 - val_mae: 754973.4375\n",
      "Epoch 322/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1493747158381.7500 - mae: 758655.8750 - val_loss: 1503784070639.8831 - val_mae: 756126.3750\n",
      "Epoch 323/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1493490127899.8430 - mae: 758685.3750 - val_loss: 1503487777759.7661 - val_mae: 754898.0000\n",
      "Epoch 324/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1493220175310.3518 - mae: 758399.3125 - val_loss: 1503499742981.7134 - val_mae: 757644.4375\n",
      "Epoch 325/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1492953623239.9399 - mae: 758555.5625 - val_loss: 1503151437995.8477 - val_mae: 756889.4375\n",
      "Epoch 326/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1492705931693.9185 - mae: 758535.9375 - val_loss: 1502719899792.7031 - val_mae: 753839.5625\n",
      "Epoch 327/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1492462545306.1589 - mae: 758289.7500 - val_loss: 1502504325153.1321 - val_mae: 754145.0000\n",
      "Epoch 328/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1492190085699.6113 - mae: 758192.0625 - val_loss: 1502397155564.9141 - val_mae: 753873.4375\n",
      "Epoch 329/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1492019872698.8418 - mae: 758143.6250 - val_loss: 1502102556740.2600 - val_mae: 756047.6875\n",
      "Epoch 330/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1491652686239.5479 - mae: 757987.1250 - val_loss: 1501703766526.9521 - val_mae: 755320.8125\n",
      "Epoch 331/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1491385555183.7085 - mae: 758033.0000 - val_loss: 1501560087133.9573 - val_mae: 754370.6250\n",
      "Epoch 332/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1491212314714.8137 - mae: 757994.1875 - val_loss: 1501483289652.7917 - val_mae: 752662.1875\n",
      "Epoch 333/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1490989875346.8987 - mae: 757850.7500 - val_loss: 1501023950568.7727 - val_mae: 753031.1250\n",
      "Epoch 334/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1490709057524.4236 - mae: 757735.3125 - val_loss: 1500821522996.2429 - val_mae: 753029.1250\n",
      "Epoch 335/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1490387034927.1284 - mae: 757505.3125 - val_loss: 1500488723191.8418 - val_mae: 755608.1250\n",
      "Epoch 336/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1490173153241.2795 - mae: 757683.2500 - val_loss: 1500129237800.1426 - val_mae: 754296.6250\n",
      "Epoch 337/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1489886575340.2654 - mae: 757483.0625 - val_loss: 1500085949116.5632 - val_mae: 755427.6875\n",
      "Epoch 338/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1489630522985.2344 - mae: 757271.8750 - val_loss: 1499753243570.0598 - val_mae: 756087.3125\n",
      "Epoch 339/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1489369210895.1689 - mae: 757514.3750 - val_loss: 1499439161718.4321 - val_mae: 753732.6875\n",
      "Epoch 340/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1489107699102.6497 - mae: 757112.1875 - val_loss: 1499167321507.2402 - val_mae: 755540.1250\n",
      "Epoch 341/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1488849480818.2656 - mae: 757263.9375 - val_loss: 1498792843259.4094 - val_mae: 752860.6250\n",
      "Epoch 342/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1488520344882.4714 - mae: 757010.0000 - val_loss: 1498596653262.9756 - val_mae: 754197.3125\n",
      "Epoch 343/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1488239780284.6880 - mae: 756808.8125 - val_loss: 1499298792075.4641 - val_mae: 758369.0625\n",
      "Epoch 344/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1488015616450.0769 - mae: 757171.6250 - val_loss: 1498201904898.3203 - val_mae: 752067.0625\n",
      "Epoch 345/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1487746849095.5283 - mae: 756743.9375 - val_loss: 1497801142856.8008 - val_mae: 752868.9375\n",
      "Epoch 346/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1487516728159.2297 - mae: 756754.0625 - val_loss: 1497601471053.5908 - val_mae: 752980.0000\n",
      "Epoch 347/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1487105036937.4680 - mae: 756817.1250 - val_loss: 1498092393258.7373 - val_mae: 749823.0000\n",
      "Epoch 348/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1486984778323.1794 - mae: 756227.8125 - val_loss: 1497829411207.7966 - val_mae: 757798.9375\n",
      "Epoch 349/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1486747282491.9771 - mae: 756616.6875 - val_loss: 1496778053411.8516 - val_mae: 753181.6250\n",
      "Epoch 350/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1486477877443.5989 - mae: 756461.4375 - val_loss: 1496547609758.1755 - val_mae: 751875.9375\n",
      "Epoch 351/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1486270337255.1260 - mae: 756301.7500 - val_loss: 1496199126731.0337 - val_mae: 753393.8750\n",
      "Epoch 352/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1485833508298.9587 - mae: 756305.8750 - val_loss: 1495944612070.5273 - val_mae: 752852.4375\n",
      "Epoch 353/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1485668438813.9636 - mae: 756199.3125 - val_loss: 1495663850998.8687 - val_mae: 751193.4375\n",
      "Epoch 354/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1485426848748.7395 - mae: 755955.8750 - val_loss: 1495385353157.8193 - val_mae: 752882.5000\n",
      "Epoch 355/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1485110563256.2971 - mae: 756007.2500 - val_loss: 1495089947328.9543 - val_mae: 752162.9375\n",
      "Epoch 356/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1484903362592.3337 - mae: 755793.5625 - val_loss: 1494992191721.1218 - val_mae: 753379.0625\n",
      "Epoch 357/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1484684357394.4871 - mae: 755855.1250 - val_loss: 1494638941369.9187 - val_mae: 753561.9375\n",
      "Epoch 358/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1484381276984.5090 - mae: 755648.0000 - val_loss: 1494471724902.3152 - val_mae: 754587.3750\n",
      "Epoch 359/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1484105750240.7888 - mae: 755717.1250 - val_loss: 1494046889414.1687 - val_mae: 753638.6875\n",
      "Epoch 360/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1483776560955.2036 - mae: 755610.0000 - val_loss: 1493864206080.1248 - val_mae: 753098.7500\n",
      "Epoch 361/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1483589281992.9878 - mae: 755446.1250 - val_loss: 1493443852919.7046 - val_mae: 752826.1250\n",
      "Epoch 362/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1483196243031.5205 - mae: 755434.6250 - val_loss: 1493287762058.3164 - val_mae: 752888.5625\n",
      "Epoch 363/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1483003126833.7979 - mae: 755270.3750 - val_loss: 1493106364950.5039 - val_mae: 753122.3125\n",
      "Epoch 364/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1482813110825.4651 - mae: 755173.6875 - val_loss: 1492805712243.4382 - val_mae: 752775.4375\n",
      "Epoch 365/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1482479483914.0793 - mae: 755024.1250 - val_loss: 1492897783904.6018 - val_mae: 754738.7500\n",
      "Epoch 366/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1482305476678.4556 - mae: 755149.7500 - val_loss: 1492345219116.5088 - val_mae: 750186.7500\n",
      "Epoch 367/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1481998622902.8250 - mae: 754875.0625 - val_loss: 1491998774641.5422 - val_mae: 752061.0000\n",
      "Epoch 368/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1481567951740.7690 - mae: 754647.7500 - val_loss: 1492831547025.1523 - val_mae: 756814.8125\n",
      "Epoch 369/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1481424863323.0134 - mae: 755051.5625 - val_loss: 1491571069978.2461 - val_mae: 750077.8750\n",
      "Epoch 370/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1481117189260.0129 - mae: 754532.8750 - val_loss: 1491235146461.4958 - val_mae: 751172.9375\n",
      "Epoch 371/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1480876099420.1360 - mae: 754704.1875 - val_loss: 1490922071536.8811 - val_mae: 750194.1875\n",
      "Epoch 372/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1480540551651.2092 - mae: 754465.6875 - val_loss: 1490940051387.3408 - val_mae: 749212.3125\n",
      "Epoch 373/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1480323908757.5933 - mae: 754372.3125 - val_loss: 1490285154016.1902 - val_mae: 750308.1250\n",
      "Epoch 374/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1480066575091.0518 - mae: 754287.6250 - val_loss: 1490118588820.9695 - val_mae: 752254.6875\n",
      "Epoch 375/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1479790091832.1348 - mae: 754220.1250 - val_loss: 1489703080668.4978 - val_mae: 751680.0000\n",
      "Epoch 376/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1479576834568.1333 - mae: 754375.0625 - val_loss: 1489481288322.4824 - val_mae: 750301.2500\n",
      "Epoch 377/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1479292498009.8159 - mae: 753997.4375 - val_loss: 1489396545286.3120 - val_mae: 749946.2500\n",
      "Epoch 378/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1479069099262.1787 - mae: 754173.6250 - val_loss: 1489924278745.2295 - val_mae: 747341.4375\n",
      "Epoch 379/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1478822258729.3154 - mae: 753764.6875 - val_loss: 1488859351701.6431 - val_mae: 752061.5625\n",
      "Epoch 380/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1478501257857.4846 - mae: 753935.3125 - val_loss: 1488587890929.9038 - val_mae: 749190.0000\n",
      "Epoch 381/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1478286919415.5425 - mae: 753677.6875 - val_loss: 1488252808376.5215 - val_mae: 751960.6250\n",
      "Epoch 382/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1477967013140.5330 - mae: 753757.4375 - val_loss: 1488027556028.6133 - val_mae: 748869.8750\n",
      "Epoch 383/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1477687304323.7300 - mae: 753556.3750 - val_loss: 1487628569366.9778 - val_mae: 750859.6250\n",
      "Epoch 384/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1477458696907.5327 - mae: 753419.5625 - val_loss: 1487534098561.9336 - val_mae: 752287.8750\n",
      "Epoch 385/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1477210321806.5327 - mae: 753611.1875 - val_loss: 1487042306511.8486 - val_mae: 750171.1875\n",
      "Epoch 386/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1476914611737.5977 - mae: 753316.5625 - val_loss: 1486860944027.4312 - val_mae: 749414.8125\n",
      "Epoch 387/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1476600225681.0276 - mae: 753103.6250 - val_loss: 1486521381349.8037 - val_mae: 750937.3750\n",
      "Epoch 388/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1476392704078.4392 - mae: 753215.0625 - val_loss: 1486398537173.9363 - val_mae: 748643.8750\n",
      "Epoch 389/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1476046356084.0120 - mae: 752979.8125 - val_loss: 1486192105390.7666 - val_mae: 749199.9375\n",
      "Epoch 390/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1475868506025.1782 - mae: 753054.8125 - val_loss: 1485830689206.7004 - val_mae: 749177.0625\n",
      "Epoch 391/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1475592163868.8906 - mae: 752953.6250 - val_loss: 1486144200228.9741 - val_mae: 746606.2500\n",
      "Epoch 392/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1475298405097.4712 - mae: 752950.3750 - val_loss: 1486297065704.9224 - val_mae: 745791.1250\n",
      "Epoch 393/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1475011673415.8276 - mae: 752429.4375 - val_loss: 1485048603563.6729 - val_mae: 751218.0000\n",
      "Epoch 394/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1474871415813.4888 - mae: 752762.6250 - val_loss: 1484628922062.6262 - val_mae: 749383.2500\n",
      "Epoch 395/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1474554840906.5720 - mae: 752538.9375 - val_loss: 1484329828762.3584 - val_mae: 749002.1875\n",
      "Epoch 396/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1474281534740.1338 - mae: 752588.7500 - val_loss: 1484207582853.9753 - val_mae: 748415.8125\n",
      "Epoch 397/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1473997416382.3347 - mae: 752470.0000 - val_loss: 1484406226691.7173 - val_mae: 746533.6875\n",
      "Epoch 398/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1473750099824.0952 - mae: 752100.6250 - val_loss: 1483995655441.9382 - val_mae: 751693.9375\n",
      "Epoch 399/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1473554136371.0703 - mae: 752289.6875 - val_loss: 1483482192455.0044 - val_mae: 747818.9375\n",
      "Epoch 400/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1473282135717.7102 - mae: 752060.2500 - val_loss: 1483717193450.5688 - val_mae: 752830.9375\n",
      "Epoch 401/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1473054795983.3748 - mae: 752227.5000 - val_loss: 1482834670984.6946 - val_mae: 749161.5625\n",
      "Epoch 402/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1472693231913.7893 - mae: 752000.1875 - val_loss: 1482640120282.4270 - val_mae: 747869.0625\n",
      "Epoch 403/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1472503172991.8628 - mae: 751764.8750 - val_loss: 1482560307334.1250 - val_mae: 750647.3125\n",
      "Epoch 404/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1472325749986.8347 - mae: 751766.5000 - val_loss: 1482188042939.7649 - val_mae: 750062.8125\n",
      "Epoch 405/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1471912477614.8662 - mae: 751754.6875 - val_loss: 1482003908622.9692 - val_mae: 751045.1875\n",
      "Epoch 406/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1471646692821.3376 - mae: 751827.8125 - val_loss: 1481686486900.6858 - val_mae: 747170.1875\n",
      "Epoch 407/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1471371543735.1245 - mae: 751388.5000 - val_loss: 1481572577789.7546 - val_mae: 750890.6250\n",
      "Epoch 408/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1471134208799.5603 - mae: 751638.2500 - val_loss: 1481174112241.8291 - val_mae: 747236.4375\n",
      "Epoch 409/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1470944026386.8862 - mae: 751461.8125 - val_loss: 1480909035631.9705 - val_mae: 747255.3125\n",
      "Epoch 410/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1470572626504.4014 - mae: 751297.7500 - val_loss: 1480594169021.7109 - val_mae: 747312.1250\n",
      "Epoch 411/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1470385023413.3032 - mae: 751396.1250 - val_loss: 1480321358141.9480 - val_mae: 746818.1875\n",
      "Epoch 412/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1470167759543.5735 - mae: 751100.8750 - val_loss: 1480041393299.7969 - val_mae: 749591.8750\n",
      "Epoch 413/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1469898239415.9976 - mae: 750887.6875 - val_loss: 1480481328171.2114 - val_mae: 752549.3125\n",
      "Epoch 414/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1469717833766.1218 - mae: 751143.5000 - val_loss: 1479448455602.1096 - val_mae: 749211.0625\n",
      "Epoch 415/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1469294901635.4055 - mae: 750928.5000 - val_loss: 1479211075913.5242 - val_mae: 748410.5000\n",
      "Epoch 416/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1469137222693.2236 - mae: 750856.8125 - val_loss: 1479061777816.0632 - val_mae: 747253.6250\n",
      "Epoch 417/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1468776414098.3250 - mae: 750767.5625 - val_loss: 1478833049926.9294 - val_mae: 749502.0625\n",
      "Epoch 418/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1468679276522.2446 - mae: 750855.9375 - val_loss: 1478529735983.0784 - val_mae: 746319.6875\n",
      "Epoch 419/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1468431351522.1860 - mae: 750499.2500 - val_loss: 1478213837881.7815 - val_mae: 747598.6250\n",
      "Epoch 420/1000\n",
      "164176/164176 [==============================] - 25s 151us/step - loss: 1468065117672.4980 - mae: 750660.4375 - val_loss: 1478167285951.2078 - val_mae: 745161.8125\n",
      "Epoch 421/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1467944674847.1860 - mae: 750344.6875 - val_loss: 1477685305319.2507 - val_mae: 746955.8125\n",
      "Epoch 422/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1467644713614.8569 - mae: 750388.6875 - val_loss: 1477411412440.8303 - val_mae: 748693.9375\n",
      "Epoch 423/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1467325546785.0073 - mae: 750389.3125 - val_loss: 1477207720421.4045 - val_mae: 747961.3125\n",
      "Epoch 424/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1467137081495.6890 - mae: 750169.3125 - val_loss: 1477071971887.1533 - val_mae: 749149.3125\n",
      "Epoch 425/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1466850060142.4983 - mae: 750270.7500 - val_loss: 1476629269187.9482 - val_mae: 747381.9375\n",
      "Epoch 426/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1466493598683.7744 - mae: 749878.1250 - val_loss: 1477180217246.7993 - val_mae: 751536.1875\n",
      "Epoch 427/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1466342411727.1501 - mae: 750137.2500 - val_loss: 1476253849255.3069 - val_mae: 748201.6875\n",
      "Epoch 428/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1466110813286.4897 - mae: 749984.8750 - val_loss: 1475881449407.5322 - val_mae: 746804.8750\n",
      "Epoch 429/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1465852426595.4709 - mae: 749855.8125 - val_loss: 1475761974533.1643 - val_mae: 747519.0625\n",
      "Epoch 430/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1465612531585.2600 - mae: 749682.8750 - val_loss: 1475303542018.7693 - val_mae: 746627.8125\n",
      "Epoch 431/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1465373593984.4116 - mae: 749714.0625 - val_loss: 1475079148434.5244 - val_mae: 746685.6250\n",
      "Epoch 432/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1465125991877.3704 - mae: 749580.3125 - val_loss: 1474927268253.1528 - val_mae: 747890.5625\n",
      "Epoch 433/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1464924508895.6914 - mae: 749526.0625 - val_loss: 1474718943155.9561 - val_mae: 748261.1875\n",
      "Epoch 434/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1464591379013.0085 - mae: 749417.5625 - val_loss: 1474887000523.0586 - val_mae: 749995.5000\n",
      "Epoch 435/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1464406258360.5715 - mae: 749559.6875 - val_loss: 1474162960877.1885 - val_mae: 747691.3750\n",
      "Epoch 436/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1464053430258.6274 - mae: 749310.7500 - val_loss: 1473862063839.1924 - val_mae: 747124.7500\n",
      "Epoch 437/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1463830635972.1726 - mae: 749202.0000 - val_loss: 1473606315305.8892 - val_mae: 747473.3125\n",
      "Epoch 438/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1463553721900.7581 - mae: 749052.6875 - val_loss: 1474242252718.1677 - val_mae: 750664.3750\n",
      "Epoch 439/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1463350395866.7764 - mae: 749277.0625 - val_loss: 1473159592224.4084 - val_mae: 744859.7500\n",
      "Epoch 440/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1463172114126.8257 - mae: 749010.0000 - val_loss: 1472890745541.3452 - val_mae: 746080.6250\n",
      "Epoch 441/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1462866683193.2576 - mae: 748961.5000 - val_loss: 1472640028641.2629 - val_mae: 744828.0000\n",
      "Epoch 442/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1462636366942.3066 - mae: 748812.6875 - val_loss: 1472499244775.7747 - val_mae: 746750.6250\n",
      "Epoch 443/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1462370006332.0518 - mae: 748774.5625 - val_loss: 1472197878549.5808 - val_mae: 744711.6250\n",
      "Epoch 444/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1462141116531.7627 - mae: 748730.0000 - val_loss: 1472023921965.2822 - val_mae: 743799.7500\n",
      "Epoch 445/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1461916238009.2202 - mae: 748524.4375 - val_loss: 1471627990829.4319 - val_mae: 746105.8125\n",
      "Epoch 446/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1461701574052.4377 - mae: 748513.6875 - val_loss: 1471581708368.6345 - val_mae: 747810.5625\n",
      "Epoch 447/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1461441429092.0449 - mae: 748532.7500 - val_loss: 1471135841994.2354 - val_mae: 746601.1875\n",
      "Epoch 448/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1461220237418.1821 - mae: 748368.6875 - val_loss: 1471320784005.0271 - val_mae: 748278.8750\n",
      "Epoch 449/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1460925955757.4941 - mae: 748275.4375 - val_loss: 1471429876274.2471 - val_mae: 749529.8125\n",
      "Epoch 450/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1460756710302.1008 - mae: 748408.0625 - val_loss: 1470360417596.0518 - val_mae: 745691.9375\n",
      "Epoch 451/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1460503632541.2275 - mae: 748083.6250 - val_loss: 1470713112120.7336 - val_mae: 748702.3750\n",
      "Epoch 452/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1460232919235.0002 - mae: 748365.0000 - val_loss: 1470042632963.6177 - val_mae: 743089.3125\n",
      "Epoch 453/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1459984033957.8599 - mae: 747973.3125 - val_loss: 1469782267140.9648 - val_mae: 746563.8750\n",
      "Epoch 454/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1459683838453.9705 - mae: 748111.3125 - val_loss: 1469451879209.5398 - val_mae: 744572.8750\n",
      "Epoch 455/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1459591193629.0405 - mae: 747851.8125 - val_loss: 1469208418430.3408 - val_mae: 745018.1250\n",
      "Epoch 456/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1459225517349.4980 - mae: 747749.6875 - val_loss: 1469180515154.6555 - val_mae: 746896.2500\n",
      "Epoch 457/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1459034945115.4624 - mae: 747815.9375 - val_loss: 1468687596729.7192 - val_mae: 745838.5000\n",
      "Epoch 458/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1458877281747.7407 - mae: 747850.7500 - val_loss: 1468686474627.7048 - val_mae: 743364.3750\n",
      "Epoch 459/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1458596940959.7725 - mae: 747563.1875 - val_loss: 1468143560691.0266 - val_mae: 744529.2500\n",
      "Epoch 460/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1458245382355.1670 - mae: 747538.4375 - val_loss: 1468070643904.2058 - val_mae: 744848.5625\n",
      "Epoch 461/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1458156784070.0688 - mae: 747591.1875 - val_loss: 1467967896159.7537 - val_mae: 742895.0625\n",
      "Epoch 462/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1457821257291.6948 - mae: 747353.8125 - val_loss: 1467571190081.9397 - val_mae: 744971.5000\n",
      "Epoch 463/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1457525814453.9270 - mae: 747138.6875 - val_loss: 1468909696739.1841 - val_mae: 750642.8750\n",
      "Epoch 464/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1457509139568.8684 - mae: 747536.1250 - val_loss: 1467026569999.3933 - val_mae: 744622.8125\n",
      "Epoch 465/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1457207154435.9170 - mae: 747271.7500 - val_loss: 1466849894696.1926 - val_mae: 744236.6250\n",
      "Epoch 466/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1456994388168.1895 - mae: 747166.8750 - val_loss: 1466526202268.9531 - val_mae: 744017.0625\n",
      "Epoch 467/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1456764162017.2632 - mae: 747207.3125 - val_loss: 1466812152232.1301 - val_mae: 741494.8750\n",
      "Epoch 468/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1456529952406.7407 - mae: 746888.7500 - val_loss: 1466035898111.5259 - val_mae: 745158.9375\n",
      "Epoch 469/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1456257610063.7117 - mae: 746995.6875 - val_loss: 1465974686069.6338 - val_mae: 743948.8125\n",
      "Epoch 470/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1456109713811.3728 - mae: 746934.0625 - val_loss: 1465697207284.3240 - val_mae: 743057.7500\n",
      "Epoch 471/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1455793830283.3892 - mae: 746706.7500 - val_loss: 1465549936472.5435 - val_mae: 744984.1250\n",
      "Epoch 472/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1455615965200.9651 - mae: 746839.6250 - val_loss: 1465300200594.2002 - val_mae: 743537.9375\n",
      "Epoch 473/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1455365869682.5649 - mae: 746693.5625 - val_loss: 1464965292358.5303 - val_mae: 743543.8125\n",
      "Epoch 474/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1455198671340.9888 - mae: 746591.0000 - val_loss: 1464806919322.9822 - val_mae: 743319.6875\n",
      "Epoch 475/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1455016570003.7969 - mae: 746528.8125 - val_loss: 1464563432255.2952 - val_mae: 743946.5000\n",
      "Epoch 476/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1454721356947.5974 - mae: 746547.4375 - val_loss: 1464196676116.7075 - val_mae: 743311.5000\n",
      "Epoch 477/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1454504932121.9717 - mae: 746386.9375 - val_loss: 1463985090014.3191 - val_mae: 744417.0625\n",
      "Epoch 478/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1454223600910.7446 - mae: 746347.9375 - val_loss: 1463860962888.7009 - val_mae: 745059.7500\n",
      "Epoch 479/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1454004805992.0615 - mae: 746447.2500 - val_loss: 1463903676795.8210 - val_mae: 741554.3125\n",
      "Epoch 480/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1453832292256.7952 - mae: 746123.4375 - val_loss: 1463266936945.9663 - val_mae: 743860.9375\n",
      "Epoch 481/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1453693935190.4727 - mae: 746186.5625 - val_loss: 1463131163898.7856 - val_mae: 743859.4375\n",
      "Epoch 482/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1453393219093.4062 - mae: 746016.5625 - val_loss: 1463340105197.8872 - val_mae: 745984.5000\n",
      "Epoch 483/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1453239830983.6655 - mae: 746117.0000 - val_loss: 1463004048463.1377 - val_mae: 745360.7500\n",
      "Epoch 484/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1453030446043.9739 - mae: 746035.3750 - val_loss: 1462490649760.3711 - val_mae: 743037.0625\n",
      "Epoch 485/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1452825219258.4177 - mae: 745932.3750 - val_loss: 1462487607212.7708 - val_mae: 742054.5000\n",
      "Epoch 486/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1452548026615.8916 - mae: 745893.3125 - val_loss: 1462212832520.3579 - val_mae: 742344.6875\n",
      "Epoch 487/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1452374217409.3535 - mae: 745816.4375 - val_loss: 1461909213927.6748 - val_mae: 743536.6250\n",
      "Epoch 488/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1452193060385.1821 - mae: 745734.0000 - val_loss: 1461657178611.3760 - val_mae: 742903.5625\n",
      "Epoch 489/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1451965674491.1101 - mae: 745694.0625 - val_loss: 1461487889587.7314 - val_mae: 743290.0000\n",
      "Epoch 490/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1451742095701.4998 - mae: 745537.9375 - val_loss: 1461161111271.3755 - val_mae: 742768.5000\n",
      "Epoch 491/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1451541731369.7146 - mae: 745663.5625 - val_loss: 1461486588933.4888 - val_mae: 740514.6875\n",
      "Epoch 492/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1451366909449.6304 - mae: 745394.3125 - val_loss: 1460940501236.6982 - val_mae: 743952.3750\n",
      "Epoch 493/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1451161144835.8420 - mae: 745417.7500 - val_loss: 1460844632722.0505 - val_mae: 745091.5000\n",
      "Epoch 494/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1450932147154.3936 - mae: 745416.3125 - val_loss: 1460622668319.2859 - val_mae: 744839.0000\n",
      "Epoch 495/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1450716844769.2881 - mae: 745374.8750 - val_loss: 1460235553458.3843 - val_mae: 742762.4375\n",
      "Epoch 496/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1450481516866.5386 - mae: 745465.9375 - val_loss: 1460196887550.5032 - val_mae: 740886.0625\n",
      "Epoch 497/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1450285330453.1567 - mae: 745110.8125 - val_loss: 1459784763223.3459 - val_mae: 742236.5625\n",
      "Epoch 498/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1450072000113.4175 - mae: 745092.1875 - val_loss: 1459682976081.7075 - val_mae: 741819.5625\n",
      "Epoch 499/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1449802581163.9473 - mae: 745204.5000 - val_loss: 1460744771767.8230 - val_mae: 737914.0625\n",
      "Epoch 500/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1449738282064.1357 - mae: 744989.5000 - val_loss: 1459138864556.2219 - val_mae: 742309.2500\n",
      "Epoch 501/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1449457037765.8691 - mae: 744859.1250 - val_loss: 1459620701955.5178 - val_mae: 746011.8750\n",
      "Epoch 502/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1449349555950.7603 - mae: 745079.0625 - val_loss: 1458748539932.2422 - val_mae: 742434.1875\n",
      "Epoch 503/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1449120592190.1475 - mae: 744975.8125 - val_loss: 1458631709318.2747 - val_mae: 741473.3750\n",
      "Epoch 504/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1448919552532.9070 - mae: 744688.6875 - val_loss: 1458396819550.6060 - val_mae: 743468.6250\n",
      "Epoch 505/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1448769014032.0420 - mae: 744845.5000 - val_loss: 1458228731117.6128 - val_mae: 742483.5000\n",
      "Epoch 506/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1448534496866.6477 - mae: 744731.9375 - val_loss: 1457990307074.7693 - val_mae: 741989.3125\n",
      "Epoch 507/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1448317191373.4788 - mae: 744672.7500 - val_loss: 1457856092010.1074 - val_mae: 741196.1875\n",
      "Epoch 508/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1448225142242.1113 - mae: 744480.9375 - val_loss: 1457695734583.3115 - val_mae: 743701.6875\n",
      "Epoch 509/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1447977893484.0286 - mae: 744638.1250 - val_loss: 1457452749053.4802 - val_mae: 742498.9375\n",
      "Epoch 510/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1447708170738.3779 - mae: 744433.5000 - val_loss: 1457354479510.8157 - val_mae: 743536.3750\n",
      "Epoch 511/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1447546994271.2546 - mae: 744476.7500 - val_loss: 1456920577949.6018 - val_mae: 742365.7500\n",
      "Epoch 512/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1447371195819.4236 - mae: 744390.0000 - val_loss: 1456695186673.8042 - val_mae: 740891.9375\n",
      "Epoch 513/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1447134505341.4180 - mae: 744358.5625 - val_loss: 1456596012892.7349 - val_mae: 742367.9375\n",
      "Epoch 514/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1446952752840.8381 - mae: 744296.6875 - val_loss: 1456332348541.8420 - val_mae: 743386.5625\n",
      "Epoch 515/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1446819213816.9644 - mae: 744320.4375 - val_loss: 1456143127798.5942 - val_mae: 741787.5625\n",
      "Epoch 516/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1446711542524.8313 - mae: 744159.8125 - val_loss: 1456113268770.5293 - val_mae: 742412.3750\n",
      "Epoch 517/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1446416175927.5112 - mae: 744348.6250 - val_loss: 1456924478634.9495 - val_mae: 737473.5000\n",
      "Epoch 518/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1446233626945.1416 - mae: 744049.6250 - val_loss: 1456188002726.7332 - val_mae: 738422.0625\n",
      "Epoch 519/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1446076508226.8628 - mae: 743856.4375 - val_loss: 1455392041667.1499 - val_mae: 741887.3750\n",
      "Epoch 520/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1445765913342.6277 - mae: 744051.8750 - val_loss: 1455221958815.4729 - val_mae: 742586.0625\n",
      "Epoch 521/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1445694793893.8599 - mae: 744053.1875 - val_loss: 1455406663757.2417 - val_mae: 739125.1875\n",
      "Epoch 522/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1445459611468.7676 - mae: 743882.8125 - val_loss: 1455816890963.7781 - val_mae: 737940.6875\n",
      "Epoch 523/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1445303158454.0767 - mae: 743812.3125 - val_loss: 1455069106164.4238 - val_mae: 738899.4375\n",
      "Epoch 524/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1445201235747.2527 - mae: 743651.5625 - val_loss: 1454622187241.3713 - val_mae: 741781.3750\n",
      "Epoch 525/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1445019012190.3066 - mae: 743695.8125 - val_loss: 1454235889622.8843 - val_mae: 741034.1250\n",
      "Epoch 526/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1444885079512.9299 - mae: 743717.8750 - val_loss: 1454165818903.4021 - val_mae: 741924.4375\n",
      "Epoch 527/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1444684576840.8506 - mae: 743607.5000 - val_loss: 1453858129446.3713 - val_mae: 741062.6875\n",
      "Epoch 528/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1444407136574.9458 - mae: 743691.8750 - val_loss: 1453728530744.9583 - val_mae: 740241.4375\n",
      "Epoch 529/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1444223381001.4307 - mae: 743388.1875 - val_loss: 1453640527642.4707 - val_mae: 742599.8125\n",
      "Epoch 530/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1444043785542.8296 - mae: 743509.7500 - val_loss: 1453561335053.9463 - val_mae: 742195.8750\n",
      "Epoch 531/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1444001239167.3389 - mae: 743436.5000 - val_loss: 1453349617496.3438 - val_mae: 742325.5625\n",
      "Epoch 532/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1443735680084.5266 - mae: 743385.7500 - val_loss: 1453082573561.0393 - val_mae: 741583.7500\n",
      "Epoch 533/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1443619691641.0518 - mae: 743383.8125 - val_loss: 1452718063306.2354 - val_mae: 740365.7500\n",
      "Epoch 534/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1443462050617.9062 - mae: 743263.5000 - val_loss: 1452597171312.4692 - val_mae: 741764.6250\n",
      "Epoch 535/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1443212830989.6470 - mae: 743150.1875 - val_loss: 1452687253140.7449 - val_mae: 743334.0625\n",
      "Epoch 536/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1443089435065.2952 - mae: 743326.2500 - val_loss: 1452314249564.6848 - val_mae: 741476.8125\n",
      "Epoch 537/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1442981097137.0869 - mae: 743070.7500 - val_loss: 1452093631699.6660 - val_mae: 741085.9375\n",
      "Epoch 538/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1442730425478.6238 - mae: 743093.3125 - val_loss: 1452011726970.7483 - val_mae: 741114.4375\n",
      "Epoch 539/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1442563575516.9966 - mae: 743271.0625 - val_loss: 1452227374005.8521 - val_mae: 738014.0625\n",
      "Epoch 540/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1442420333495.8479 - mae: 743016.6875 - val_loss: 1451870764834.0552 - val_mae: 738453.1250\n",
      "Epoch 541/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1442230690696.5449 - mae: 742834.8750 - val_loss: 1451540821622.6067 - val_mae: 740687.8750\n",
      "Epoch 542/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1442062377470.1538 - mae: 742948.5000 - val_loss: 1451359250580.2959 - val_mae: 740157.8750\n",
      "Epoch 543/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1441702828589.4568 - mae: 742892.2500 - val_loss: 1451212945506.3982 - val_mae: 740002.9375\n",
      "Epoch 544/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1441736532072.5854 - mae: 742842.0625 - val_loss: 1450863341667.3962 - val_mae: 739995.7500\n",
      "Epoch 545/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1441604050844.2046 - mae: 742765.6250 - val_loss: 1450878027409.3518 - val_mae: 741364.3125\n",
      "Epoch 546/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1441391483665.7886 - mae: 742663.9375 - val_loss: 1450715527809.2849 - val_mae: 740108.4375\n",
      "Epoch 547/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1441265160624.3135 - mae: 742702.0000 - val_loss: 1450467109497.7004 - val_mae: 740825.2500\n",
      "Epoch 548/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1441096248817.4797 - mae: 742617.3750 - val_loss: 1450307822279.3413 - val_mae: 739862.3750\n",
      "Epoch 549/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1440942815367.1230 - mae: 742581.0000 - val_loss: 1450169117548.4026 - val_mae: 741003.4375\n",
      "Epoch 550/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1440742999721.3030 - mae: 742637.6875 - val_loss: 1450042838899.0889 - val_mae: 738568.0000\n",
      "Epoch 551/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1440656599616.2183 - mae: 742431.4375 - val_loss: 1449959136263.6843 - val_mae: 741806.1875\n",
      "Epoch 552/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1440429764204.4275 - mae: 742567.4375 - val_loss: 1449601558511.8333 - val_mae: 739353.5625\n",
      "Epoch 553/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1440292124604.3386 - mae: 742494.0625 - val_loss: 1449600282715.4126 - val_mae: 739515.1875\n",
      "Epoch 554/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1440240243010.1394 - mae: 742337.8125 - val_loss: 1449305037153.0759 - val_mae: 739279.1250\n",
      "Epoch 555/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1440010850907.1631 - mae: 742349.0000 - val_loss: 1449166891148.3123 - val_mae: 740095.7500\n",
      "Epoch 556/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1439765665284.4409 - mae: 742353.1875 - val_loss: 1449068489492.5828 - val_mae: 738341.3750\n",
      "Epoch 557/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1439600729940.3521 - mae: 742231.4375 - val_loss: 1449024064975.1501 - val_mae: 737952.8750\n",
      "Epoch 558/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1439533427757.6064 - mae: 742055.3125 - val_loss: 1448765397627.6963 - val_mae: 740714.1250\n",
      "Epoch 559/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1439279185332.4050 - mae: 742311.3125 - val_loss: 1449176811297.0571 - val_mae: 736697.8750\n",
      "Epoch 560/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1439163446607.2126 - mae: 741926.9375 - val_loss: 1448305952323.1123 - val_mae: 740407.5625\n",
      "Epoch 561/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1439064150676.6450 - mae: 742035.1250 - val_loss: 1448085110067.0703 - val_mae: 740144.8125\n",
      "Epoch 562/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1438892508216.9832 - mae: 742024.1875 - val_loss: 1448037193249.1819 - val_mae: 739556.8125\n",
      "Epoch 563/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1438790709055.1956 - mae: 741942.6875 - val_loss: 1447864093263.3872 - val_mae: 739672.1875\n",
      "Epoch 564/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1438540511381.4934 - mae: 741894.0625 - val_loss: 1448020598960.4382 - val_mae: 742065.5625\n",
      "Epoch 565/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1438392617651.9810 - mae: 741940.3125 - val_loss: 1447493749493.4465 - val_mae: 738873.0625\n",
      "Epoch 566/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1438200484689.0588 - mae: 741950.3125 - val_loss: 1447504592568.6714 - val_mae: 738725.5625\n",
      "Epoch 567/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1438149501677.6626 - mae: 741807.5625 - val_loss: 1447367598371.4023 - val_mae: 739577.3750\n",
      "Epoch 568/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1437963448864.8826 - mae: 741619.0625 - val_loss: 1447392803385.4321 - val_mae: 741953.6250\n",
      "Epoch 569/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1437696822249.7456 - mae: 741850.5625 - val_loss: 1446950009660.1018 - val_mae: 739477.1250\n",
      "Epoch 570/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1437681062189.3821 - mae: 741796.0625 - val_loss: 1446909927392.8638 - val_mae: 738132.3750\n",
      "Epoch 571/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1437528304792.5869 - mae: 741618.6875 - val_loss: 1446701499702.8625 - val_mae: 738828.5000\n",
      "Epoch 572/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1437299550736.3164 - mae: 741451.8750 - val_loss: 1446744227373.7561 - val_mae: 741697.3750\n",
      "Epoch 573/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1437212825924.3347 - mae: 741701.8750 - val_loss: 1446633062407.9836 - val_mae: 736923.3125\n",
      "Epoch 574/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1437003140104.0835 - mae: 741265.5625 - val_loss: 1447189808253.6423 - val_mae: 743469.5000\n",
      "Epoch 575/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1436911099515.1975 - mae: 741720.8125 - val_loss: 1446054749106.0598 - val_mae: 737558.1250\n",
      "Epoch 576/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1436768765481.5647 - mae: 741300.1250 - val_loss: 1445922418784.2026 - val_mae: 740629.7500\n",
      "Epoch 577/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1436544565886.6902 - mae: 741454.5625 - val_loss: 1445747081900.9951 - val_mae: 737875.7500\n",
      "Epoch 578/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1436478797745.1619 - mae: 741403.8750 - val_loss: 1445660965336.0320 - val_mae: 737894.0625\n",
      "Epoch 579/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1436330020937.0503 - mae: 741266.2500 - val_loss: 1445367358997.6057 - val_mae: 738403.3750\n",
      "Epoch 580/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1436184200616.2300 - mae: 741285.7500 - val_loss: 1445349642855.9368 - val_mae: 738831.8125\n",
      "Epoch 581/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1435929972283.7275 - mae: 741078.7500 - val_loss: 1445138696331.5139 - val_mae: 739771.8125\n",
      "Epoch 582/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1435867921252.0198 - mae: 741317.1250 - val_loss: 1444900591442.1565 - val_mae: 738118.3750\n",
      "Epoch 583/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1435610092864.0437 - mae: 741145.9375 - val_loss: 1445602471284.9353 - val_mae: 735717.3125\n",
      "Epoch 584/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1435521482464.5894 - mae: 741117.8750 - val_loss: 1445140076221.6611 - val_mae: 736143.0625\n",
      "Epoch 585/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1435433011288.4187 - mae: 740939.6250 - val_loss: 1444463504458.3474 - val_mae: 740007.9375\n",
      "Epoch 586/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1435253754734.7976 - mae: 740985.9375 - val_loss: 1444273347082.3289 - val_mae: 738344.6250\n",
      "Epoch 587/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1435139052613.3579 - mae: 741010.5625 - val_loss: 1444106462069.3843 - val_mae: 738717.3125\n",
      "Epoch 588/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1434963525100.2903 - mae: 740890.7500 - val_loss: 1444096552855.0154 - val_mae: 737757.8125\n",
      "Epoch 589/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1434872596273.0247 - mae: 740949.0625 - val_loss: 1443792868157.8979 - val_mae: 738647.5000\n",
      "Epoch 590/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1434662175224.5652 - mae: 740781.0625 - val_loss: 1443776562582.6660 - val_mae: 739847.5000\n",
      "Epoch 591/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1434537248051.6692 - mae: 740809.1875 - val_loss: 1443549262236.3542 - val_mae: 739287.8125\n",
      "Epoch 592/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1434465897794.6382 - mae: 740718.9375 - val_loss: 1443461795256.7961 - val_mae: 739763.8750\n",
      "Epoch 593/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1434308469244.5569 - mae: 740743.8750 - val_loss: 1443688560042.2258 - val_mae: 740775.2500\n",
      "Epoch 594/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1434126411351.0715 - mae: 740863.2500 - val_loss: 1443217381577.3870 - val_mae: 737369.5625\n",
      "Epoch 595/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1433803823776.4209 - mae: 740610.0000 - val_loss: 1443239397260.0378 - val_mae: 737577.8125\n",
      "Epoch 596/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1433905807441.4331 - mae: 740610.1875 - val_loss: 1443053615927.3115 - val_mae: 737305.3125\n",
      "Epoch 597/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1433700690373.2705 - mae: 740342.1875 - val_loss: 1443374286321.6794 - val_mae: 741906.6875\n",
      "Epoch 598/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1433549868067.2278 - mae: 740550.2500 - val_loss: 1443214015526.8203 - val_mae: 741962.3750\n",
      "Epoch 599/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1433319161959.7871 - mae: 740629.0000 - val_loss: 1442570459588.0730 - val_mae: 737620.5000\n",
      "Epoch 600/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1433276010521.1484 - mae: 740402.1250 - val_loss: 1442218769671.9587 - val_mae: 738687.6250\n",
      "Epoch 601/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1433038871554.8940 - mae: 740498.8750 - val_loss: 1442272325574.9170 - val_mae: 737352.7500\n",
      "Epoch 602/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1433035132103.6904 - mae: 740342.5000 - val_loss: 1442103616635.2473 - val_mae: 738354.1875\n",
      "Epoch 603/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1432899134831.5464 - mae: 740353.1875 - val_loss: 1441909635395.5364 - val_mae: 737607.3750\n",
      "Epoch 604/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1432699667010.0146 - mae: 740310.3750 - val_loss: 1441930672362.1199 - val_mae: 736991.2500\n",
      "Epoch 605/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1432469254157.1731 - mae: 740317.1250 - val_loss: 1442446052058.3022 - val_mae: 734859.6250\n",
      "Epoch 606/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1432471007438.3767 - mae: 740106.5625 - val_loss: 1441455296859.6870 - val_mae: 738045.2500\n",
      "Epoch 607/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1432226811277.6846 - mae: 740118.3125 - val_loss: 1441515743680.4802 - val_mae: 739539.3125\n",
      "Epoch 608/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1432184621546.7935 - mae: 740322.4375 - val_loss: 1441374368969.0876 - val_mae: 735791.1875\n",
      "Epoch 609/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1432056077035.4670 - mae: 740019.0625 - val_loss: 1441072846378.7622 - val_mae: 736744.1250\n",
      "Epoch 610/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1431766239732.3738 - mae: 740039.6875 - val_loss: 1440884051009.5654 - val_mae: 737075.2500\n",
      "Epoch 611/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1431615832657.9819 - mae: 740181.7500 - val_loss: 1441426192584.5886 - val_mae: 734505.4375\n",
      "Epoch 612/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1431702851605.9551 - mae: 739916.6250 - val_loss: 1440605190494.6809 - val_mae: 737322.1250\n",
      "Epoch 613/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1431431253576.5012 - mae: 739940.6875 - val_loss: 1440649355129.0767 - val_mae: 739445.0000\n",
      "Epoch 614/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1431205337607.1355 - mae: 740109.9375 - val_loss: 1442415895866.8542 - val_mae: 732528.9375\n",
      "Epoch 615/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1431266148759.1650 - mae: 739595.7500 - val_loss: 1440279232141.5596 - val_mae: 738442.8750\n",
      "Epoch 616/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1431083817266.7708 - mae: 739863.6875 - val_loss: 1440214273587.7439 - val_mae: 738978.6250\n",
      "Epoch 617/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1430941046966.2263 - mae: 739900.0625 - val_loss: 1440546964335.7957 - val_mae: 734824.1250\n",
      "Epoch 618/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1430779711308.7676 - mae: 739668.4375 - val_loss: 1439939648961.9773 - val_mae: 736098.5625\n",
      "Epoch 619/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1430646655642.4333 - mae: 739765.6250 - val_loss: 1439821650662.6770 - val_mae: 735902.0625\n",
      "Epoch 620/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1430480077469.0281 - mae: 739542.0000 - val_loss: 1439946196277.2659 - val_mae: 739736.1250\n",
      "Epoch 621/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1430350917726.6060 - mae: 739690.6875 - val_loss: 1439333412259.2402 - val_mae: 736232.5000\n",
      "Epoch 622/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1430149489509.4170 - mae: 739523.8750 - val_loss: 1439266197078.8718 - val_mae: 737457.8750\n",
      "Epoch 623/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1430006990431.8535 - mae: 739540.9375 - val_loss: 1439366241420.2124 - val_mae: 737178.5000\n",
      "Epoch 624/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1430000185603.0688 - mae: 739467.3750 - val_loss: 1439061672871.6812 - val_mae: 737529.7500\n",
      "Epoch 625/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1429842573847.9009 - mae: 739347.3750 - val_loss: 1439618108799.3140 - val_mae: 740690.3125\n",
      "Epoch 626/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1429638112835.9106 - mae: 739411.2500 - val_loss: 1438971737597.2556 - val_mae: 739121.1875\n",
      "Epoch 627/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1429578029046.6191 - mae: 739381.9375 - val_loss: 1438910547130.3179 - val_mae: 739240.6875\n",
      "Epoch 628/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1429457852392.2488 - mae: 739539.5625 - val_loss: 1438635114056.9004 - val_mae: 736190.7500\n",
      "Epoch 629/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1429307745205.2534 - mae: 739287.0625 - val_loss: 1438329216905.0439 - val_mae: 736882.1250\n",
      "Epoch 630/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1429044391337.6272 - mae: 739293.1250 - val_loss: 1438349399406.3486 - val_mae: 735764.5000\n",
      "Epoch 631/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1429002913910.7563 - mae: 739207.6875 - val_loss: 1438013902745.4104 - val_mae: 736605.6875\n",
      "Epoch 632/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1428829663684.2725 - mae: 739060.5000 - val_loss: 1438569481493.3313 - val_mae: 740364.0000\n",
      "Epoch 633/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1428880855038.0042 - mae: 739283.9375 - val_loss: 1437801659168.9575 - val_mae: 737006.8750\n",
      "Epoch 634/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1428597457686.3792 - mae: 739173.0000 - val_loss: 1438197103974.1655 - val_mae: 734132.8750\n",
      "Epoch 635/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1428479650963.7969 - mae: 739094.6250 - val_loss: 1437865169677.9963 - val_mae: 734862.1875\n",
      "Epoch 636/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1428408663987.3572 - mae: 738797.8750 - val_loss: 1437957054860.7864 - val_mae: 739655.1250\n",
      "Epoch 637/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1428253416305.5920 - mae: 739056.4375 - val_loss: 1437445862724.0354 - val_mae: 736609.5000\n",
      "Epoch 638/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1428162619498.7810 - mae: 738912.7500 - val_loss: 1437262574735.6055 - val_mae: 736390.5000\n",
      "Epoch 639/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1428016582425.6724 - mae: 738884.1875 - val_loss: 1437174956354.5386 - val_mae: 736409.0000\n",
      "Epoch 640/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1427862913276.2827 - mae: 738835.6875 - val_loss: 1437015737769.8267 - val_mae: 735908.0000\n",
      "Epoch 641/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1427756521299.7532 - mae: 738822.5625 - val_loss: 1436858479804.3137 - val_mae: 735578.6875\n",
      "Epoch 642/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1427507240101.7600 - mae: 738823.0625 - val_loss: 1437251792929.9304 - val_mae: 733901.8750\n",
      "Epoch 643/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1427412952492.2217 - mae: 738570.6250 - val_loss: 1436622580011.9849 - val_mae: 737142.4375\n",
      "Epoch 644/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1427260116166.0938 - mae: 738723.8750 - val_loss: 1436769556649.4524 - val_mae: 738575.8125\n",
      "Epoch 645/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1427260059418.4707 - mae: 738707.1875 - val_loss: 1436491624464.3665 - val_mae: 737172.6875\n",
      "Epoch 646/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1426881635061.6462 - mae: 738530.0625 - val_loss: 1436560837199.3872 - val_mae: 738935.8750\n",
      "Epoch 647/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1426974830862.2458 - mae: 738674.1875 - val_loss: 1436139715624.4170 - val_mae: 735547.6875\n",
      "Epoch 648/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1426699820627.2793 - mae: 738624.6250 - val_loss: 1436598524946.8613 - val_mae: 733494.0625\n",
      "Epoch 649/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1426606998533.8879 - mae: 738229.9375 - val_loss: 1437190418456.3501 - val_mae: 741512.9375\n",
      "Epoch 650/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1426507502498.4917 - mae: 738478.0000 - val_loss: 1435953890911.8535 - val_mae: 738372.0625\n",
      "Epoch 651/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1426431601789.4429 - mae: 738533.5625 - val_loss: 1435678844501.4746 - val_mae: 736472.7500\n",
      "Epoch 652/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1426289087514.2461 - mae: 738349.6875 - val_loss: 1435515380741.0896 - val_mae: 735581.6875\n",
      "Epoch 653/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1426093334468.1228 - mae: 738263.7500 - val_loss: 1435509119584.5520 - val_mae: 737707.1875\n",
      "Epoch 654/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1425993887979.1179 - mae: 738406.7500 - val_loss: 1435301835392.6860 - val_mae: 734791.8750\n",
      "Epoch 655/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1425893075695.8582 - mae: 738291.7500 - val_loss: 1435356049689.4229 - val_mae: 734256.5000\n",
      "Epoch 656/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1425701169841.5859 - mae: 738122.0000 - val_loss: 1434885537589.2158 - val_mae: 735440.3750\n",
      "Epoch 657/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1425670113825.0823 - mae: 738083.6250 - val_loss: 1434747530518.7283 - val_mae: 736189.6875\n",
      "Epoch 658/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1425468435653.2954 - mae: 738185.8750 - val_loss: 1434726553276.0642 - val_mae: 736404.2500\n",
      "Epoch 659/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1425247009714.2595 - mae: 738067.2500 - val_loss: 1434685480981.5559 - val_mae: 737141.4375\n",
      "Epoch 660/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1425239017324.1033 - mae: 738017.9375 - val_loss: 1434705475133.1248 - val_mae: 737482.3750\n",
      "Epoch 661/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1425106774796.5991 - mae: 738035.9375 - val_loss: 1434479648781.5723 - val_mae: 735553.4375\n",
      "Epoch 662/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1424954776866.3047 - mae: 737960.5625 - val_loss: 1434144891117.1138 - val_mae: 735626.3125\n",
      "Epoch 663/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1424760640632.4529 - mae: 737960.3750 - val_loss: 1434444693191.0417 - val_mae: 733433.5625\n",
      "Epoch 664/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1424784199543.8792 - mae: 737829.0000 - val_loss: 1434054682732.3777 - val_mae: 734903.6250\n",
      "Epoch 665/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1424656052725.7710 - mae: 737755.7500 - val_loss: 1433834008860.6162 - val_mae: 734955.3750\n",
      "Epoch 666/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1424402290688.1995 - mae: 737717.6875 - val_loss: 1433824266791.4690 - val_mae: 736646.5000\n",
      "Epoch 667/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1424196810288.3508 - mae: 737728.6875 - val_loss: 1433662388062.1318 - val_mae: 737055.4375\n",
      "Epoch 668/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1424229317951.8440 - mae: 737741.3750 - val_loss: 1433530592668.8533 - val_mae: 734601.7500\n",
      "Epoch 669/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1424052561610.0356 - mae: 737746.3750 - val_loss: 1433460216661.8489 - val_mae: 734070.1250\n",
      "Epoch 670/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1423847435084.4680 - mae: 737695.0625 - val_loss: 1433973843670.8096 - val_mae: 732442.8750\n",
      "Epoch 671/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1423893427338.8152 - mae: 737516.6875 - val_loss: 1433240753107.9902 - val_mae: 734242.8750\n",
      "Epoch 672/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1423706803575.3303 - mae: 737547.4375 - val_loss: 1433097758742.1545 - val_mae: 735310.0625\n",
      "Epoch 673/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1423563831646.6807 - mae: 737482.1875 - val_loss: 1433080882969.8718 - val_mae: 735792.2500\n",
      "Epoch 674/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1423499509046.2637 - mae: 737545.3750 - val_loss: 1432876734593.5344 - val_mae: 735268.1250\n",
      "Epoch 675/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1423332077518.9006 - mae: 737391.3750 - val_loss: 1432805350682.3210 - val_mae: 735843.2500\n",
      "Epoch 676/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1423120780714.4255 - mae: 737456.3125 - val_loss: 1432639159523.2339 - val_mae: 736228.5000\n",
      "Epoch 677/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1423091811464.8193 - mae: 737475.1250 - val_loss: 1432858906029.6189 - val_mae: 732933.0625\n",
      "Epoch 678/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1422943609854.7026 - mae: 737151.7500 - val_loss: 1432538287931.2036 - val_mae: 737222.2500\n",
      "Epoch 679/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1422862058695.0918 - mae: 737360.5000 - val_loss: 1432256447460.3567 - val_mae: 735092.6250\n",
      "Epoch 680/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1422524933287.9556 - mae: 737105.6875 - val_loss: 1432669251673.2170 - val_mae: 738447.1875\n",
      "Epoch 681/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1422512024477.9011 - mae: 737356.5625 - val_loss: 1432544222327.4551 - val_mae: 732362.5625\n",
      "Epoch 682/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1422420041064.5605 - mae: 737054.1875 - val_loss: 1432268061172.7729 - val_mae: 737441.0000\n",
      "Epoch 683/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1422257079292.3076 - mae: 737255.7500 - val_loss: 1432110062166.9717 - val_mae: 732924.8125\n",
      "Epoch 684/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1422212145192.7163 - mae: 736945.5000 - val_loss: 1431713930111.9626 - val_mae: 735858.6250\n",
      "Epoch 685/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1422066496307.5193 - mae: 737019.3125 - val_loss: 1431657094509.3508 - val_mae: 734076.9375\n",
      "Epoch 686/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1421865635504.2883 - mae: 737067.1875 - val_loss: 1432194995366.7581 - val_mae: 731692.9375\n",
      "Epoch 687/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1421809933988.7122 - mae: 736751.0000 - val_loss: 1431301187334.7112 - val_mae: 735822.4375\n",
      "Epoch 688/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1421739116729.8188 - mae: 736930.6250 - val_loss: 1431621238512.5566 - val_mae: 737325.0000\n",
      "Epoch 689/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1421295148114.7302 - mae: 736874.3750 - val_loss: 1431192603207.4036 - val_mae: 735268.4375\n",
      "Epoch 690/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1421458283553.2319 - mae: 736806.7500 - val_loss: 1430956493279.6165 - val_mae: 733870.6250\n",
      "Epoch 691/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1421280141708.7861 - mae: 736747.4375 - val_loss: 1430830711901.7078 - val_mae: 734684.9375\n",
      "Epoch 692/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1421211641870.1709 - mae: 736808.5000 - val_loss: 1430642257593.0706 - val_mae: 734830.4375\n",
      "Epoch 693/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1421061502365.4521 - mae: 736683.8125 - val_loss: 1430742687888.5037 - val_mae: 736171.7500\n",
      "Epoch 694/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1420934357639.4722 - mae: 736764.5625 - val_loss: 1430326507041.9802 - val_mae: 734848.0625\n",
      "Epoch 695/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1420773373817.2764 - mae: 736645.4375 - val_loss: 1430558619427.0532 - val_mae: 734010.4375\n",
      "Epoch 696/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1420686086025.4431 - mae: 736497.3125 - val_loss: 1430479567203.4709 - val_mae: 736278.3125\n",
      "Epoch 697/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1420542542446.7229 - mae: 736570.1250 - val_loss: 1430139026218.5378 - val_mae: 735008.9375\n",
      "Epoch 698/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1420393983559.7029 - mae: 736466.3750 - val_loss: 1430054505312.4272 - val_mae: 735888.3750\n",
      "Epoch 699/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1420336699264.4614 - mae: 736411.3125 - val_loss: 1430030376529.6824 - val_mae: 736043.7500\n",
      "Epoch 700/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1420181623128.4934 - mae: 736541.5000 - val_loss: 1429893144522.5098 - val_mae: 733451.5000\n",
      "Epoch 701/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1420111452763.7617 - mae: 736405.1250 - val_loss: 1429787684171.4204 - val_mae: 733778.0625\n",
      "Epoch 702/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1420009259164.4790 - mae: 736333.5625 - val_loss: 1429553409564.5913 - val_mae: 734464.2500\n",
      "Epoch 703/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1419886316347.7026 - mae: 736222.2500 - val_loss: 1429515660898.8472 - val_mae: 736330.0000\n",
      "Epoch 704/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1419704341170.9829 - mae: 736335.0625 - val_loss: 1429324573570.7568 - val_mae: 734254.5000\n",
      "Epoch 705/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1419590322109.5364 - mae: 736214.8125 - val_loss: 1429410535898.4270 - val_mae: 735922.3125\n",
      "Epoch 706/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1419482616960.4365 - mae: 736200.8750 - val_loss: 1429100777487.5681 - val_mae: 735630.8125\n",
      "Epoch 707/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1419377969289.9172 - mae: 736270.5000 - val_loss: 1428936916736.5239 - val_mae: 734360.6875\n",
      "Epoch 708/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1419184317705.3560 - mae: 736243.1875 - val_loss: 1429268418377.5742 - val_mae: 732002.8750\n",
      "Epoch 709/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1419065722821.5200 - mae: 735919.8125 - val_loss: 1428814934025.9795 - val_mae: 735679.5625\n",
      "Epoch 710/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1419026163555.3213 - mae: 736154.5000 - val_loss: 1428652175577.5540 - val_mae: 734521.9375\n",
      "Epoch 711/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1418909644111.2126 - mae: 736003.6875 - val_loss: 1428726005218.7100 - val_mae: 732598.5625\n",
      "Epoch 712/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1418695966122.8247 - mae: 735824.6875 - val_loss: 1428805102725.4263 - val_mae: 736398.8750\n",
      "Epoch 713/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1418655421217.1570 - mae: 736061.8125 - val_loss: 1428561524542.1975 - val_mae: 735385.3125\n",
      "Epoch 714/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1418383365987.7205 - mae: 736068.6875 - val_loss: 1428217794289.7542 - val_mae: 733553.8125\n",
      "Epoch 715/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1418437376264.4575 - mae: 735820.1250 - val_loss: 1428397118298.6392 - val_mae: 732157.5000\n",
      "Epoch 716/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1418302613736.9221 - mae: 735704.8125 - val_loss: 1427867625593.1516 - val_mae: 734517.9375\n",
      "Epoch 717/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1418063269443.7112 - mae: 735917.3750 - val_loss: 1428045092335.6833 - val_mae: 732576.9375\n",
      "Epoch 718/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1418059535242.5408 - mae: 735650.0000 - val_loss: 1427746967187.5474 - val_mae: 734552.1250\n",
      "Epoch 719/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1417908089648.3259 - mae: 735770.2500 - val_loss: 1427638393707.2051 - val_mae: 733518.6250\n",
      "Epoch 720/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1417794777645.6565 - mae: 735747.0000 - val_loss: 1427930673524.4363 - val_mae: 731410.3125\n",
      "Epoch 721/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1417586399674.4924 - mae: 735507.9375 - val_loss: 1427420760958.7651 - val_mae: 733276.4375\n",
      "Epoch 722/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1417388868878.9443 - mae: 735472.3750 - val_loss: 1427385674040.7585 - val_mae: 735858.1250\n",
      "Epoch 723/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1417443034673.5483 - mae: 735761.5000 - val_loss: 1427250730722.3857 - val_mae: 733014.7500\n",
      "Epoch 724/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1417194409506.1799 - mae: 735373.6250 - val_loss: 1427878095104.8733 - val_mae: 737446.3125\n",
      "Epoch 725/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1417262760641.7527 - mae: 735641.5000 - val_loss: 1426972226385.8572 - val_mae: 734130.4375\n",
      "Epoch 726/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1417088587832.0850 - mae: 735352.6875 - val_loss: 1426804717602.5293 - val_mae: 734471.6875\n",
      "Epoch 727/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1416958363613.4709 - mae: 735511.5625 - val_loss: 1426719209191.9744 - val_mae: 734451.6875\n",
      "Epoch 728/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1416882161441.0574 - mae: 735468.4375 - val_loss: 1426819549157.4543 - val_mae: 731773.6875\n",
      "Epoch 729/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1416643335963.8679 - mae: 735267.5625 - val_loss: 1426686649551.8738 - val_mae: 733349.1875\n",
      "Epoch 730/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1416503297060.2256 - mae: 735502.4375 - val_loss: 1427740305304.0132 - val_mae: 729605.1875\n",
      "Epoch 731/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1416497086548.6265 - mae: 735022.4375 - val_loss: 1426483518046.9553 - val_mae: 735397.5625\n",
      "Epoch 732/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1416380310756.6311 - mae: 735320.6250 - val_loss: 1426059520226.5354 - val_mae: 733310.1875\n",
      "Epoch 733/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1416254879103.1145 - mae: 735194.8125 - val_loss: 1426066771199.5759 - val_mae: 733156.0625\n",
      "Epoch 734/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1416142686938.3022 - mae: 735251.1250 - val_loss: 1426173966667.5200 - val_mae: 732137.0625\n",
      "Epoch 735/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1416042925926.3152 - mae: 735087.4375 - val_loss: 1425880278033.8633 - val_mae: 732372.6875\n",
      "Epoch 736/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1415891335068.4043 - mae: 735011.2500 - val_loss: 1425941273419.2708 - val_mae: 735267.5000\n",
      "Epoch 737/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1415737830068.7793 - mae: 734955.8125 - val_loss: 1426382605815.0684 - val_mae: 737154.9375\n",
      "Epoch 738/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1415691930908.3169 - mae: 735230.1875 - val_loss: 1425549601836.0098 - val_mae: 732513.5000\n",
      "Epoch 739/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1415546637181.1685 - mae: 734938.9375 - val_loss: 1425591823587.6331 - val_mae: 731689.5625\n",
      "Epoch 740/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1415443575619.4866 - mae: 734767.6250 - val_loss: 1425755480430.4485 - val_mae: 736083.9375\n",
      "Epoch 741/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1415188256733.0718 - mae: 735105.0000 - val_loss: 1425533030156.8984 - val_mae: 731157.3750\n",
      "Epoch 742/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1415231052600.1099 - mae: 734852.5625 - val_loss: 1425052606689.9368 - val_mae: 732562.3750\n",
      "Epoch 743/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1414997649613.7781 - mae: 734678.8125 - val_loss: 1425021833344.9355 - val_mae: 733987.0000\n",
      "Epoch 744/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1414958845672.1738 - mae: 734899.2500 - val_loss: 1424947760158.4375 - val_mae: 731834.3750\n",
      "Epoch 745/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1414839960723.9966 - mae: 734733.0625 - val_loss: 1424812848103.9492 - val_mae: 732615.9375\n",
      "Epoch 746/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1414743053172.1868 - mae: 734817.3125 - val_loss: 1424625227400.9690 - val_mae: 732341.3750\n",
      "Epoch 747/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1414643192974.8071 - mae: 734653.7500 - val_loss: 1424655452851.9810 - val_mae: 734343.3125\n",
      "Epoch 748/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1414487759357.5552 - mae: 734579.0625 - val_loss: 1424913049122.3794 - val_mae: 736362.6875\n",
      "Epoch 749/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1414416170934.5505 - mae: 734841.8125 - val_loss: 1424544555803.9675 - val_mae: 730860.1250\n",
      "Epoch 750/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1414273687086.5544 - mae: 734512.8750 - val_loss: 1424200355545.8032 - val_mae: 733334.5625\n",
      "Epoch 751/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1414156955485.5332 - mae: 734635.3125 - val_loss: 1423990581528.8240 - val_mae: 732524.7500\n",
      "Epoch 752/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1414110964423.9399 - mae: 734572.3750 - val_loss: 1424154125393.7324 - val_mae: 731652.1250\n",
      "Epoch 753/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1413803455813.6323 - mae: 734494.6875 - val_loss: 1424328695644.3357 - val_mae: 731065.2500\n",
      "Epoch 754/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1413874937538.6506 - mae: 734412.5000 - val_loss: 1423658420989.4302 - val_mae: 733196.2500\n",
      "Epoch 755/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1413710909798.8640 - mae: 734469.0000 - val_loss: 1423759301159.0698 - val_mae: 732848.0625\n",
      "Epoch 756/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1413628598422.2917 - mae: 734524.5000 - val_loss: 1423885287601.2366 - val_mae: 730683.0625\n",
      "Epoch 757/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1413537628894.4937 - mae: 734272.6250 - val_loss: 1423495641317.3296 - val_mae: 733750.3125\n",
      "Epoch 758/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1413377119057.0588 - mae: 734433.3125 - val_loss: 1423467127255.8323 - val_mae: 732177.4375\n",
      "Epoch 759/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1413248505650.2219 - mae: 734280.8125 - val_loss: 1423678582548.0837 - val_mae: 730607.0000\n",
      "Epoch 760/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1413088990467.3682 - mae: 734125.6250 - val_loss: 1423481587489.0571 - val_mae: 734875.7500\n",
      "Epoch 761/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1412937436953.6724 - mae: 734404.6250 - val_loss: 1423185926509.8496 - val_mae: 732990.6250\n",
      "Epoch 762/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1412907939941.5918 - mae: 734240.0625 - val_loss: 1422946248871.4565 - val_mae: 731443.9375\n",
      "Epoch 763/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1412816805005.8091 - mae: 734174.1250 - val_loss: 1422782320981.3000 - val_mae: 731582.0000\n",
      "Epoch 764/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1412648586573.8154 - mae: 734042.8125 - val_loss: 1422736732214.6877 - val_mae: 732646.7500\n",
      "Epoch 765/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1412461095844.2881 - mae: 733990.1250 - val_loss: 1422685725879.9229 - val_mae: 733756.3750\n",
      "Epoch 766/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1412479828748.7986 - mae: 734166.3750 - val_loss: 1422491930211.6458 - val_mae: 731820.0625\n",
      "Epoch 767/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1412369478968.8584 - mae: 734102.8750 - val_loss: 1422350531089.5142 - val_mae: 731724.0625\n",
      "Epoch 768/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1412196668312.9114 - mae: 733811.1875 - val_loss: 1422623004970.7874 - val_mae: 735146.0625\n",
      "Epoch 769/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1412059802061.2542 - mae: 733978.3750 - val_loss: 1422671506963.0110 - val_mae: 735548.5000\n",
      "Epoch 770/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1412020713053.0593 - mae: 734073.5000 - val_loss: 1421968457525.2158 - val_mae: 731276.6875\n",
      "Epoch 771/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1411795553414.2246 - mae: 734003.0000 - val_loss: 1422282033815.2397 - val_mae: 730292.5000\n",
      "Epoch 772/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1411828064146.6243 - mae: 733687.7500 - val_loss: 1421727468866.6384 - val_mae: 733495.8125\n",
      "Epoch 773/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1411700375991.0996 - mae: 734010.0000 - val_loss: 1421770307899.5530 - val_mae: 731350.6250\n",
      "Epoch 774/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1411559366377.3716 - mae: 733651.8750 - val_loss: 1421639659331.9856 - val_mae: 733228.6250\n",
      "Epoch 775/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1411436089009.8853 - mae: 733790.5000 - val_loss: 1421509321519.1284 - val_mae: 732665.7500\n",
      "Epoch 776/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1411327013653.1816 - mae: 733662.0625 - val_loss: 1421486720764.3325 - val_mae: 733696.8125\n",
      "Epoch 777/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1411230162379.5576 - mae: 733696.8750 - val_loss: 1421625913020.2639 - val_mae: 734226.2500\n",
      "Epoch 778/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1411046502577.6357 - mae: 733813.1875 - val_loss: 1421117062002.3904 - val_mae: 732005.0625\n",
      "Epoch 779/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1410928809786.2056 - mae: 733727.1875 - val_loss: 1422295036225.3411 - val_mae: 728425.0000\n",
      "Epoch 780/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1410947596299.1772 - mae: 733447.3125 - val_loss: 1420996311203.8640 - val_mae: 733201.4375\n",
      "Epoch 781/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1410805821333.4187 - mae: 733658.6875 - val_loss: 1420916699398.2622 - val_mae: 731224.2500\n",
      "Epoch 782/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1410739052175.9548 - mae: 733442.1875 - val_loss: 1420811357307.0476 - val_mae: 732939.8750\n",
      "Epoch 783/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1410577804636.9841 - mae: 733607.7500 - val_loss: 1420761922629.1582 - val_mae: 731597.5625\n",
      "Epoch 784/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1410411410634.6843 - mae: 733558.0625 - val_loss: 1421328989452.7488 - val_mae: 728603.0000\n",
      "Epoch 785/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1410188445247.9189 - mae: 733315.5000 - val_loss: 1420414109370.8667 - val_mae: 731554.0625\n",
      "Epoch 786/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1410211126560.7078 - mae: 733391.2500 - val_loss: 1420519837793.0010 - val_mae: 732576.8750\n",
      "Epoch 787/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1410122791990.7876 - mae: 733482.1250 - val_loss: 1420297708735.9065 - val_mae: 731670.3750\n",
      "Epoch 788/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1409967203484.7783 - mae: 733352.5625 - val_loss: 1420533607998.1226 - val_mae: 729348.6875\n",
      "Epoch 789/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1409929671865.1204 - mae: 733263.3750 - val_loss: 1420005614662.5554 - val_mae: 730648.9375\n",
      "Epoch 790/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1409676397387.6699 - mae: 733168.5000 - val_loss: 1420108989211.1692 - val_mae: 733958.3125\n",
      "Epoch 791/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1409666118866.6680 - mae: 733494.0625 - val_loss: 1420060629225.3215 - val_mae: 729401.1250\n",
      "Epoch 792/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1409522742315.0120 - mae: 733152.0000 - val_loss: 1419687345098.2102 - val_mae: 731955.4375\n",
      "Epoch 793/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1409352133179.4282 - mae: 733139.6875 - val_loss: 1419570000044.8455 - val_mae: 732137.3125\n",
      "Epoch 794/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1409305770109.5425 - mae: 733227.3750 - val_loss: 1419808436005.6477 - val_mae: 729050.0000\n",
      "Epoch 795/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1409249875093.1941 - mae: 733154.2500 - val_loss: 1419778509448.8694 - val_mae: 729381.3750\n",
      "Epoch 796/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1409060648327.0981 - mae: 732798.8125 - val_loss: 1419859566980.8027 - val_mae: 735006.1250\n",
      "Epoch 797/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1409048097739.2083 - mae: 733092.3125 - val_loss: 1419493703814.1250 - val_mae: 733846.8750\n",
      "Epoch 798/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1408839690120.0459 - mae: 733257.3750 - val_loss: 1419799106217.4026 - val_mae: 728434.7500\n",
      "Epoch 799/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1408799511908.7683 - mae: 732718.7500 - val_loss: 1419208386077.5894 - val_mae: 734061.7500\n",
      "Epoch 800/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1408751565972.9944 - mae: 732988.6875 - val_loss: 1418821017541.7195 - val_mae: 732650.2500\n",
      "Epoch 801/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1408544638649.2700 - mae: 733080.8125 - val_loss: 1419186646270.7776 - val_mae: 729328.5625\n",
      "Epoch 802/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1408494700137.4338 - mae: 732623.3750 - val_loss: 1418861228990.4346 - val_mae: 733785.2500\n",
      "Epoch 803/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1408405562025.0034 - mae: 732917.2500 - val_loss: 1418767705292.6802 - val_mae: 733467.2500\n",
      "Epoch 804/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1408221799936.8481 - mae: 733012.5000 - val_loss: 1418502401813.6804 - val_mae: 730741.8750\n",
      "Epoch 805/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1408193364616.2705 - mae: 732725.8750 - val_loss: 1418397637149.4895 - val_mae: 731767.0625\n",
      "Epoch 806/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1407980714583.7700 - mae: 732865.1250 - val_loss: 1418446398175.2920 - val_mae: 729678.5625\n",
      "Epoch 807/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1407940047528.6042 - mae: 732726.5625 - val_loss: 1418138841687.1711 - val_mae: 730627.3125\n",
      "Epoch 808/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1407785248191.3826 - mae: 732656.3750 - val_loss: 1417914076436.3333 - val_mae: 730491.2500\n",
      "Epoch 809/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1407672605970.3374 - mae: 732723.7500 - val_loss: 1417866588716.0596 - val_mae: 731107.4375\n",
      "Epoch 810/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1407605917239.9353 - mae: 732621.8750 - val_loss: 1417850973443.5676 - val_mae: 730973.3750\n",
      "Epoch 811/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1407358933819.3035 - mae: 732635.6875 - val_loss: 1417680548177.3083 - val_mae: 731367.7500\n",
      "Epoch 812/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1407382144644.5781 - mae: 732508.0000 - val_loss: 1417602255630.9941 - val_mae: 732348.6875\n",
      "Epoch 813/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1407254713252.0884 - mae: 732675.4375 - val_loss: 1417671078675.5847 - val_mae: 729607.6875\n",
      "Epoch 814/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1407069179667.2854 - mae: 732281.4375 - val_loss: 1418002560415.0488 - val_mae: 734558.8125\n",
      "Epoch 815/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1407112923072.7297 - mae: 732705.3125 - val_loss: 1417418130575.2063 - val_mae: 729746.6875\n",
      "Epoch 816/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1406963911309.6594 - mae: 732365.5000 - val_loss: 1417217602837.7305 - val_mae: 730197.3750\n",
      "Epoch 817/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1406838674539.6792 - mae: 732446.0000 - val_loss: 1417207981819.5342 - val_mae: 730922.0000\n",
      "Epoch 818/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1406714438958.0806 - mae: 732298.8750 - val_loss: 1417157552087.8823 - val_mae: 729160.7500\n",
      "Epoch 819/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1406635679359.9875 - mae: 732318.3125 - val_loss: 1416904995249.1118 - val_mae: 730534.8125\n",
      "Epoch 820/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1406540353323.2363 - mae: 732368.2500 - val_loss: 1416746969022.3347 - val_mae: 730655.0625\n",
      "Epoch 821/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1406353968865.7869 - mae: 732267.5625 - val_loss: 1416723513866.1292 - val_mae: 731441.5000\n",
      "Epoch 822/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1406264375970.8159 - mae: 732249.0625 - val_loss: 1416528521203.0266 - val_mae: 730984.1250\n",
      "Epoch 823/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1406073057286.7859 - mae: 732098.5625 - val_loss: 1416785931863.6702 - val_mae: 733404.4375\n",
      "Epoch 824/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1406067908252.7285 - mae: 732210.9375 - val_loss: 1416450811582.8586 - val_mae: 731838.5625\n",
      "Epoch 825/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1405930351340.6646 - mae: 732203.7500 - val_loss: 1416294166217.0376 - val_mae: 732284.8125\n",
      "Epoch 826/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1405811637088.8264 - mae: 732250.8125 - val_loss: 1416408911705.5413 - val_mae: 728660.9375\n",
      "Epoch 827/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1405674710289.3396 - mae: 731944.5625 - val_loss: 1416302317634.6633 - val_mae: 732984.2500\n",
      "Epoch 828/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1405640096386.8816 - mae: 732179.2500 - val_loss: 1415959602714.3958 - val_mae: 730172.9375\n",
      "Epoch 829/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1405494603003.5840 - mae: 732005.5625 - val_loss: 1415948421382.4617 - val_mae: 729229.6875\n",
      "Epoch 830/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1405389138948.0916 - mae: 731855.6250 - val_loss: 1415698870666.7903 - val_mae: 731926.5625\n",
      "Epoch 831/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1405197187695.7209 - mae: 731975.0000 - val_loss: 1415805014537.7300 - val_mae: 732810.6875\n",
      "Epoch 832/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1405160925939.0518 - mae: 731995.3125 - val_loss: 1415590266835.8904 - val_mae: 729681.6875\n",
      "Epoch 833/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1405051956678.3682 - mae: 731897.0625 - val_loss: 1415518131582.3159 - val_mae: 729468.3125\n",
      "Epoch 834/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1404929868504.1067 - mae: 731775.0000 - val_loss: 1415318635182.5920 - val_mae: 731329.8125\n",
      "Epoch 835/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1404758983722.6125 - mae: 731733.8125 - val_loss: 1415672219492.1196 - val_mae: 733520.2500\n",
      "Epoch 836/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1404779816622.8914 - mae: 731898.1250 - val_loss: 1415287498754.9939 - val_mae: 731935.8750\n",
      "Epoch 837/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1404591252403.6565 - mae: 731879.4375 - val_loss: 1415443081142.3511 - val_mae: 728532.8750\n",
      "Epoch 838/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1404545989471.2297 - mae: 731570.7500 - val_loss: 1414907427714.3577 - val_mae: 731212.2500\n",
      "Epoch 839/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1404360694794.8777 - mae: 731856.5000 - val_loss: 1415536648034.7227 - val_mae: 727313.8125\n",
      "Epoch 840/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1404294790577.2117 - mae: 731573.5000 - val_loss: 1414718833449.5398 - val_mae: 729813.6250\n",
      "Epoch 841/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1404214897829.3608 - mae: 731678.3125 - val_loss: 1414572943958.5725 - val_mae: 729743.5625\n",
      "Epoch 842/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1404106557044.6108 - mae: 731548.8750 - val_loss: 1414415909943.3865 - val_mae: 731364.3125\n",
      "Epoch 843/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1403957262527.0083 - mae: 731594.3125 - val_loss: 1414267588160.5176 - val_mae: 729752.0625\n",
      "Epoch 844/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1403890881584.6003 - mae: 731436.1875 - val_loss: 1414138526214.3369 - val_mae: 729603.0625\n",
      "Epoch 845/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1403730634893.9089 - mae: 731509.5625 - val_loss: 1414037949888.7795 - val_mae: 730640.7500\n",
      "Epoch 846/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1403674326676.9443 - mae: 731475.6250 - val_loss: 1414104540564.2710 - val_mae: 731690.6250\n",
      "Epoch 847/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1403446647558.6113 - mae: 731615.5000 - val_loss: 1414968556560.4663 - val_mae: 726619.5625\n",
      "Epoch 848/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1403446636771.1343 - mae: 731208.1250 - val_loss: 1413793984122.7981 - val_mae: 730389.1875\n",
      "Epoch 849/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1403276651113.7332 - mae: 731220.0000 - val_loss: 1414161380522.7498 - val_mae: 733221.8125\n",
      "Epoch 850/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1403219428527.1409 - mae: 731552.6250 - val_loss: 1413527354588.8472 - val_mae: 729899.3750\n",
      "Epoch 851/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1403091398749.0093 - mae: 731383.9375 - val_loss: 1413601980900.2070 - val_mae: 728635.0000\n",
      "Epoch 852/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1402936487200.3086 - mae: 731317.8125 - val_loss: 1413133967426.2642 - val_mae: 729750.8750\n",
      "Epoch 853/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1402747282168.8396 - mae: 731214.9375 - val_loss: 1413270399058.8301 - val_mae: 730760.6250\n",
      "Epoch 854/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1402785494641.8167 - mae: 731307.1875 - val_loss: 1413326107577.6443 - val_mae: 728367.8750\n",
      "Epoch 855/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1402553863126.5850 - mae: 731058.5000 - val_loss: 1413031290282.6250 - val_mae: 729552.3750\n",
      "Epoch 856/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1402417054299.5623 - mae: 731136.6250 - val_loss: 1412902295673.5508 - val_mae: 730389.0625\n",
      "Epoch 857/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1402418453235.4507 - mae: 731232.1875 - val_loss: 1413071193639.9680 - val_mae: 727833.7500\n",
      "Epoch 858/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1402263215258.5830 - mae: 730857.1875 - val_loss: 1413047021783.9570 - val_mae: 732479.6875\n",
      "Epoch 859/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1402178524729.1328 - mae: 731107.1250 - val_loss: 1412620466015.7288 - val_mae: 730624.3125\n",
      "Epoch 860/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1402076672316.5508 - mae: 731036.0000 - val_loss: 1412709328016.0046 - val_mae: 728378.3125\n",
      "Epoch 861/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1401946129500.1111 - mae: 730837.6875 - val_loss: 1412595984976.1855 - val_mae: 731502.1250\n",
      "Epoch 862/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1401831456266.6282 - mae: 730991.6250 - val_loss: 1412296819408.1233 - val_mae: 730731.0000\n",
      "Epoch 863/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1401808076539.7339 - mae: 730928.0000 - val_loss: 1412195989279.8596 - val_mae: 729815.0625\n",
      "Epoch 864/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1401705193868.7861 - mae: 730877.4375 - val_loss: 1412117947138.4199 - val_mae: 729933.8750\n",
      "Epoch 865/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1401553543673.3638 - mae: 730950.5000 - val_loss: 1412102761870.4829 - val_mae: 728925.5625\n",
      "Epoch 866/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1401480867206.4990 - mae: 730727.9375 - val_loss: 1412008640441.7441 - val_mae: 731450.3750\n",
      "Epoch 867/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1401308503585.7808 - mae: 730883.0625 - val_loss: 1411916026328.2314 - val_mae: 729900.3125\n",
      "Epoch 868/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1401199640092.1921 - mae: 730720.1250 - val_loss: 1411901776830.8337 - val_mae: 731405.2500\n",
      "Epoch 869/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1401066891983.9236 - mae: 730817.7500 - val_loss: 1411622572592.0515 - val_mae: 730599.0625\n",
      "Epoch 870/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1400938628636.7908 - mae: 730785.3750 - val_loss: 1411590130745.5820 - val_mae: 729611.9375\n",
      "Epoch 871/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1400815169577.9141 - mae: 730692.5000 - val_loss: 1411456660023.2368 - val_mae: 727880.8125\n",
      "Epoch 872/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1400787431403.6418 - mae: 730548.3750 - val_loss: 1411327141605.1799 - val_mae: 729604.4375\n",
      "Epoch 873/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1400620090314.8091 - mae: 730672.3750 - val_loss: 1411302134508.4651 - val_mae: 727967.1875\n",
      "Epoch 874/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1400563198699.0681 - mae: 730507.7500 - val_loss: 1411081273794.2766 - val_mae: 729063.6250\n",
      "Epoch 875/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1400334778775.1650 - mae: 730637.8750 - val_loss: 1411304934558.1755 - val_mae: 727105.9375\n",
      "Epoch 876/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1400395236263.6812 - mae: 730483.0625 - val_loss: 1411047695890.4124 - val_mae: 727113.6250\n",
      "Epoch 877/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1400207427424.9260 - mae: 730325.0625 - val_loss: 1411046150253.7749 - val_mae: 731198.7500\n",
      "Epoch 878/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1400164066633.2249 - mae: 730597.6250 - val_loss: 1410843652303.9734 - val_mae: 729483.4375\n",
      "Epoch 879/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1400071894582.7378 - mae: 730450.6875 - val_loss: 1410502012043.8132 - val_mae: 728213.5625\n",
      "Epoch 880/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1399833665761.9368 - mae: 730415.5625 - val_loss: 1410486737231.5120 - val_mae: 727831.2500\n",
      "Epoch 881/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1399913569771.4922 - mae: 730364.6250 - val_loss: 1410326831741.8918 - val_mae: 728110.5000\n",
      "Epoch 882/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1399722372492.8860 - mae: 730140.4375 - val_loss: 1410838054963.6941 - val_mae: 732365.3750\n",
      "Epoch 883/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1399597624266.6094 - mae: 730445.6250 - val_loss: 1410284035526.7673 - val_mae: 727475.9375\n",
      "Epoch 884/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1399450675242.9119 - mae: 730353.8125 - val_loss: 1410883143463.4441 - val_mae: 725930.4375\n",
      "Epoch 885/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1399476532295.7529 - mae: 730083.6875 - val_loss: 1409793248941.0950 - val_mae: 729551.9375\n",
      "Epoch 886/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1399255741205.1816 - mae: 730356.3750 - val_loss: 1409894356580.2444 - val_mae: 729511.8125\n",
      "Epoch 887/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1399126699716.2476 - mae: 730153.7500 - val_loss: 1409726215244.5430 - val_mae: 727970.0000\n",
      "Epoch 888/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1399102035826.0911 - mae: 730100.1875 - val_loss: 1409834775708.3794 - val_mae: 730284.9375\n",
      "Epoch 889/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1399015797303.2368 - mae: 730005.3750 - val_loss: 1410412381520.4102 - val_mae: 732621.3125\n",
      "Epoch 890/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1398827015946.9026 - mae: 730254.6875 - val_loss: 1409497110873.0923 - val_mae: 727590.5625\n",
      "Epoch 891/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1398709591731.0828 - mae: 730025.7500 - val_loss: 1409387553688.5122 - val_mae: 729383.3750\n",
      "Epoch 892/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1398703293073.8508 - mae: 729952.7500 - val_loss: 1409383086427.4873 - val_mae: 730755.8125\n",
      "Epoch 893/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1398642776140.8423 - mae: 730109.9375 - val_loss: 1409052659146.3601 - val_mae: 728958.1250\n",
      "Epoch 894/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1398440096256.9480 - mae: 729829.6250 - val_loss: 1409783967948.7800 - val_mae: 732394.8750\n",
      "Epoch 895/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1398409397469.4458 - mae: 730156.1250 - val_loss: 1409010830885.3733 - val_mae: 727624.8750\n",
      "Epoch 896/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1398143409748.8757 - mae: 729963.5000 - val_loss: 1409732370883.0750 - val_mae: 725722.3750\n",
      "Epoch 897/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1398188023542.1453 - mae: 729754.1875 - val_loss: 1408733485477.7351 - val_mae: 729508.9375\n",
      "Epoch 898/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1398006757499.9458 - mae: 729828.5625 - val_loss: 1408737314795.8413 - val_mae: 730038.6250\n",
      "Epoch 899/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1397914663008.8015 - mae: 729942.1250 - val_loss: 1408816101158.2466 - val_mae: 727778.8750\n",
      "Epoch 900/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1397806461677.2634 - mae: 729824.0000 - val_loss: 1408520511067.2629 - val_mae: 729438.2500\n",
      "Epoch 901/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1397745120699.7898 - mae: 729829.5625 - val_loss: 1408519222720.5801 - val_mae: 729537.5625\n",
      "Epoch 902/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1397496891743.8784 - mae: 729756.6875 - val_loss: 1409476846440.6104 - val_mae: 724655.3125\n",
      "Epoch 903/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1397623101836.1875 - mae: 729614.7500 - val_loss: 1408331975568.3291 - val_mae: 726711.4375\n",
      "Epoch 904/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1397461446635.6418 - mae: 729652.0625 - val_loss: 1408215096675.7705 - val_mae: 726834.1875\n",
      "Epoch 905/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1397253339405.2478 - mae: 729607.8750 - val_loss: 1408231096893.2244 - val_mae: 730372.8750\n",
      "Epoch 906/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1397194592886.3074 - mae: 729702.6250 - val_loss: 1407985152567.2368 - val_mae: 727476.6250\n",
      "Epoch 907/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1397082791820.2373 - mae: 729542.6875 - val_loss: 1407974941373.7607 - val_mae: 726496.6250\n",
      "Epoch 908/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1397033929322.5315 - mae: 729339.5625 - val_loss: 1407807686029.0857 - val_mae: 730393.1875\n",
      "Epoch 909/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1396950784145.5017 - mae: 729693.1875 - val_loss: 1407890975604.3862 - val_mae: 729297.2500\n",
      "Epoch 910/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1396745973942.4258 - mae: 729405.9375 - val_loss: 1408069137414.2871 - val_mae: 730796.8125\n",
      "Epoch 911/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1396678626629.8318 - mae: 729510.3750 - val_loss: 1407365490226.3467 - val_mae: 728843.5625\n",
      "Epoch 912/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1396649659880.6978 - mae: 729471.9375 - val_loss: 1407231134554.0403 - val_mae: 727567.3750\n",
      "Epoch 913/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1396508942202.6733 - mae: 729466.1250 - val_loss: 1407169422596.5657 - val_mae: 727813.2500\n",
      "Epoch 914/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1396428275261.7234 - mae: 729394.7500 - val_loss: 1407187305436.3730 - val_mae: 727557.9375\n",
      "Epoch 915/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1396307950407.2788 - mae: 729476.4375 - val_loss: 1407420385839.4526 - val_mae: 726150.6875\n",
      "Epoch 916/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1396172859885.1887 - mae: 729266.8125 - val_loss: 1407130978365.2744 - val_mae: 728695.8750\n",
      "Epoch 917/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1396127774355.3479 - mae: 729213.4375 - val_loss: 1407264004246.2917 - val_mae: 730716.9375\n",
      "Epoch 918/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1395946948766.6746 - mae: 729276.2500 - val_loss: 1406938633430.6597 - val_mae: 729817.1250\n",
      "Epoch 919/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1395962060243.5413 - mae: 729272.5000 - val_loss: 1406740192083.2542 - val_mae: 729323.1250\n",
      "Epoch 920/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1395688356433.3831 - mae: 729258.4375 - val_loss: 1406392574181.1301 - val_mae: 727271.7500\n",
      "Epoch 921/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1395608128796.3169 - mae: 729417.7500 - val_loss: 1407841454575.5837 - val_mae: 724069.0000\n",
      "Epoch 922/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1395632687724.1282 - mae: 729016.0000 - val_loss: 1406495112256.3679 - val_mae: 727883.8750\n",
      "Epoch 923/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1395428440544.0156 - mae: 729177.0000 - val_loss: 1406585620014.3550 - val_mae: 725543.1250\n",
      "Epoch 924/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1395418849887.5542 - mae: 729025.5000 - val_loss: 1406156045413.0928 - val_mae: 728360.1875\n",
      "Epoch 925/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1395283901296.7937 - mae: 729044.0000 - val_loss: 1406086610407.5002 - val_mae: 727449.7500\n",
      "Epoch 926/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1395119080137.7363 - mae: 729080.1250 - val_loss: 1405868736151.1401 - val_mae: 727512.0625\n",
      "Epoch 927/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1395051780710.9387 - mae: 729087.1250 - val_loss: 1406135292286.8149 - val_mae: 726010.3125\n",
      "Epoch 928/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1395027605702.9917 - mae: 728890.4375 - val_loss: 1405816784950.8875 - val_mae: 727330.8125\n",
      "Epoch 929/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1394924068189.3835 - mae: 728936.9375 - val_loss: 1405658525155.1091 - val_mae: 728626.0625\n",
      "Epoch 930/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1394659220083.7126 - mae: 729141.6875 - val_loss: 1406106973479.2944 - val_mae: 725097.8125\n",
      "Epoch 931/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1394707371629.8247 - mae: 728818.8750 - val_loss: 1405391371141.5510 - val_mae: 728263.7500\n",
      "Epoch 932/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1394659688112.6875 - mae: 728891.3750 - val_loss: 1405366153206.8188 - val_mae: 728398.1875\n",
      "Epoch 933/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1394462590636.4961 - mae: 728855.6250 - val_loss: 1405417899764.3489 - val_mae: 728758.5625\n",
      "Epoch 934/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1394346396895.6414 - mae: 728946.6250 - val_loss: 1405684115586.7319 - val_mae: 725818.8125\n",
      "Epoch 935/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1394319979125.5090 - mae: 728755.0625 - val_loss: 1405135103710.7932 - val_mae: 727415.5625\n",
      "Epoch 936/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1394077946229.2346 - mae: 728891.3750 - val_loss: 1405236145302.9902 - val_mae: 726904.1250\n",
      "Epoch 937/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1394078379251.6006 - mae: 728810.5625 - val_loss: 1405057914029.1450 - val_mae: 726760.8125\n",
      "Epoch 938/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1394002863152.2012 - mae: 728653.3125 - val_loss: 1404769830348.9546 - val_mae: 727909.1875\n",
      "Epoch 939/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1393868461931.7041 - mae: 728654.7500 - val_loss: 1404761172456.3984 - val_mae: 727815.1875\n",
      "Epoch 940/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1393692227549.8701 - mae: 728577.8125 - val_loss: 1404794314064.6096 - val_mae: 728371.7500\n",
      "Epoch 941/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1393697454291.7656 - mae: 728757.9375 - val_loss: 1404899951467.0056 - val_mae: 725563.0625\n",
      "Epoch 942/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1393509726767.8518 - mae: 728497.7500 - val_loss: 1404469121813.9800 - val_mae: 727566.8125\n",
      "Epoch 943/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1393403510348.0940 - mae: 728564.3125 - val_loss: 1404486636397.7998 - val_mae: 728638.5625\n",
      "Epoch 944/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1393266384446.5217 - mae: 728677.0625 - val_loss: 1405075969494.1357 - val_mae: 724428.9375\n",
      "Epoch 945/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1393248597828.5842 - mae: 728436.1875 - val_loss: 1404306151199.0613 - val_mae: 725728.5000\n",
      "Epoch 946/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1393149814581.0164 - mae: 728483.0625 - val_loss: 1404153166358.3042 - val_mae: 727152.8125\n",
      "Epoch 947/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1393160670097.9258 - mae: 728480.6250 - val_loss: 1404153561841.6545 - val_mae: 728483.5625\n",
      "Epoch 948/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1392957695292.4509 - mae: 728379.5000 - val_loss: 1404104877611.9600 - val_mae: 729524.1250\n",
      "Epoch 949/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1392987810163.8374 - mae: 728490.7500 - val_loss: 1403878040878.0806 - val_mae: 726239.5625\n",
      "Epoch 950/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1392806312609.2195 - mae: 728350.3750 - val_loss: 1403701502823.0137 - val_mae: 726770.8750\n",
      "Epoch 951/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1392528137262.0056 - mae: 728309.1250 - val_loss: 1403963611950.5295 - val_mae: 729682.6250\n",
      "Epoch 952/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1392621755556.9617 - mae: 728405.6250 - val_loss: 1403578554074.4021 - val_mae: 727669.3125\n",
      "Epoch 953/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1392538227264.0188 - mae: 728307.3750 - val_loss: 1403472036237.5847 - val_mae: 728354.9375\n",
      "Epoch 954/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1392378134928.7781 - mae: 728346.1875 - val_loss: 1403308662232.3313 - val_mae: 726947.3125\n",
      "Epoch 955/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1392350420810.7715 - mae: 728222.9375 - val_loss: 1403278117487.3218 - val_mae: 727738.1250\n",
      "Epoch 956/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1392190851998.6995 - mae: 728145.7500 - val_loss: 1403242052836.8308 - val_mae: 728417.6250\n",
      "Epoch 957/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1392140064177.4111 - mae: 728203.2500 - val_loss: 1403098031105.8960 - val_mae: 727619.0625\n",
      "Epoch 958/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1392107878678.8281 - mae: 728237.9375 - val_loss: 1403004703262.4875 - val_mae: 727059.3750\n",
      "Epoch 959/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1391953437030.4648 - mae: 728113.0000 - val_loss: 1402967496320.9856 - val_mae: 725548.3750\n",
      "Epoch 960/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1391928467530.2478 - mae: 728130.9375 - val_loss: 1402997586331.3564 - val_mae: 726822.5000\n",
      "Epoch 961/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1391706675123.5569 - mae: 728140.5000 - val_loss: 1402909478330.3928 - val_mae: 727476.1250\n",
      "Epoch 962/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1391747355368.4731 - mae: 728052.0625 - val_loss: 1402802017333.3906 - val_mae: 727182.2500\n",
      "Epoch 963/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1391521185835.1116 - mae: 728055.3750 - val_loss: 1402643212543.2764 - val_mae: 728287.0625\n",
      "Epoch 964/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1391478358928.9277 - mae: 728022.8750 - val_loss: 1402737155074.8940 - val_mae: 725666.1875\n",
      "Epoch 965/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1391352345251.8140 - mae: 728061.9375 - val_loss: 1402397756285.1682 - val_mae: 725499.5000\n",
      "Epoch 966/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1391253726489.3232 - mae: 728045.3750 - val_loss: 1403367187374.8665 - val_mae: 723204.0625\n",
      "Epoch 967/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1391266514238.8459 - mae: 727838.9375 - val_loss: 1402449302785.7715 - val_mae: 724723.8750\n",
      "Epoch 968/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1391185039639.0278 - mae: 727837.8125 - val_loss: 1402066629427.5193 - val_mae: 727382.2500\n",
      "Epoch 969/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390880693996.5647 - mae: 727946.6250 - val_loss: 1402342253680.0701 - val_mae: 725825.9375\n",
      "Epoch 970/1000\n",
      "164176/164176 [==============================] - 25s 152us/step - loss: 1390951014986.7966 - mae: 727890.5625 - val_loss: 1402127583438.5764 - val_mae: 725882.7500\n",
      "Epoch 971/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390771532924.4448 - mae: 727717.3125 - val_loss: 1402299609837.2634 - val_mae: 728871.6250\n",
      "Epoch 972/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390716398942.5811 - mae: 727858.0000 - val_loss: 1402036999516.7847 - val_mae: 725439.8125\n",
      "Epoch 973/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390712236993.0293 - mae: 727668.4375 - val_loss: 1401563909049.8440 - val_mae: 726726.0625\n",
      "Epoch 974/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390543179307.7603 - mae: 727817.3750 - val_loss: 1401678954780.2170 - val_mae: 726762.0625\n",
      "Epoch 975/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390494264063.8254 - mae: 727615.3750 - val_loss: 1401874702119.6438 - val_mae: 728731.9375\n",
      "Epoch 976/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390303108102.0874 - mae: 727839.6250 - val_loss: 1402116793240.5122 - val_mae: 724038.1875\n",
      "Epoch 977/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390185700380.4417 - mae: 727564.7500 - val_loss: 1402013550108.7910 - val_mae: 728780.5625\n",
      "Epoch 978/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390164936779.6450 - mae: 727696.3750 - val_loss: 1401580923960.7834 - val_mae: 726579.5000\n",
      "Epoch 979/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1390152181960.9878 - mae: 727612.2500 - val_loss: 1401192864566.5132 - val_mae: 726287.6875\n",
      "Epoch 980/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1389951606385.6169 - mae: 727570.0625 - val_loss: 1401172049602.2517 - val_mae: 725163.7500\n",
      "Epoch 981/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1389896015420.9250 - mae: 727630.5000 - val_loss: 1401261954034.3281 - val_mae: 724954.7500\n",
      "Epoch 982/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1389847802145.4065 - mae: 727465.0000 - val_loss: 1400994895875.3931 - val_mae: 727698.7500\n",
      "Epoch 983/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1389678444353.9897 - mae: 727570.4375 - val_loss: 1401029698244.1477 - val_mae: 725584.5625\n",
      "Epoch 984/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1389640173466.4082 - mae: 727528.1875 - val_loss: 1400991969000.9722 - val_mae: 725168.7500\n",
      "Epoch 985/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1389500848717.3914 - mae: 727339.1250 - val_loss: 1400760073572.5688 - val_mae: 727283.8750\n",
      "Epoch 986/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1389372576166.7329 - mae: 727525.1875 - val_loss: 1400621063023.9954 - val_mae: 725750.8125\n",
      "Epoch 987/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1389325540457.0845 - mae: 727353.0625 - val_loss: 1400729078761.7456 - val_mae: 727957.9375\n",
      "Epoch 988/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1389318412139.7041 - mae: 727320.9375 - val_loss: 1400488585713.9788 - val_mae: 727647.5000\n",
      "Epoch 989/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1389157773863.2695 - mae: 727477.5000 - val_loss: 1400373413816.7461 - val_mae: 726251.9375\n",
      "Epoch 990/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1389137117479.3943 - mae: 727325.8750 - val_loss: 1400215362827.9504 - val_mae: 725259.2500\n",
      "Epoch 991/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1388985939403.7571 - mae: 727396.3750 - val_loss: 1400544644560.7468 - val_mae: 725249.5000\n",
      "Epoch 992/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1388856238705.1182 - mae: 727298.2500 - val_loss: 1400141228895.6289 - val_mae: 727012.1875\n",
      "Epoch 993/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1388663807357.3181 - mae: 727217.1250 - val_loss: 1399977407551.7693 - val_mae: 726920.6875\n",
      "Epoch 994/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1388735764796.3513 - mae: 727211.3750 - val_loss: 1399897881100.7239 - val_mae: 727042.9375\n",
      "Epoch 995/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1388542192110.7852 - mae: 727339.3125 - val_loss: 1400185681247.1797 - val_mae: 724224.6875\n",
      "Epoch 996/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1388589848604.9407 - mae: 727101.3125 - val_loss: 1399809102053.5293 - val_mae: 727199.0000\n",
      "Epoch 997/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1388487331299.9077 - mae: 727156.7500 - val_loss: 1400080052837.5417 - val_mae: 727767.4375\n",
      "Epoch 998/1000\n",
      "164176/164176 [==============================] - 25s 155us/step - loss: 1388372929209.5696 - mae: 727248.0000 - val_loss: 1400007142507.8787 - val_mae: 724179.0625\n",
      "Epoch 999/1000\n",
      "164176/164176 [==============================] - 25s 154us/step - loss: 1388226122013.4148 - mae: 727174.4375 - val_loss: 1400109629954.1455 - val_mae: 723360.6875\n",
      "Epoch 1000/1000\n",
      "164176/164176 [==============================] - 25s 153us/step - loss: 1388111128754.6335 - mae: 726966.1875 - val_loss: 1399485124598.0205 - val_mae: 725518.5000\n"
     ]
    }
   ],
   "source": [
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "train_x.antiguedad = scaler.fit_transform(np.array(train_x.antiguedad).reshape(-1, 1))\n",
    "train_x.metroscubiertos = scaler.fit_transform(np.array(train_x.metroscubiertos).reshape(-1, 1))\n",
    "train_x.metrostotales = scaler.fit_transform(np.array(train_x.metrostotales).reshape(-1, 1))\n",
    "#inverse = scaler.inverse_transform(normalized)\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "#get number of columns in training data\n",
    "n_cols = train_x.shape[0]\n",
    "\n",
    "wide_model = Sequential()\n",
    "\n",
    "wide_model.add(Dense(10000, activation = 'relu', input_shape = (43,)))\n",
    "wide_model.add(Dense(1))\n",
    "\n",
    "wide_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "training_history = wide_model.fit(train_x, train_y, validation_split=0.2, epochs=1000, callbacks=[tensorboard_callback])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 164176 samples, validate on 41044 samples\n",
      "Epoch 1/1000\n",
      "164176/164176 [==============================] - 17s 103us/step - loss: 1792935120732.7349 - mae: 847945.7500 - val_loss: 1699616154942.0479 - val_mae: 877074.6875\n",
      "Epoch 2/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1539043235409.0837 - mae: 769724.9375 - val_loss: 1573141770666.7249 - val_mae: 740336.5625\n",
      "Epoch 3/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1500462316629.1255 - mae: 759182.5000 - val_loss: 1493326597813.0786 - val_mae: 769343.8125\n",
      "Epoch 4/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1472184019390.8835 - mae: 751163.7500 - val_loss: 1450157822638.1929 - val_mae: 727758.7500\n",
      "Epoch 5/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1439320011491.6831 - mae: 742697.0625 - val_loss: 1416405030302.2505 - val_mae: 723468.0000\n",
      "Epoch 6/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 1417935023254.9902 - mae: 737054.1875 - val_loss: 1435407395545.5039 - val_mae: 769422.3125\n",
      "Epoch 7/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1401242651849.2874 - mae: 731847.5000 - val_loss: 1385918058256.0920 - val_mae: 731946.2500\n",
      "Epoch 8/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1384797138907.6746 - mae: 726301.8125 - val_loss: 1385989595836.1641 - val_mae: 706450.5000\n",
      "Epoch 9/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1378562940116.0652 - mae: 723742.9375 - val_loss: 1354463378363.8398 - val_mae: 713464.5625\n",
      "Epoch 10/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1367717835798.2544 - mae: 720949.0625 - val_loss: 1375884924156.9812 - val_mae: 734712.0000\n",
      "Epoch 11/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1356899635020.7676 - mae: 717521.2500 - val_loss: 1355017569717.0039 - val_mae: 725573.8125\n",
      "Epoch 12/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1347114748999.9524 - mae: 715082.5625 - val_loss: 1357004989109.7771 - val_mae: 728317.7500\n",
      "Epoch 13/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1338345623674.5486 - mae: 712661.3750 - val_loss: 1335253625071.3093 - val_mae: 711185.7500\n",
      "Epoch 14/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 1336640724504.2004 - mae: 712226.3750 - val_loss: 1326769699867.6433 - val_mae: 695021.2500\n",
      "Epoch 15/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1327030514242.4138 - mae: 708932.9375 - val_loss: 1318673618021.2922 - val_mae: 709598.2500\n",
      "Epoch 16/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1322996631576.5496 - mae: 708612.5000 - val_loss: 1343159427609.2981 - val_mae: 692558.1875\n",
      "Epoch 17/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1318056386787.5334 - mae: 706517.8750 - val_loss: 1324807682154.3818 - val_mae: 702033.4375\n",
      "Epoch 18/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1310343636610.1831 - mae: 704695.1250 - val_loss: 1514408083599.7053 - val_mae: 800794.3125\n",
      "Epoch 19/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1309501267057.0681 - mae: 703864.8125 - val_loss: 1316695452224.2183 - val_mae: 697399.0000\n",
      "Epoch 20/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1303509698466.3918 - mae: 702520.0000 - val_loss: 1344868094454.5693 - val_mae: 689252.3125\n",
      "Epoch 21/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 1302832133948.6006 - mae: 701017.9375 - val_loss: 1314812830257.2490 - val_mae: 698232.7500\n",
      "Epoch 22/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1295187175883.6575 - mae: 700028.3750 - val_loss: 1303432867521.0542 - val_mae: 705355.7500\n",
      "Epoch 23/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1293803784545.4751 - mae: 699267.5625 - val_loss: 1311025543694.3206 - val_mae: 705700.0000\n",
      "Epoch 24/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1291624026608.9807 - mae: 699094.4375 - val_loss: 1327005633267.8499 - val_mae: 717289.3750\n",
      "Epoch 25/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1286037758594.6819 - mae: 697451.9375 - val_loss: 1311329690228.3115 - val_mae: 692204.8125\n",
      "Epoch 26/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1281194460212.1929 - mae: 696018.0000 - val_loss: 1337631475062.7314 - val_mae: 689030.1250\n",
      "Epoch 27/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1280582355667.9155 - mae: 695141.1875 - val_loss: 1303520943491.7048 - val_mae: 699256.9375\n",
      "Epoch 28/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1279756161767.6748 - mae: 695258.1250 - val_loss: 1314261161110.7908 - val_mae: 699786.8750\n",
      "Epoch 29/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1275177027322.7358 - mae: 694124.6875 - val_loss: 1297009678591.6758 - val_mae: 688932.5000\n",
      "Epoch 30/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1271249866323.6785 - mae: 692934.3750 - val_loss: 1297462155690.5254 - val_mae: 709503.1875\n",
      "Epoch 31/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1268821139605.2939 - mae: 692449.5625 - val_loss: 1288091186535.1633 - val_mae: 693705.4375\n",
      "Epoch 32/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1265576547106.7537 - mae: 690735.0000 - val_loss: 1302668111056.4724 - val_mae: 719298.5000\n",
      "Epoch 33/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1262756920203.9380 - mae: 690700.3750 - val_loss: 1310582860072.7913 - val_mae: 710337.3750\n",
      "Epoch 34/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1261535690241.0479 - mae: 690869.1875 - val_loss: 1281959545164.7178 - val_mae: 682324.2500\n",
      "Epoch 35/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1256802654818.1487 - mae: 689247.8750 - val_loss: 1275860829033.4089 - val_mae: 685878.6875\n",
      "Epoch 36/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1250886206664.6885 - mae: 687247.0000 - val_loss: 1291602103685.7009 - val_mae: 704820.6875\n",
      "Epoch 37/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1253072753367.7078 - mae: 687774.5000 - val_loss: 1314765870970.0747 - val_mae: 715009.5000\n",
      "Epoch 38/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1248924840557.5254 - mae: 687314.8750 - val_loss: 1275501392865.5625 - val_mae: 691400.9375\n",
      "Epoch 39/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1245675542255.1597 - mae: 685978.1250 - val_loss: 1379589339374.9102 - val_mae: 693331.0625\n",
      "Epoch 40/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1245684488313.5508 - mae: 685850.0000 - val_loss: 1283841256452.0916 - val_mae: 683006.6875\n",
      "Epoch 41/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1241949807364.0168 - mae: 685078.2500 - val_loss: 1287716863061.1255 - val_mae: 691484.1250\n",
      "Epoch 42/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1238240449016.1660 - mae: 683826.6875 - val_loss: 1327979475544.7681 - val_mae: 687980.4375\n",
      "Epoch 43/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1238369823138.1426 - mae: 683875.5625 - val_loss: 1336038695837.3025 - val_mae: 684972.7500\n",
      "Epoch 44/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1234609036258.9595 - mae: 682746.2500 - val_loss: 1286821555001.3074 - val_mae: 683072.1875\n",
      "Epoch 45/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 1231683399940.6653 - mae: 682326.1250 - val_loss: 1319859842154.2820 - val_mae: 716650.0000\n",
      "Epoch 46/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 1229713157531.2566 - mae: 680825.5000 - val_loss: 1303158577907.4507 - val_mae: 685576.5625\n",
      "Epoch 47/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1226047098633.7051 - mae: 680991.5000 - val_loss: 1303616712440.9395 - val_mae: 680738.6250\n",
      "Epoch 48/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1223513033573.2175 - mae: 679749.6250 - val_loss: 1278363521486.6511 - val_mae: 693914.8125\n",
      "Epoch 49/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1220989141086.2068 - mae: 679457.3125 - val_loss: 1307565449778.8457 - val_mae: 682057.3125\n",
      "Epoch 50/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 1220043499245.1638 - mae: 679113.3125 - val_loss: 1289195473851.4407 - val_mae: 685181.9375\n",
      "Epoch 51/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1218332744470.5786 - mae: 678500.2500 - val_loss: 1275641175679.9875 - val_mae: 690327.2500\n",
      "Epoch 52/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1213118068144.6128 - mae: 677017.6250 - val_loss: 1282195539275.6199 - val_mae: 690569.3125\n",
      "Epoch 53/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1214839180807.0356 - mae: 677619.0625 - val_loss: 1292817218375.2788 - val_mae: 682495.8125\n",
      "Epoch 54/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1211114456751.6897 - mae: 676996.0000 - val_loss: 1285395005489.4985 - val_mae: 687492.1250\n",
      "Epoch 55/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1207122697384.9534 - mae: 675532.7500 - val_loss: 1286211894874.4644 - val_mae: 685617.2500\n",
      "Epoch 56/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1204475426663.7124 - mae: 674722.5625 - val_loss: 1275733280728.6807 - val_mae: 679847.9375\n",
      "Epoch 57/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1206306352302.9412 - mae: 675593.3750 - val_loss: 1287056553014.1890 - val_mae: 689850.9375\n",
      "Epoch 58/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1200645544625.9851 - mae: 673824.7500 - val_loss: 1286359479532.2156 - val_mae: 693389.6875\n",
      "Epoch 59/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1201079945023.3950 - mae: 673557.7500 - val_loss: 1296174871896.7930 - val_mae: 678395.3750\n",
      "Epoch 60/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1200041207925.9580 - mae: 673044.6875 - val_loss: 1291411698223.2532 - val_mae: 708585.8125\n",
      "Epoch 61/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1196471455735.6172 - mae: 673061.1250 - val_loss: 1298200429990.6333 - val_mae: 693087.0000\n",
      "Epoch 62/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1193273292785.0308 - mae: 672235.0625 - val_loss: 1290711263985.4548 - val_mae: 693994.7500\n",
      "Epoch 63/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1194866421597.5332 - mae: 671697.7500 - val_loss: 1311454011464.6511 - val_mae: 713001.1250\n",
      "Epoch 64/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 1189884041152.3306 - mae: 671371.8750 - val_loss: 1286708743390.0447 - val_mae: 685449.9375\n",
      "Epoch 65/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1186832533143.7388 - mae: 670243.8750 - val_loss: 1277397932029.0061 - val_mae: 686623.0625\n",
      "Epoch 66/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1184567408744.7849 - mae: 669823.5625 - val_loss: 1289491909299.9810 - val_mae: 692012.1875\n",
      "Epoch 67/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1181078899162.8262 - mae: 668947.3125 - val_loss: 1318759708048.6782 - val_mae: 686658.5000\n",
      "Epoch 68/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1180274762000.2417 - mae: 668311.5625 - val_loss: 1291620580443.5122 - val_mae: 695033.9375\n",
      "Epoch 69/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1179246434217.9763 - mae: 667873.2500 - val_loss: 1299095073659.3721 - val_mae: 681853.2500\n",
      "Epoch 70/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1175303618809.0894 - mae: 667346.0625 - val_loss: 1283347258787.6394 - val_mae: 685189.0000\n",
      "Epoch 71/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1172512938594.1487 - mae: 666425.8125 - val_loss: 1283900497100.0815 - val_mae: 680525.2500\n",
      "Epoch 72/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1171161852849.0620 - mae: 666220.7500 - val_loss: 1301457267518.0977 - val_mae: 702637.4375\n",
      "Epoch 73/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1170338198499.1592 - mae: 666112.5625 - val_loss: 1315727066149.8225 - val_mae: 684396.5625\n",
      "Epoch 74/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1166190366556.7349 - mae: 665243.9375 - val_loss: 1297178783876.9275 - val_mae: 700229.5625\n",
      "Epoch 75/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1165064366766.3923 - mae: 664679.5625 - val_loss: 1292093199480.4529 - val_mae: 680835.6250\n",
      "Epoch 76/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1161779301323.4080 - mae: 663911.0625 - val_loss: 1323524492109.2666 - val_mae: 685741.5000\n",
      "Epoch 77/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1160286546986.4131 - mae: 663127.0000 - val_loss: 1287484985733.2019 - val_mae: 688651.3750\n",
      "Epoch 78/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1162004620843.9600 - mae: 663410.5625 - val_loss: 1286930860784.6565 - val_mae: 693500.8125\n",
      "Epoch 79/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1158104008391.6406 - mae: 663425.3750 - val_loss: 1305886480254.8647 - val_mae: 705548.6250\n",
      "Epoch 80/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1155859639215.4653 - mae: 662324.9375 - val_loss: 1283995346788.9180 - val_mae: 684874.4375\n",
      "Epoch 81/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1153412786982.1467 - mae: 661212.9375 - val_loss: 1300684893612.8206 - val_mae: 686207.3125\n",
      "Epoch 82/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1153484585363.3728 - mae: 661411.0625 - val_loss: 1290507743160.5466 - val_mae: 689451.4375\n",
      "Epoch 83/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1146190251866.5393 - mae: 659967.0625 - val_loss: 1342069464121.1826 - val_mae: 728041.4375\n",
      "Epoch 84/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1147099158053.9722 - mae: 660222.3750 - val_loss: 1288514360518.6926 - val_mae: 686812.1250\n",
      "Epoch 85/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1145014110595.0063 - mae: 659424.4375 - val_loss: 1292751602315.4641 - val_mae: 685714.2500\n",
      "Epoch 86/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1142442900980.2739 - mae: 658443.5000 - val_loss: 1296407372385.1509 - val_mae: 688047.6250\n",
      "Epoch 87/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1139461678956.9016 - mae: 658225.7500 - val_loss: 1358171944154.6516 - val_mae: 685781.6875\n",
      "Epoch 88/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1138070471676.3076 - mae: 657743.0000 - val_loss: 1302226789358.7354 - val_mae: 681747.0625\n",
      "Epoch 89/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1135004923232.7764 - mae: 656268.5625 - val_loss: 1338105146245.9502 - val_mae: 688366.1250\n",
      "Epoch 90/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1133348712148.2148 - mae: 655975.5625 - val_loss: 1300545886567.4629 - val_mae: 682598.0625\n",
      "Epoch 91/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1131119383755.4827 - mae: 655847.8750 - val_loss: 1292120623067.8740 - val_mae: 682269.3750\n",
      "Epoch 92/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1130980068756.0713 - mae: 655960.8750 - val_loss: 1302886492222.0728 - val_mae: 687216.0625\n",
      "Epoch 93/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1124131797671.6062 - mae: 654197.6875 - val_loss: 1310413837804.4900 - val_mae: 701271.1250\n",
      "Epoch 94/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1123426726306.7412 - mae: 653540.3750 - val_loss: 1301288920511.3826 - val_mae: 688318.6875\n",
      "Epoch 95/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1122673496884.3176 - mae: 653331.1250 - val_loss: 1304712366383.9766 - val_mae: 706898.3125\n",
      "Epoch 96/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1117785109326.6636 - mae: 652876.0625 - val_loss: 1299213417771.0867 - val_mae: 681750.5000\n",
      "Epoch 97/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1118797348204.5522 - mae: 652688.8125 - val_loss: 1298599311415.2866 - val_mae: 694222.1250\n",
      "Epoch 98/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1116301715366.3838 - mae: 652236.1250 - val_loss: 1303122736804.9119 - val_mae: 690082.3750\n",
      "Epoch 99/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1113049662835.5381 - mae: 651195.0625 - val_loss: 1312671016153.4541 - val_mae: 684096.9375\n",
      "Epoch 100/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1107554065249.6248 - mae: 648881.5625 - val_loss: 1348898408180.3489 - val_mae: 719636.6875\n",
      "Epoch 101/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1108739709678.6604 - mae: 650871.5000 - val_loss: 1324490677503.1768 - val_mae: 692417.1875\n",
      "Epoch 102/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1106746001108.4143 - mae: 649687.5625 - val_loss: 1311205252699.2629 - val_mae: 704499.8750\n",
      "Epoch 103/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1103324143503.4309 - mae: 648271.1250 - val_loss: 1293683701986.1362 - val_mae: 682987.6875\n",
      "Epoch 104/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1101691042487.5735 - mae: 647791.6875 - val_loss: 1302896419441.1179 - val_mae: 688528.0000\n",
      "Epoch 105/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1099853234504.9253 - mae: 647995.3125 - val_loss: 1289810454095.0879 - val_mae: 689904.1875\n",
      "Epoch 106/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1099106024478.8367 - mae: 647170.9375 - val_loss: 1304671882923.4983 - val_mae: 682987.8125\n",
      "Epoch 107/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 1096092332573.6891 - mae: 647176.5625 - val_loss: 1312837157513.8672 - val_mae: 694585.6875\n",
      "Epoch 108/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1096387712738.3857 - mae: 646657.0000 - val_loss: 1318249429443.2744 - val_mae: 686573.3125\n",
      "Epoch 109/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1091794806727.9150 - mae: 646009.6875 - val_loss: 1309856508314.1589 - val_mae: 695154.0625\n",
      "Epoch 110/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1091047850344.8599 - mae: 645392.9375 - val_loss: 1316581542500.1445 - val_mae: 686417.8125\n",
      "Epoch 111/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1087039611848.4139 - mae: 644149.5625 - val_loss: 1342534659751.5066 - val_mae: 688199.5000\n",
      "Epoch 112/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1083784632783.2500 - mae: 643958.8750 - val_loss: 1319534477010.2190 - val_mae: 689743.0625\n",
      "Epoch 113/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1083000339408.5972 - mae: 643135.3750 - val_loss: 1319071894350.8633 - val_mae: 690829.8750\n",
      "Epoch 114/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1080599342192.6688 - mae: 643264.3125 - val_loss: 1309037919652.4377 - val_mae: 686358.6875\n",
      "Epoch 115/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1076525653489.8790 - mae: 641618.8750 - val_loss: 1331293351519.6538 - val_mae: 704068.2500\n",
      "Epoch 116/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 1075679489363.4041 - mae: 641967.7500 - val_loss: 1333500780137.6333 - val_mae: 696417.6250\n",
      "Epoch 117/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1073042019801.3293 - mae: 640870.6875 - val_loss: 1320332892511.4792 - val_mae: 694670.2500\n",
      "Epoch 118/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1071613936944.4756 - mae: 640668.6875 - val_loss: 1317886176188.2390 - val_mae: 697262.8750\n",
      "Epoch 119/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 1068922432635.8461 - mae: 639795.2500 - val_loss: 1337581737818.1401 - val_mae: 686483.7500\n",
      "Epoch 120/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1068315713868.9171 - mae: 639678.6250 - val_loss: 1320705234628.0479 - val_mae: 699361.7500\n",
      "Epoch 121/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1065358118886.0532 - mae: 639280.6875 - val_loss: 1333671997699.0686 - val_mae: 701676.6250\n",
      "Epoch 122/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1062333510001.2428 - mae: 638602.6875 - val_loss: 1355248491213.1294 - val_mae: 689711.0625\n",
      "Epoch 123/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1060129173247.3264 - mae: 637847.4375 - val_loss: 1315702367540.4673 - val_mae: 682719.0000\n",
      "Epoch 124/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1059530747399.8340 - mae: 637337.1875 - val_loss: 1328921807581.4958 - val_mae: 687425.1875\n",
      "Epoch 125/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1058325882304.9791 - mae: 637216.9375 - val_loss: 1347850236183.8262 - val_mae: 705598.7500\n",
      "Epoch 126/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 1054910573077.1068 - mae: 636081.2500 - val_loss: 1342852935651.6582 - val_mae: 688869.1875\n",
      "Epoch 127/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1053160594530.6975 - mae: 636144.5625 - val_loss: 1331710534241.1509 - val_mae: 681565.9375\n",
      "Epoch 128/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1049216637557.3094 - mae: 634131.5625 - val_loss: 1377112376980.5454 - val_mae: 712072.0625\n",
      "Epoch 129/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1050547308535.0186 - mae: 634586.0625 - val_loss: 1329568059166.4626 - val_mae: 688275.8750\n",
      "Epoch 130/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1046693632573.4241 - mae: 634076.0625 - val_loss: 1335292594599.7310 - val_mae: 688713.7500\n",
      "Epoch 131/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1044013706830.0898 - mae: 633840.5625 - val_loss: 1359320862119.2319 - val_mae: 702768.6875\n",
      "Epoch 132/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1041631013492.9102 - mae: 632669.5625 - val_loss: 1351528062132.1306 - val_mae: 710715.7500\n",
      "Epoch 133/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1041813818066.3186 - mae: 633138.6875 - val_loss: 1346170441612.6365 - val_mae: 698689.1875\n",
      "Epoch 134/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1038583638198.6254 - mae: 632583.5625 - val_loss: 1332754914480.9373 - val_mae: 690130.2500\n",
      "Epoch 135/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1038226753420.0378 - mae: 632535.5000 - val_loss: 1331288479233.2476 - val_mae: 694810.8125\n",
      "Epoch 136/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1033595596912.1699 - mae: 630453.2500 - val_loss: 1356183755290.2961 - val_mae: 696895.9375\n",
      "Epoch 137/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1033923614372.1134 - mae: 630520.8750 - val_loss: 1332022414591.5759 - val_mae: 688548.3750\n",
      "Epoch 138/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1030963986222.8289 - mae: 629710.6250 - val_loss: 1344391993122.2546 - val_mae: 693342.4375\n",
      "Epoch 139/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1030350624185.3949 - mae: 629080.6250 - val_loss: 1359387904141.1106 - val_mae: 697971.6250\n",
      "Epoch 140/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1025918818797.5878 - mae: 628229.8125 - val_loss: 1334109142854.7798 - val_mae: 691519.3750\n",
      "Epoch 141/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1024556938078.6309 - mae: 628055.4375 - val_loss: 1355367989942.5757 - val_mae: 705428.6875\n",
      "Epoch 142/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1021986147299.3588 - mae: 627235.2500 - val_loss: 1374334052457.1843 - val_mae: 698325.2500\n",
      "Epoch 143/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1019238469590.0859 - mae: 626976.4375 - val_loss: 1371708752737.4253 - val_mae: 693652.7500\n",
      "Epoch 144/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1017685455576.5060 - mae: 626022.9375 - val_loss: 1382405009783.6296 - val_mae: 716520.5000\n",
      "Epoch 145/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1015887071285.2908 - mae: 625732.6875 - val_loss: 1390898076540.7690 - val_mae: 712916.6250\n",
      "Epoch 146/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 1014757127027.3884 - mae: 625373.3750 - val_loss: 1436830631986.0972 - val_mae: 725839.5625\n",
      "Epoch 147/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1014343570763.2208 - mae: 625604.8125 - val_loss: 1381788976282.3833 - val_mae: 717397.6250\n",
      "Epoch 148/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1010340999051.8383 - mae: 624184.1250 - val_loss: 1337348589857.6062 - val_mae: 690230.0000\n",
      "Epoch 149/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1007873844449.4377 - mae: 623178.1875 - val_loss: 1366130616460.3123 - val_mae: 699374.8125\n",
      "Epoch 150/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1005260248938.1074 - mae: 623209.2500 - val_loss: 1368196188915.8499 - val_mae: 708204.5000\n",
      "Epoch 151/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1005331236605.9292 - mae: 622741.6250 - val_loss: 1372230114131.2542 - val_mae: 702514.6250\n",
      "Epoch 152/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 1002968755515.4531 - mae: 621634.7500 - val_loss: 1369524857566.2942 - val_mae: 710946.3750\n",
      "Epoch 153/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 1001179393812.0839 - mae: 622005.6250 - val_loss: 1358232846543.5742 - val_mae: 696008.1875\n",
      "Epoch 154/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 999964905920.2806 - mae: 621688.6250 - val_loss: 1358203320471.0901 - val_mae: 697521.6250\n",
      "Epoch 155/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 997099488595.6036 - mae: 620952.1875 - val_loss: 1351328375990.3262 - val_mae: 696170.0625\n",
      "Epoch 156/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 998471260982.5132 - mae: 620379.4375 - val_loss: 1361447457382.3401 - val_mae: 694308.8750\n",
      "Epoch 157/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 996018007861.6151 - mae: 620441.0625 - val_loss: 1360511073738.7593 - val_mae: 693563.5625\n",
      "Epoch 158/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 990237911561.9297 - mae: 618792.9375 - val_loss: 1368547842040.2161 - val_mae: 700874.5000\n",
      "Epoch 159/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 991647939616.4335 - mae: 618493.3750 - val_loss: 1398342920755.4446 - val_mae: 706605.8125\n",
      "Epoch 160/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 987562767190.6472 - mae: 617847.2500 - val_loss: 1348928542748.9407 - val_mae: 697126.6875\n",
      "Epoch 161/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 987445702119.8994 - mae: 617439.0625 - val_loss: 1392102933163.3984 - val_mae: 720590.1250\n",
      "Epoch 162/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 985753791595.8788 - mae: 617031.6875 - val_loss: 1371693122806.9934 - val_mae: 697065.6875\n",
      "Epoch 163/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 983207126936.0133 - mae: 616654.5625 - val_loss: 1389419742309.6914 - val_mae: 695336.0000\n",
      "Epoch 164/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 979507208045.8997 - mae: 615660.2500 - val_loss: 1388398916369.9880 - val_mae: 701616.3125\n",
      "Epoch 165/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 979923376532.1711 - mae: 615386.8750 - val_loss: 1363239677304.1287 - val_mae: 697661.0000\n",
      "Epoch 166/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 979584529779.9373 - mae: 614764.6250 - val_loss: 1385923338767.7178 - val_mae: 719291.8125\n",
      "Epoch 167/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 974290592509.6299 - mae: 614592.6250 - val_loss: 1418476599126.1484 - val_mae: 709186.5000\n",
      "Epoch 168/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 973066917742.8975 - mae: 614131.4375 - val_loss: 1364738946193.2021 - val_mae: 701674.1875\n",
      "Epoch 169/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 971254275232.9698 - mae: 613441.9375 - val_loss: 1370475634498.7881 - val_mae: 693953.5000\n",
      "Epoch 170/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 969108217301.1379 - mae: 612890.6250 - val_loss: 1388862431012.4502 - val_mae: 701224.3125\n",
      "Epoch 171/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 969650899174.5272 - mae: 612537.2500 - val_loss: 1354474374772.6108 - val_mae: 701782.6250\n",
      "Epoch 172/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 964697456247.2056 - mae: 611177.1875 - val_loss: 1381096475375.0598 - val_mae: 705130.0625\n",
      "Epoch 173/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 963325651105.8680 - mae: 611131.3125 - val_loss: 1418955575678.8149 - val_mae: 717222.8750\n",
      "Epoch 174/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 964107930394.1713 - mae: 611560.8125 - val_loss: 1407085671245.0669 - val_mae: 714157.0000\n",
      "Epoch 175/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 958231812645.4731 - mae: 609880.8125 - val_loss: 1398946312591.5806 - val_mae: 706982.6875\n",
      "Epoch 176/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 958525254329.5695 - mae: 609238.2500 - val_loss: 1379966502685.8638 - val_mae: 697981.9375\n",
      "Epoch 177/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 954386911293.8732 - mae: 608173.3750 - val_loss: 1410082815784.4421 - val_mae: 707811.0625\n",
      "Epoch 178/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 954718838162.0754 - mae: 608727.1250 - val_loss: 1402015344809.0532 - val_mae: 700367.8750\n",
      "Epoch 179/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 952767391855.5713 - mae: 608059.6875 - val_loss: 1390166798922.3975 - val_mae: 700340.7500\n",
      "Epoch 180/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 955527182800.0483 - mae: 608865.8750 - val_loss: 1413607872478.6685 - val_mae: 714572.3125\n",
      "Epoch 181/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 951039277277.3462 - mae: 607727.6875 - val_loss: 1391590448780.7612 - val_mae: 702186.4375\n",
      "Epoch 182/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 949974420251.7681 - mae: 607226.9375 - val_loss: 1442428852763.8928 - val_mae: 722201.7500\n",
      "Epoch 183/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 951663063281.5048 - mae: 607648.5625 - val_loss: 1388078007603.6689 - val_mae: 708415.6250\n",
      "Epoch 184/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 946995382013.9292 - mae: 606086.0625 - val_loss: 1425644178222.9287 - val_mae: 704250.8750\n",
      "Epoch 185/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 942467520384.4615 - mae: 605362.4375 - val_loss: 1388505749400.4124 - val_mae: 710612.0625\n",
      "Epoch 186/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 942626610704.2167 - mae: 605210.3750 - val_loss: 1427062503985.4485 - val_mae: 718426.4375\n",
      "Epoch 187/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 943250928469.0507 - mae: 605002.6250 - val_loss: 1376611559244.9670 - val_mae: 702843.6250\n",
      "Epoch 188/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 942236218929.9476 - mae: 604203.8750 - val_loss: 1395788472930.7476 - val_mae: 704102.5625\n",
      "Epoch 189/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 936943065643.8601 - mae: 603120.9375 - val_loss: 1393217802464.7390 - val_mae: 713526.4375\n",
      "Epoch 190/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 936023569005.5254 - mae: 603947.3125 - val_loss: 1403515571410.8677 - val_mae: 702769.8125\n",
      "Epoch 191/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 936824303669.0912 - mae: 602694.6875 - val_loss: 1389138414617.6475 - val_mae: 703910.5625\n",
      "Epoch 192/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 932119117242.7919 - mae: 602636.8750 - val_loss: 1413629344022.8281 - val_mae: 705347.8750\n",
      "Epoch 193/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 926961692552.6447 - mae: 600156.7500 - val_loss: 1409181429754.0122 - val_mae: 711675.1875\n",
      "Epoch 194/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 931392616873.7269 - mae: 601238.4375 - val_loss: 1375189490633.4119 - val_mae: 705610.3750\n",
      "Epoch 195/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 930153316300.9047 - mae: 600626.8750 - val_loss: 1426315918742.0671 - val_mae: 713044.4375\n",
      "Epoch 196/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 924018377736.2830 - mae: 599472.0625 - val_loss: 1444587903301.9316 - val_mae: 725204.1875\n",
      "Epoch 197/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 926108172359.8527 - mae: 600214.6250 - val_loss: 1418783049763.7268 - val_mae: 703124.1250\n",
      "Epoch 198/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 923209432808.5730 - mae: 599093.7500 - val_loss: 1410142248687.7583 - val_mae: 708854.2500\n",
      "Epoch 199/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 921342644745.7301 - mae: 598808.2500 - val_loss: 1455560870706.1223 - val_mae: 727642.5625\n",
      "Epoch 200/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 921777244213.2908 - mae: 598785.2500 - val_loss: 1396927215708.0112 - val_mae: 698594.2500\n",
      "Epoch 201/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 916579274679.7482 - mae: 597085.4375 - val_loss: 1394692586884.5032 - val_mae: 704306.1250\n",
      "Epoch 202/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 913240359846.0844 - mae: 596296.5000 - val_loss: 1433501900377.6660 - val_mae: 716954.6250\n",
      "Epoch 203/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 915433992048.6938 - mae: 597351.8750 - val_loss: 1407414220122.3896 - val_mae: 710896.6875\n",
      "Epoch 204/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 911564003075.9170 - mae: 595836.7500 - val_loss: 1398050151334.2839 - val_mae: 702069.6875\n",
      "Epoch 205/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 915982583244.9546 - mae: 596360.7500 - val_loss: 1464617375677.1372 - val_mae: 718040.1875\n",
      "Epoch 206/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 906912339366.2340 - mae: 594078.5000 - val_loss: 1403363270792.5200 - val_mae: 712922.1250\n",
      "Epoch 207/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 905566471780.6436 - mae: 594222.3125 - val_loss: 1387466845625.5945 - val_mae: 707233.4375\n",
      "Epoch 208/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 904125092823.1838 - mae: 594456.3750 - val_loss: 1416627358046.2817 - val_mae: 711187.6250\n",
      "Epoch 209/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 906322637344.8824 - mae: 593802.5000 - val_loss: 1407359151820.2312 - val_mae: 707799.4375\n",
      "Epoch 210/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 903964794764.2374 - mae: 593016.1875 - val_loss: 1416163981095.8433 - val_mae: 708623.4375\n",
      "Epoch 211/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 899105228114.0068 - mae: 591908.1250 - val_loss: 1462041479147.8413 - val_mae: 718849.6250\n",
      "Epoch 212/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 900527052116.8010 - mae: 592137.5625 - val_loss: 1428474142294.4727 - val_mae: 707758.6875\n",
      "Epoch 213/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 895430459468.6427 - mae: 591155.6250 - val_loss: 1419656492555.6262 - val_mae: 701928.3750\n",
      "Epoch 214/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 893246020823.2587 - mae: 589474.9375 - val_loss: 1420996206069.0725 - val_mae: 704834.0625\n",
      "Epoch 215/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 896000302263.3241 - mae: 591743.3125 - val_loss: 1432225103867.2097 - val_mae: 718998.7500\n",
      "Epoch 216/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 891031555057.7292 - mae: 589252.7500 - val_loss: 1397101960142.2021 - val_mae: 696055.5000\n",
      "Epoch 217/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 892519040021.8552 - mae: 590078.5625 - val_loss: 1430803253077.7493 - val_mae: 719276.0625\n",
      "Epoch 218/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 892820620455.1572 - mae: 590042.8125 - val_loss: 1459715108565.8115 - val_mae: 730716.8125\n",
      "Epoch 219/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 890161163169.0947 - mae: 588479.2500 - val_loss: 1432131530338.8472 - val_mae: 718792.1875\n",
      "Epoch 220/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 885310001429.4312 - mae: 587797.8750 - val_loss: 1405423852995.5740 - val_mae: 708620.6875\n",
      "Epoch 221/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 886330749472.8824 - mae: 588053.0625 - val_loss: 1400036900421.7070 - val_mae: 704193.8750\n",
      "Epoch 222/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 885350267296.1466 - mae: 588115.2500 - val_loss: 1416802456908.8174 - val_mae: 700925.8125\n",
      "Epoch 223/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 884315259598.5264 - mae: 587087.9375 - val_loss: 1430650072529.5452 - val_mae: 708230.5000\n",
      "Epoch 224/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 880131560640.3055 - mae: 586029.7500 - val_loss: 1405449645729.1196 - val_mae: 702514.8125\n",
      "Epoch 225/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 884037772861.5239 - mae: 586942.9375 - val_loss: 1411522172928.1997 - val_mae: 704392.1250\n",
      "Epoch 226/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 877561724659.7501 - mae: 585065.6875 - val_loss: 1451100446936.3562 - val_mae: 711183.6250\n",
      "Epoch 227/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 876791704696.6526 - mae: 585132.2500 - val_loss: 1428515897479.7217 - val_mae: 705036.2500\n",
      "Epoch 228/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 875084716316.4166 - mae: 584074.6250 - val_loss: 1429992260728.8521 - val_mae: 708535.5625\n",
      "Epoch 229/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 875366449666.1456 - mae: 585096.1250 - val_loss: 1449720300853.2659 - val_mae: 717488.1250\n",
      "Epoch 230/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 876101940811.4951 - mae: 584594.5000 - val_loss: 1443210410270.8118 - val_mae: 707097.8125\n",
      "Epoch 231/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 871006454457.7690 - mae: 582710.0625 - val_loss: 1431362025343.6631 - val_mae: 708829.3125\n",
      "Epoch 232/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 867923290344.9224 - mae: 581593.9375 - val_loss: 1440743417600.3242 - val_mae: 710067.7500\n",
      "Epoch 233/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 869792484733.7172 - mae: 582549.4375 - val_loss: 1444690707768.4592 - val_mae: 719250.5625\n",
      "Epoch 234/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 865421041493.9487 - mae: 581498.1250 - val_loss: 1421178633735.4348 - val_mae: 704390.6250\n",
      "Epoch 235/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 863535617405.5176 - mae: 581208.4375 - val_loss: 1429276639798.8376 - val_mae: 705421.0625\n",
      "Epoch 236/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 864353012458.2695 - mae: 580903.3750 - val_loss: 1477614900896.9199 - val_mae: 727790.8750\n",
      "Epoch 237/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 861234477580.7239 - mae: 580482.0625 - val_loss: 1438298560101.8411 - val_mae: 704791.6250\n",
      "Epoch 238/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 860206012050.5496 - mae: 580176.0000 - val_loss: 1433751621412.8494 - val_mae: 706440.3750\n",
      "Epoch 239/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 859447016339.9216 - mae: 579638.5625 - val_loss: 1482747492095.9250 - val_mae: 724002.8750\n",
      "Epoch 240/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 853934510515.5070 - mae: 577988.6250 - val_loss: 1455001822136.4468 - val_mae: 720976.0625\n",
      "Epoch 241/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 853913236810.4224 - mae: 577983.6875 - val_loss: 1458452193836.4587 - val_mae: 717740.5625\n",
      "Epoch 242/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 858898061310.6029 - mae: 579493.0625 - val_loss: 1437038937737.3684 - val_mae: 708824.3125\n",
      "Epoch 243/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 856454295617.8650 - mae: 577745.0000 - val_loss: 1474272916642.7661 - val_mae: 719872.1875\n",
      "Epoch 244/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 852794174013.0249 - mae: 577394.0000 - val_loss: 1480275671023.4341 - val_mae: 719465.5000\n",
      "Epoch 245/1000\n",
      "164176/164176 [==============================] - 16s 101us/step - loss: 849481733364.9976 - mae: 576533.3750 - val_loss: 1458525301035.9849 - val_mae: 720038.9375\n",
      "Epoch 246/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 851129196918.4321 - mae: 576698.6875 - val_loss: 1454687806041.1672 - val_mae: 718767.1875\n",
      "Epoch 247/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 846409834899.1732 - mae: 575881.2500 - val_loss: 1440853779706.9854 - val_mae: 712569.3125\n",
      "Epoch 248/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 847488562201.4478 - mae: 575436.6875 - val_loss: 1490405207531.0928 - val_mae: 724671.6875\n",
      "Epoch 249/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 845132553804.8923 - mae: 575171.6250 - val_loss: 1456947067554.8162 - val_mae: 714263.4375\n",
      "Epoch 250/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 844237075502.2052 - mae: 574685.9375 - val_loss: 1446894105740.4121 - val_mae: 711380.0625\n",
      "Epoch 251/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 844235842866.8707 - mae: 575058.8125 - val_loss: 1459095470772.9788 - val_mae: 714599.8125\n",
      "Epoch 252/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 841138342707.7190 - mae: 574003.0000 - val_loss: 1483408789490.8269 - val_mae: 725906.6250\n",
      "Epoch 253/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 843315991460.0885 - mae: 574794.1875 - val_loss: 1456901771975.7405 - val_mae: 715005.0000\n",
      "Epoch 254/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 838505584676.9243 - mae: 572860.6250 - val_loss: 1493288134896.9060 - val_mae: 723776.7500\n",
      "Epoch 255/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 833079488732.7473 - mae: 571250.1250 - val_loss: 1458449280160.5708 - val_mae: 713086.3750\n",
      "Epoch 256/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 837612688756.7356 - mae: 572627.0625 - val_loss: 1443470763942.3838 - val_mae: 708457.5625\n",
      "Epoch 257/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 835120822525.2805 - mae: 571564.6875 - val_loss: 1460098085001.7175 - val_mae: 725577.3750\n",
      "Epoch 258/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 830454884747.7883 - mae: 570289.8125 - val_loss: 1437250514138.0527 - val_mae: 706957.3750\n",
      "Epoch 259/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 830848961472.2307 - mae: 569912.6250 - val_loss: 1469297144108.5835 - val_mae: 713763.9375\n",
      "Epoch 260/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 832036736962.9253 - mae: 571015.1875 - val_loss: 1483600588160.1123 - val_mae: 726351.8125\n",
      "Epoch 261/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 828426560633.6505 - mae: 570227.1875 - val_loss: 1444920934471.8525 - val_mae: 715800.3750\n",
      "Epoch 262/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 825330692888.3750 - mae: 568624.4375 - val_loss: 1476532479490.4451 - val_mae: 719349.6875\n",
      "Epoch 263/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 824612882418.2283 - mae: 569218.8125 - val_loss: 1478761087637.0444 - val_mae: 726422.5000\n",
      "Epoch 264/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 823208247666.4403 - mae: 567685.6250 - val_loss: 1480608621025.4128 - val_mae: 728287.0000\n",
      "Epoch 265/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 822656006451.5693 - mae: 567572.3125 - val_loss: 1466536173991.9307 - val_mae: 709862.6875\n",
      "Epoch 266/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 822060599751.6655 - mae: 568006.5000 - val_loss: 1446226887524.6187 - val_mae: 711386.6250\n",
      "Epoch 267/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 821852745880.0881 - mae: 568002.5000 - val_loss: 1466115630674.2812 - val_mae: 711802.8125\n",
      "Epoch 268/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 818842232182.7316 - mae: 567316.6875 - val_loss: 1450623695458.9470 - val_mae: 707907.1250\n",
      "Epoch 269/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 822298874544.5879 - mae: 566926.6875 - val_loss: 1463943620879.2437 - val_mae: 712002.4375\n",
      "Epoch 270/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 817201182184.4982 - mae: 566462.5000 - val_loss: 1460005031491.7112 - val_mae: 716456.0000\n",
      "Epoch 271/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 817113710983.1978 - mae: 565923.9375 - val_loss: 1483483096853.0818 - val_mae: 718682.5625\n",
      "Epoch 272/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 815846053223.9618 - mae: 565585.5000 - val_loss: 1482359615146.7000 - val_mae: 716898.8750\n",
      "Epoch 273/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 814175894265.0393 - mae: 564342.4375 - val_loss: 1485024050221.2073 - val_mae: 719924.3750\n",
      "Epoch 274/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 812473329595.1412 - mae: 564454.9375 - val_loss: 1498047367938.2205 - val_mae: 721323.6250\n",
      "Epoch 275/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 811436322733.3694 - mae: 564118.5625 - val_loss: 1465725002397.8264 - val_mae: 712507.3750\n",
      "Epoch 276/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 808169653667.4398 - mae: 563375.0000 - val_loss: 1473212743791.7708 - val_mae: 729328.6875\n",
      "Epoch 277/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 806032042111.5385 - mae: 563121.8125 - val_loss: 1477446983840.4709 - val_mae: 711409.0000\n",
      "Epoch 278/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 808138187908.8276 - mae: 562872.6250 - val_loss: 1467487283238.1218 - val_mae: 720931.3750\n",
      "Epoch 279/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 807936050371.8983 - mae: 562608.0000 - val_loss: 1494608567822.6201 - val_mae: 715060.6875\n",
      "Epoch 280/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 806923492294.2185 - mae: 562672.6875 - val_loss: 1450036582267.6714 - val_mae: 708166.1250\n",
      "Epoch 281/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 804624198034.1752 - mae: 562217.1875 - val_loss: 1472183716310.0361 - val_mae: 720586.8125\n",
      "Epoch 282/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 803056227288.7804 - mae: 561745.4375 - val_loss: 1495262347329.4658 - val_mae: 718462.5625\n",
      "Epoch 283/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 800569614654.5468 - mae: 560899.6250 - val_loss: 1463546116454.6646 - val_mae: 713724.5625\n",
      "Epoch 284/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 796778541495.4987 - mae: 559914.3125 - val_loss: 1489499332374.0798 - val_mae: 714630.1250\n",
      "Epoch 285/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 800657837207.7887 - mae: 561137.8750 - val_loss: 1485640289639.3630 - val_mae: 715230.8125\n",
      "Epoch 286/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 798421393156.5157 - mae: 559590.9375 - val_loss: 1482846384722.3811 - val_mae: 717356.7500\n",
      "Epoch 287/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 796166678738.5682 - mae: 558974.8125 - val_loss: 1509993365246.9272 - val_mae: 726223.0625\n",
      "Epoch 288/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 795731818730.1199 - mae: 559994.3125 - val_loss: 1471854616437.8833 - val_mae: 715830.8125\n",
      "Epoch 289/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 794421064938.6189 - mae: 558762.0000 - val_loss: 1500101693113.8689 - val_mae: 718130.4375\n",
      "Epoch 290/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 795184043941.1863 - mae: 558715.5000 - val_loss: 1491704668221.4739 - val_mae: 723526.6250\n",
      "Epoch 291/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 789213686058.3881 - mae: 557507.9375 - val_loss: 1492520394283.3611 - val_mae: 712691.2500\n",
      "Epoch 292/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 789731453403.6246 - mae: 558040.8125 - val_loss: 1484906627761.7854 - val_mae: 716211.4375\n",
      "Epoch 293/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 792030640368.5068 - mae: 557838.5625 - val_loss: 1481007585259.4421 - val_mae: 724162.6250\n",
      "Epoch 294/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 786474348022.2699 - mae: 556297.1250 - val_loss: 1480734038659.6799 - val_mae: 720601.3750\n",
      "Epoch 295/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 788734545357.5535 - mae: 556062.1250 - val_loss: 1482069875540.8511 - val_mae: 717077.1250\n",
      "Epoch 296/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 784380671742.3284 - mae: 555145.4375 - val_loss: 1553738265044.2397 - val_mae: 740989.1250\n",
      "Epoch 297/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 784758813608.0803 - mae: 555032.6250 - val_loss: 1468479555683.5957 - val_mae: 715331.4375\n",
      "Epoch 298/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 786488795933.5645 - mae: 556351.5625 - val_loss: 1481981616170.3132 - val_mae: 717225.3750\n",
      "Epoch 299/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 780886805510.4867 - mae: 554328.3750 - val_loss: 1487478052342.0703 - val_mae: 714215.5000\n",
      "Epoch 300/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 782203003634.4528 - mae: 554910.3125 - val_loss: 1531726425504.0469 - val_mae: 734743.6250\n",
      "Epoch 301/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 779684487537.6420 - mae: 554557.5625 - val_loss: 1528014573233.8853 - val_mae: 727839.5625\n",
      "Epoch 302/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 777191569581.4442 - mae: 552939.8750 - val_loss: 1507333761074.8955 - val_mae: 723617.6875\n",
      "Epoch 303/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 776523637054.5468 - mae: 552758.3750 - val_loss: 1524954972551.1978 - val_mae: 721345.1250\n",
      "Epoch 304/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 776822441741.8965 - mae: 552353.8125 - val_loss: 1480416460982.0266 - val_mae: 723836.9375\n",
      "Epoch 305/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 773861060429.5659 - mae: 551815.5000 - val_loss: 1497645978492.1704 - val_mae: 722076.1250\n",
      "Epoch 306/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 778543595048.9659 - mae: 552659.8750 - val_loss: 1507726842579.5161 - val_mae: 720091.6250\n",
      "Epoch 307/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 772138244030.7339 - mae: 550930.5625 - val_loss: 1493691228045.8342 - val_mae: 720657.0000\n",
      "Epoch 308/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 773664321625.8158 - mae: 551563.8125 - val_loss: 1496495890383.2998 - val_mae: 720551.6875\n",
      "Epoch 309/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 771210946304.3243 - mae: 550225.8750 - val_loss: 1472116460049.1150 - val_mae: 711183.0000\n",
      "Epoch 310/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 772310828496.1481 - mae: 550932.3125 - val_loss: 1498542965119.2141 - val_mae: 719829.8125\n",
      "Epoch 311/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 769246107988.3021 - mae: 550495.0000 - val_loss: 1494160091392.8733 - val_mae: 716306.5000\n",
      "Epoch 312/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 770755344599.7576 - mae: 550012.1875 - val_loss: 1520719827274.1230 - val_mae: 724076.9375\n",
      "Epoch 313/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 768047698045.7422 - mae: 549306.6250 - val_loss: 1488891430206.8462 - val_mae: 714847.8125\n",
      "Epoch 314/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 769394569751.3021 - mae: 550274.4375 - val_loss: 1512240130831.7927 - val_mae: 725128.0625\n",
      "Epoch 315/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 762606646477.8778 - mae: 547680.2500 - val_loss: 1533042202438.9795 - val_mae: 732287.3750\n",
      "Epoch 316/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 767317180949.3063 - mae: 549427.3125 - val_loss: 1523872589507.6487 - val_mae: 724249.1875\n",
      "Epoch 317/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 764761279401.8766 - mae: 548776.5625 - val_loss: 1521867247519.7974 - val_mae: 721002.5000\n",
      "Epoch 318/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 763333530008.3625 - mae: 548076.5625 - val_loss: 1518221105902.6606 - val_mae: 719526.6250\n",
      "Epoch 319/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 757519092282.6298 - mae: 546599.5625 - val_loss: 1523439452120.0818 - val_mae: 725287.0625\n",
      "Epoch 320/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 759358233719.4550 - mae: 546712.0625 - val_loss: 1509369430669.0608 - val_mae: 720737.3125\n",
      "Epoch 321/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 760497904291.6145 - mae: 547612.3750 - val_loss: 1507376877516.5056 - val_mae: 726592.2500\n",
      "Epoch 322/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 758973709491.3322 - mae: 546702.7500 - val_loss: 1509701262040.3064 - val_mae: 730667.9375\n",
      "Epoch 323/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 750805626832.4973 - mae: 543595.3750 - val_loss: 1508656441982.4905 - val_mae: 722282.6875\n",
      "Epoch 324/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 756643315838.1414 - mae: 545571.1250 - val_loss: 1547585179491.1218 - val_mae: 734523.3125\n",
      "Epoch 325/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 757480549993.2341 - mae: 545333.0625 - val_loss: 1493670426251.3643 - val_mae: 720471.6250\n",
      "Epoch 326/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 752720221848.9364 - mae: 544690.7500 - val_loss: 1519353152757.3967 - val_mae: 727689.1875\n",
      "Epoch 327/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 752409194847.6788 - mae: 544304.1250 - val_loss: 1524018730230.5942 - val_mae: 724222.2500\n",
      "Epoch 328/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 752492210792.1365 - mae: 543978.5625 - val_loss: 1528315273651.1077 - val_mae: 731961.3125\n",
      "Epoch 329/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 747842120958.7776 - mae: 542868.5000 - val_loss: 1502371033867.6013 - val_mae: 718970.2500\n",
      "Epoch 330/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 749179559939.9918 - mae: 542623.2500 - val_loss: 1499856789932.7207 - val_mae: 720531.3125\n",
      "Epoch 331/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 750023685404.1173 - mae: 543140.6250 - val_loss: 1546540393743.7427 - val_mae: 735284.0000\n",
      "Epoch 332/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 746417708662.4071 - mae: 541517.1875 - val_loss: 1553709120758.3948 - val_mae: 732175.4375\n",
      "Epoch 333/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 745508668607.1080 - mae: 541840.2500 - val_loss: 1489347116963.3899 - val_mae: 715936.4375\n",
      "Epoch 334/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 748681391467.3549 - mae: 542992.6875 - val_loss: 1503461498082.4355 - val_mae: 725276.8125\n",
      "Epoch 335/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 743713124484.9274 - mae: 541520.8750 - val_loss: 1525117539568.2073 - val_mae: 725501.2500\n",
      "Epoch 336/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 742024501902.8571 - mae: 540279.2500 - val_loss: 1552944968296.7351 - val_mae: 745165.1250\n",
      "Epoch 337/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 741898304982.7346 - mae: 540964.1250 - val_loss: 1523637404946.1377 - val_mae: 725619.5000\n",
      "Epoch 338/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 739950010632.2581 - mae: 540143.0625 - val_loss: 1552593736659.9902 - val_mae: 732969.0625\n",
      "Epoch 339/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 737280017723.6527 - mae: 539260.0000 - val_loss: 1542600461462.1919 - val_mae: 729452.0000\n",
      "Epoch 340/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 737351644286.0415 - mae: 539286.0000 - val_loss: 1557206799068.4978 - val_mae: 735564.0625\n",
      "Epoch 341/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 738209741522.7179 - mae: 539534.6250 - val_loss: 1561447235552.0654 - val_mae: 736514.2500\n",
      "Epoch 342/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 737882097324.4962 - mae: 539356.0000 - val_loss: 1524192022388.6858 - val_mae: 730377.1875\n",
      "Epoch 343/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 737524307259.2534 - mae: 539473.0625 - val_loss: 1566188032635.0977 - val_mae: 739467.8125\n",
      "Epoch 344/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 737170147069.9292 - mae: 539243.6875 - val_loss: 1503005087741.0061 - val_mae: 729204.7500\n",
      "Epoch 345/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 736620895279.6024 - mae: 538836.5625 - val_loss: 1574173015993.7441 - val_mae: 743997.6875\n",
      "Epoch 346/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 735496062389.0038 - mae: 538612.7500 - val_loss: 1567495739342.7012 - val_mae: 728556.8750\n",
      "Epoch 347/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 732963724249.3792 - mae: 537514.4375 - val_loss: 1533783342468.5032 - val_mae: 733479.1250\n",
      "Epoch 348/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 732250520123.0289 - mae: 537628.2500 - val_loss: 1538806915994.1089 - val_mae: 726852.5000\n",
      "Epoch 349/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 731575508047.5370 - mae: 537173.7500 - val_loss: 1505566847358.0166 - val_mae: 718366.4375\n",
      "Epoch 350/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 729786689989.2704 - mae: 536738.4375 - val_loss: 1526727272955.5591 - val_mae: 725206.4375\n",
      "Epoch 351/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 728713530146.1549 - mae: 536070.0625 - val_loss: 1539537112198.2249 - val_mae: 729766.5625\n",
      "Epoch 352/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 730910567817.6926 - mae: 536375.3750 - val_loss: 1531810981199.6118 - val_mae: 730660.0000\n",
      "Epoch 353/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 728204939964.5634 - mae: 535940.3750 - val_loss: 1539521175447.4146 - val_mae: 730922.3125\n",
      "Epoch 354/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 727100394684.5134 - mae: 535279.3750 - val_loss: 1539153882598.3027 - val_mae: 729843.6875\n",
      "Epoch 355/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 723183957066.9463 - mae: 534300.7500 - val_loss: 1543216704916.8696 - val_mae: 725949.5625\n",
      "Epoch 356/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 725568171208.4888 - mae: 535314.4375 - val_loss: 1519369928612.9866 - val_mae: 726518.8125\n",
      "Epoch 357/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 720647787769.7877 - mae: 533073.3125 - val_loss: 1539276046326.4197 - val_mae: 729803.0625\n",
      "Epoch 358/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 724042959896.0507 - mae: 534383.8750 - val_loss: 1532429994235.2847 - val_mae: 727227.1250\n",
      "Epoch 359/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 724907679396.7123 - mae: 534580.6875 - val_loss: 1523803830472.5886 - val_mae: 723217.2500\n",
      "Epoch 360/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 719845813640.0959 - mae: 533430.1250 - val_loss: 1513722621645.8279 - val_mae: 718724.1875\n",
      "Epoch 361/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 722004332521.2466 - mae: 533560.8125 - val_loss: 1523011832121.5569 - val_mae: 722040.8750\n",
      "Epoch 362/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 716594836612.4283 - mae: 531306.3750 - val_loss: 1557312312462.8071 - val_mae: 742388.9375\n",
      "Epoch 363/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 716970730323.8530 - mae: 531546.0000 - val_loss: 1530420955403.2520 - val_mae: 726965.5000\n",
      "Epoch 364/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 717461984162.4917 - mae: 532413.0625 - val_loss: 1542237538946.4824 - val_mae: 726382.0000\n",
      "Epoch 365/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 718733951078.1904 - mae: 531809.3125 - val_loss: 1510289937394.3281 - val_mae: 718710.8125\n",
      "Epoch 366/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 715797621652.3208 - mae: 531493.3750 - val_loss: 1529291111020.5273 - val_mae: 728312.5625\n",
      "Epoch 367/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 716605877106.7897 - mae: 531449.6250 - val_loss: 1531358314262.8782 - val_mae: 727020.8750\n",
      "Epoch 368/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 713894695060.0963 - mae: 530611.3750 - val_loss: 1551871259097.0300 - val_mae: 734204.9375\n",
      "Epoch 369/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 712815147719.5408 - mae: 530198.5000 - val_loss: 1560274550661.0522 - val_mae: 729463.2500\n",
      "Epoch 370/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 713008939447.0995 - mae: 530242.8125 - val_loss: 1539622791068.8035 - val_mae: 728068.2500\n",
      "Epoch 371/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 711812010192.7719 - mae: 529600.5625 - val_loss: 1562131969590.7378 - val_mae: 736612.0625\n",
      "Epoch 372/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 709733193397.6775 - mae: 529532.0625 - val_loss: 1559925528713.5181 - val_mae: 729824.8750\n",
      "Epoch 373/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 706978080293.5730 - mae: 528234.4375 - val_loss: 1551737994384.2043 - val_mae: 733777.8125\n",
      "Epoch 374/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 705083968186.3678 - mae: 528054.1250 - val_loss: 1545068018326.0422 - val_mae: 731334.8750\n",
      "Epoch 375/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 706939420320.7205 - mae: 527719.8750 - val_loss: 1550775803729.1584 - val_mae: 727451.1875\n",
      "Epoch 376/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 704110483200.7235 - mae: 527555.3750 - val_loss: 1532950381875.6689 - val_mae: 727728.8125\n",
      "Epoch 377/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 704156265918.3845 - mae: 527498.8750 - val_loss: 1573913915774.0166 - val_mae: 734005.8750\n",
      "Epoch 378/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 706381312639.0894 - mae: 528653.1250 - val_loss: 1583758402993.0120 - val_mae: 741309.0000\n",
      "Epoch 379/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 706178719922.0349 - mae: 527882.5000 - val_loss: 1631766241858.8130 - val_mae: 751896.2500\n",
      "Epoch 380/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 702168740451.9451 - mae: 526364.3125 - val_loss: 1553510140503.3708 - val_mae: 729493.9375\n",
      "Epoch 381/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 698024108358.5304 - mae: 524934.0000 - val_loss: 1591588650123.4141 - val_mae: 748312.5000\n",
      "Epoch 382/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 702822553272.8708 - mae: 526981.5625 - val_loss: 1565850609304.5371 - val_mae: 730625.2500\n",
      "Epoch 383/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 700616002735.7396 - mae: 526791.3125 - val_loss: 1570189347622.9451 - val_mae: 734213.1875\n",
      "Epoch 384/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 698056418741.0038 - mae: 525863.2500 - val_loss: 1532102561849.0830 - val_mae: 726849.1250\n",
      "Epoch 385/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 695407994129.3394 - mae: 524792.6250 - val_loss: 1516072099304.2986 - val_mae: 727365.5000\n",
      "Epoch 386/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 701246065619.3915 - mae: 526246.8125 - val_loss: 1554199994161.2241 - val_mae: 732874.4375\n",
      "Epoch 387/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 700903597867.4359 - mae: 525655.1250 - val_loss: 1550812268071.9680 - val_mae: 735437.1875\n",
      "Epoch 388/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 695098992697.2825 - mae: 524271.2500 - val_loss: 1554707174902.0703 - val_mae: 727570.3750\n",
      "Epoch 389/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 694937573895.1354 - mae: 523739.3125 - val_loss: 1561404392955.2598 - val_mae: 730265.8125\n",
      "Epoch 390/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 696648733493.9144 - mae: 524830.8125 - val_loss: 1559501121007.2842 - val_mae: 732157.6875\n",
      "Epoch 391/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 691855711940.6467 - mae: 523289.3125 - val_loss: 1548624244024.2595 - val_mae: 735567.1875\n",
      "Epoch 392/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 692137364957.5208 - mae: 523648.1250 - val_loss: 1538437225687.2585 - val_mae: 724436.8750\n",
      "Epoch 393/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 695184624838.0938 - mae: 523652.2188 - val_loss: 1539864752101.2549 - val_mae: 725993.1875\n",
      "Epoch 394/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 691727986214.1718 - mae: 522563.3438 - val_loss: 1519275889121.8120 - val_mae: 719972.4375\n",
      "Epoch 395/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 689990199354.8793 - mae: 522454.5000 - val_loss: 1586781890258.0193 - val_mae: 741461.8750\n",
      "Epoch 396/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 692428782712.2534 - mae: 522790.8125 - val_loss: 1535379862741.8613 - val_mae: 730354.5625\n",
      "Epoch 397/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 687553594926.3549 - mae: 521254.5312 - val_loss: 1593402337884.9595 - val_mae: 737251.1875\n",
      "Epoch 398/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 688029675126.0079 - mae: 521931.4375 - val_loss: 1572354121949.1465 - val_mae: 736416.2500\n",
      "Epoch 399/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 688535371790.3705 - mae: 521410.4062 - val_loss: 1552275742920.3892 - val_mae: 738205.6250\n",
      "Epoch 400/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 686053930063.7365 - mae: 520634.2500 - val_loss: 1564073940339.1389 - val_mae: 732307.5000\n",
      "Epoch 401/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 684906015791.2031 - mae: 520538.3438 - val_loss: 1563393564134.7019 - val_mae: 735368.5000\n",
      "Epoch 402/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 683250545493.0505 - mae: 519812.2188 - val_loss: 1673835533919.4543 - val_mae: 760636.1250\n",
      "Epoch 403/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 685291558374.1031 - mae: 520052.7812 - val_loss: 1627569876728.9395 - val_mae: 757760.7500\n",
      "Epoch 404/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 683660403493.5481 - mae: 519501.1562 - val_loss: 1522772523588.6094 - val_mae: 724832.6250\n",
      "Epoch 405/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 682410993997.5161 - mae: 519176.0938 - val_loss: 1543695740784.6938 - val_mae: 730797.8750\n",
      "Epoch 406/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 681132274663.9493 - mae: 518443.0000 - val_loss: 1582209546884.4783 - val_mae: 739328.0000\n",
      "Epoch 407/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 682448375558.5117 - mae: 519203.9375 - val_loss: 1527753923430.4150 - val_mae: 727911.7500\n",
      "Epoch 408/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 677918798707.3883 - mae: 517754.5000 - val_loss: 1565164563568.6689 - val_mae: 740196.0000\n",
      "Epoch 409/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 680884795480.8179 - mae: 518324.1250 - val_loss: 1555722605193.4680 - val_mae: 736035.6250\n",
      "Epoch 410/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 677344749404.9344 - mae: 517797.2188 - val_loss: 1603533307690.0388 - val_mae: 741233.3125\n",
      "Epoch 411/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 679926619028.7201 - mae: 518135.4062 - val_loss: 1602183278799.9734 - val_mae: 739396.1250\n",
      "Epoch 412/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 678269288250.0060 - mae: 517522.8125 - val_loss: 1550711670618.5393 - val_mae: 730293.1250\n",
      "Epoch 413/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 674880487067.9302 - mae: 516667.3438 - val_loss: 1557461358194.6150 - val_mae: 730565.0000\n",
      "Epoch 414/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 675285314715.7805 - mae: 517455.2500 - val_loss: 1567549600373.5090 - val_mae: 729000.2500\n",
      "Epoch 415/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 678249091358.2130 - mae: 517688.5312 - val_loss: 1574298882847.5603 - val_mae: 731167.3125\n",
      "Epoch 416/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 675536450789.8287 - mae: 515865.8750 - val_loss: 1547707618759.7654 - val_mae: 730696.6250\n",
      "Epoch 417/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 670301449299.1295 - mae: 515088.3750 - val_loss: 1580468579499.7478 - val_mae: 735184.3750\n",
      "Epoch 418/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 675247252772.5002 - mae: 516649.0312 - val_loss: 1567706853095.3755 - val_mae: 737270.5625\n",
      "Epoch 419/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 670581640521.0253 - mae: 515156.6250 - val_loss: 1572211242355.9373 - val_mae: 736615.6875\n",
      "Epoch 420/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 667820428799.0519 - mae: 513617.9688 - val_loss: 1573265194786.0552 - val_mae: 732428.8750\n",
      "Epoch 421/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 669216833634.9969 - mae: 514390.5938 - val_loss: 1544127520591.5618 - val_mae: 733728.8125\n",
      "Epoch 422/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 667000459162.2087 - mae: 513969.0000 - val_loss: 1574898141425.7043 - val_mae: 738392.5625\n",
      "Epoch 423/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 668735595218.5183 - mae: 514269.5000 - val_loss: 1568924105468.2327 - val_mae: 737256.8750\n",
      "Epoch 424/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 669545425065.0533 - mae: 514450.9688 - val_loss: 1583270988563.7844 - val_mae: 740143.0000\n",
      "Epoch 425/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 663833957022.2255 - mae: 512492.5000 - val_loss: 1531442387172.5312 - val_mae: 724754.2500\n",
      "Epoch 426/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 666656753742.8384 - mae: 513106.9688 - val_loss: 1548019091957.9705 - val_mae: 731898.0000\n",
      "Epoch 427/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 665081568654.8820 - mae: 511957.8750 - val_loss: 1599889597097.0034 - val_mae: 745890.5625\n",
      "Epoch 428/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 664985204047.2125 - mae: 512952.2500 - val_loss: 1591684442570.0605 - val_mae: 740350.8750\n",
      "Epoch 429/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 663590801268.6858 - mae: 512544.9688 - val_loss: 1588399782669.7966 - val_mae: 741117.6875\n",
      "Epoch 430/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 663758012201.2404 - mae: 511895.1562 - val_loss: 1576777407053.3914 - val_mae: 734106.8750\n",
      "Epoch 431/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 666083262486.3541 - mae: 512499.6250 - val_loss: 1613498983165.6299 - val_mae: 742424.3125\n",
      "Epoch 432/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 663197880738.7413 - mae: 512439.8125 - val_loss: 1562859083674.0093 - val_mae: 740575.0625\n",
      "Epoch 433/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 660523609959.1135 - mae: 510544.9688 - val_loss: 1557471679447.1836 - val_mae: 731455.3750\n",
      "Epoch 434/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 656740571246.3737 - mae: 509135.1250 - val_loss: 1594020507887.9080 - val_mae: 738996.0000\n",
      "Epoch 435/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 658846966447.3903 - mae: 510950.5625 - val_loss: 1559281876798.7961 - val_mae: 730240.5000\n",
      "Epoch 436/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 661254395849.9109 - mae: 510990.6250 - val_loss: 1572807293246.7463 - val_mae: 736495.1875\n",
      "Epoch 437/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 656431934142.3596 - mae: 509348.6875 - val_loss: 1570260853000.2581 - val_mae: 734076.5625\n",
      "Epoch 438/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 658644500487.9836 - mae: 509777.7500 - val_loss: 1572868830560.7766 - val_mae: 733301.9375\n",
      "Epoch 439/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 654175096868.3254 - mae: 509157.2188 - val_loss: 1622336975899.0444 - val_mae: 749624.0000\n",
      "Epoch 440/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 655864826670.9287 - mae: 508684.5938 - val_loss: 1553266336683.6729 - val_mae: 729476.6250\n",
      "Epoch 441/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 658615235689.2841 - mae: 509715.1250 - val_loss: 1563328539859.6660 - val_mae: 732251.9375\n",
      "Epoch 442/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 654120620766.0945 - mae: 508306.4375 - val_loss: 1592839584921.7847 - val_mae: 748877.5625\n",
      "Epoch 443/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 655349934483.7720 - mae: 508971.4688 - val_loss: 1570604701589.2190 - val_mae: 731809.9375\n",
      "Epoch 444/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 651483386082.3358 - mae: 508574.5625 - val_loss: 1628812562677.4966 - val_mae: 751697.2500\n",
      "Epoch 445/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 651734938844.2483 - mae: 507893.6562 - val_loss: 1553515432639.1580 - val_mae: 734490.4375\n",
      "Epoch 446/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 655035437663.9532 - mae: 507617.0312 - val_loss: 1615445652700.2483 - val_mae: 740652.5625\n",
      "Epoch 447/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 650585434316.1813 - mae: 506864.6875 - val_loss: 1580282303245.9963 - val_mae: 736024.4375\n",
      "Epoch 448/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 648510273789.2805 - mae: 507620.8438 - val_loss: 1582059253254.2373 - val_mae: 741129.5000\n",
      "Epoch 449/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 652150520276.8385 - mae: 507555.3750 - val_loss: 1646323921520.7188 - val_mae: 755665.6250\n",
      "Epoch 450/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 649730306177.7340 - mae: 506576.9688 - val_loss: 1588869752643.3867 - val_mae: 741063.5000\n",
      "Epoch 451/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 649818397947.9833 - mae: 506982.5312 - val_loss: 1572951624288.9512 - val_mae: 738833.6875\n",
      "Epoch 452/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 649648572121.2046 - mae: 506921.1250 - val_loss: 1603408565154.4917 - val_mae: 749531.6875\n",
      "Epoch 453/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 646571769382.5709 - mae: 506108.7812 - val_loss: 1561556260694.9468 - val_mae: 734669.4375\n",
      "Epoch 454/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 649546637544.3235 - mae: 506188.2188 - val_loss: 1598131206020.4534 - val_mae: 739790.1250\n",
      "Epoch 455/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 645983538977.6560 - mae: 505565.2812 - val_loss: 1573178643292.2358 - val_mae: 736500.1250\n",
      "Epoch 456/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 647780655411.3696 - mae: 505771.6250 - val_loss: 1591111774554.8887 - val_mae: 738121.7500\n",
      "Epoch 457/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 644299215008.8701 - mae: 505087.8750 - val_loss: 1579216633175.7949 - val_mae: 731439.7500\n",
      "Epoch 458/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 647092064908.6616 - mae: 505377.6875 - val_loss: 1548195841055.4355 - val_mae: 732700.3125\n",
      "Epoch 459/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 641093160456.6323 - mae: 503858.9062 - val_loss: 1598473131813.4482 - val_mae: 745424.2500\n",
      "Epoch 460/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 645321228599.3615 - mae: 505374.5312 - val_loss: 1581316156913.4797 - val_mae: 743866.3125\n",
      "Epoch 461/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 641401388940.2374 - mae: 504046.7188 - val_loss: 1575238080828.3513 - val_mae: 737982.1875\n",
      "Epoch 462/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 639886935826.6868 - mae: 503199.9375 - val_loss: 1576916453151.4604 - val_mae: 734792.9375\n",
      "Epoch 463/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 641966845853.4022 - mae: 503330.2188 - val_loss: 1590432462465.7839 - val_mae: 739587.3750\n",
      "Epoch 464/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 637822595604.1088 - mae: 502093.8125 - val_loss: 1577636993145.5508 - val_mae: 738508.1250\n",
      "Epoch 465/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 640788888474.3085 - mae: 503228.7500 - val_loss: 1601998491812.0635 - val_mae: 746491.1875\n",
      "Epoch 466/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 637315840115.9622 - mae: 501907.6562 - val_loss: 1588166117724.6848 - val_mae: 743407.2500\n",
      "Epoch 467/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 641659822254.8414 - mae: 502984.2812 - val_loss: 1589022960195.4119 - val_mae: 739801.8750\n",
      "Epoch 468/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 638211120276.1960 - mae: 501844.3750 - val_loss: 1588217683500.9578 - val_mae: 737600.8125\n",
      "Epoch 469/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 635743541549.9807 - mae: 502011.5312 - val_loss: 1616459080020.9009 - val_mae: 752390.6875\n",
      "Epoch 470/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 634109622250.6438 - mae: 500552.7188 - val_loss: 1572110702212.1790 - val_mae: 733903.6250\n",
      "Epoch 471/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 633755727609.5383 - mae: 500534.7500 - val_loss: 1574746226480.4258 - val_mae: 735340.1875\n",
      "Epoch 472/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 634124212434.9674 - mae: 500524.9375 - val_loss: 1610841793779.0017 - val_mae: 746669.8750\n",
      "Epoch 473/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 635239879185.7137 - mae: 500980.0000 - val_loss: 1571039568013.3103 - val_mae: 734312.9375\n",
      "Epoch 474/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 635614588610.6509 - mae: 500780.3125 - val_loss: 1606131730624.2058 - val_mae: 745631.0625\n",
      "Epoch 475/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 632202032772.9773 - mae: 499451.5938 - val_loss: 1582660682842.5144 - val_mae: 738384.0625\n",
      "Epoch 476/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 633575850772.7823 - mae: 501297.5625 - val_loss: 1586262192230.5896 - val_mae: 742891.5625\n",
      "Epoch 477/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 633597217753.7784 - mae: 500247.3125 - val_loss: 1582160062452.6233 - val_mae: 739121.0625\n",
      "Epoch 478/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 629018984682.7186 - mae: 499122.0000 - val_loss: 1603117229800.3735 - val_mae: 741550.0625\n",
      "Epoch 479/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 630734341617.6794 - mae: 499194.0625 - val_loss: 1590270026409.0034 - val_mae: 740766.3750\n",
      "Epoch 480/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 630381944436.8104 - mae: 499008.7500 - val_loss: 1589536152971.6885 - val_mae: 742543.3750\n",
      "Epoch 481/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 629550627680.8264 - mae: 499294.4688 - val_loss: 1573601921806.9941 - val_mae: 734366.2500\n",
      "Epoch 482/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 627313467477.4248 - mae: 497962.6250 - val_loss: 1650641064995.8264 - val_mae: 756149.3750\n",
      "Epoch 483/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 627562669005.2041 - mae: 498174.0000 - val_loss: 1596250634220.2405 - val_mae: 737162.1875\n",
      "Epoch 484/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 624458206088.3453 - mae: 496855.5000 - val_loss: 1649848247058.3872 - val_mae: 759086.0625\n",
      "Epoch 485/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 629295645060.0043 - mae: 499013.2188 - val_loss: 1595584910724.7029 - val_mae: 735839.3750\n",
      "Epoch 486/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 626494989602.9034 - mae: 496765.5000 - val_loss: 1597548527188.0776 - val_mae: 740512.6875\n",
      "Epoch 487/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 624170712449.3098 - mae: 497735.5000 - val_loss: 1585857517119.5198 - val_mae: 738048.3125\n",
      "Epoch 488/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 623349191524.2195 - mae: 497241.7812 - val_loss: 1624211848277.1255 - val_mae: 749044.2500\n",
      "Epoch 489/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 621446503810.1082 - mae: 495819.8750 - val_loss: 1625186086302.4500 - val_mae: 749217.4375\n",
      "Epoch 490/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 623803724447.0239 - mae: 496285.2812 - val_loss: 1573159026727.9182 - val_mae: 739754.0000\n",
      "Epoch 491/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 622184205047.8418 - mae: 495810.6875 - val_loss: 1582132697062.6521 - val_mae: 739925.5000\n",
      "Epoch 492/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 624458617610.8029 - mae: 497447.2500 - val_loss: 1628170367742.8274 - val_mae: 751455.2500\n",
      "Epoch 493/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 621959851254.7939 - mae: 496325.6875 - val_loss: 1586808442097.2053 - val_mae: 738082.8750\n",
      "Epoch 494/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 620403213935.8207 - mae: 495542.1875 - val_loss: 1567605768097.7932 - val_mae: 735602.8750\n",
      "Epoch 495/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 621030343906.2360 - mae: 495457.0625 - val_loss: 1620343440715.6199 - val_mae: 747270.6250\n",
      "Epoch 496/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 618583531642.1495 - mae: 495049.5000 - val_loss: 1586301218532.1821 - val_mae: 741048.6250\n",
      "Epoch 497/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 617513942513.0806 - mae: 493748.8438 - val_loss: 1617940672644.9275 - val_mae: 746208.0000\n",
      "Epoch 498/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 618737829764.0542 - mae: 494818.6875 - val_loss: 1622111086163.8779 - val_mae: 748091.5000\n",
      "Epoch 499/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 620161024434.3093 - mae: 495359.6875 - val_loss: 1577031542127.5461 - val_mae: 739335.8125\n",
      "Epoch 500/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 615232382739.4850 - mae: 493940.2812 - val_loss: 1582951422816.0281 - val_mae: 736967.5625\n",
      "Epoch 501/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 616076653717.9924 - mae: 493902.4688 - val_loss: 1636280730535.6812 - val_mae: 748248.6250\n",
      "Epoch 502/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 616671639813.2642 - mae: 493809.1562 - val_loss: 1621315210733.4880 - val_mae: 745526.2500\n",
      "Epoch 503/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 614428240381.1558 - mae: 492280.0312 - val_loss: 1587030049542.4119 - val_mae: 739749.8125\n",
      "Epoch 504/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 617330789297.0619 - mae: 494773.0312 - val_loss: 1580003072654.8569 - val_mae: 741717.8750\n",
      "Epoch 505/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 612260499893.1036 - mae: 492542.5000 - val_loss: 1617428519062.2917 - val_mae: 746524.6875\n",
      "Epoch 506/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 615124305267.5381 - mae: 492717.1562 - val_loss: 1608551391258.3459 - val_mae: 740757.6875\n",
      "Epoch 507/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 612589515339.9941 - mae: 492071.2812 - val_loss: 1612748642545.3052 - val_mae: 751388.8125\n",
      "Epoch 508/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 612081475399.5782 - mae: 491341.3750 - val_loss: 1634694578656.6143 - val_mae: 756533.4375\n",
      "Epoch 509/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 610129597000.0023 - mae: 491747.4375 - val_loss: 1597790114356.8416 - val_mae: 743906.2500\n",
      "Epoch 510/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 613800653906.9299 - mae: 492428.2812 - val_loss: 1624884429306.1619 - val_mae: 747971.6250\n",
      "Epoch 511/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 609314166484.0153 - mae: 491551.2812 - val_loss: 1604728704608.4521 - val_mae: 744282.3125\n",
      "Epoch 512/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 609333196335.5525 - mae: 490536.6562 - val_loss: 1604813144065.5967 - val_mae: 743551.5625\n",
      "Epoch 513/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 608395181008.5973 - mae: 490781.5625 - val_loss: 1595400469945.1953 - val_mae: 739039.6875\n",
      "Epoch 514/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 608576753322.7000 - mae: 490321.2500 - val_loss: 1611070413838.0710 - val_mae: 750478.3750\n",
      "Epoch 515/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 608532512866.9969 - mae: 490843.6875 - val_loss: 1598136926883.4148 - val_mae: 742381.8125\n",
      "Epoch 516/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 605859165482.4878 - mae: 489102.7500 - val_loss: 1591357164021.9705 - val_mae: 735326.0000\n",
      "Epoch 517/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 608272330752.3992 - mae: 491136.5000 - val_loss: 1638506600125.6611 - val_mae: 762242.7500\n",
      "Epoch 518/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 604850128588.4307 - mae: 488682.1250 - val_loss: 1586588442110.6528 - val_mae: 739039.8750\n",
      "Epoch 519/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 605910783974.6520 - mae: 489236.9688 - val_loss: 1568073225611.0898 - val_mae: 739667.9375\n",
      "Epoch 520/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 607123575712.8950 - mae: 489780.8750 - val_loss: 1645604570576.1482 - val_mae: 752539.5625\n",
      "Epoch 521/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 609131199863.7295 - mae: 490719.9062 - val_loss: 1615952647052.1377 - val_mae: 750390.3125\n",
      "Epoch 522/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 602532482134.3230 - mae: 488482.5938 - val_loss: 1612534460914.5776 - val_mae: 752869.3750\n",
      "Epoch 523/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 604882356633.3605 - mae: 489274.7188 - val_loss: 1580103059935.6165 - val_mae: 736764.5625\n",
      "Epoch 524/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 605148113315.7393 - mae: 488575.6875 - val_loss: 1614925589680.5378 - val_mae: 755457.0625\n",
      "Epoch 525/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 601362280855.5642 - mae: 488273.0938 - val_loss: 1652678743609.1328 - val_mae: 757194.5000\n",
      "Epoch 526/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 604558174677.2377 - mae: 488352.6875 - val_loss: 1604577856263.0107 - val_mae: 742805.5000\n",
      "Epoch 527/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 602210543708.4104 - mae: 488005.3438 - val_loss: 1636058515459.7922 - val_mae: 756511.9375\n",
      "Epoch 528/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 599255057793.9086 - mae: 487063.2500 - val_loss: 1581193660165.9128 - val_mae: 738135.8750\n",
      "Epoch 529/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 602968759725.2197 - mae: 488052.5000 - val_loss: 1612415005290.3320 - val_mae: 749678.0000\n",
      "Epoch 530/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 599797798359.7327 - mae: 487240.1250 - val_loss: 1618774478869.9551 - val_mae: 745039.6875\n",
      "Epoch 531/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 603038061241.3698 - mae: 488155.9375 - val_loss: 1622119060517.4233 - val_mae: 748320.4375\n",
      "Epoch 532/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 600207330862.9537 - mae: 487558.4688 - val_loss: 1600745299850.6406 - val_mae: 743805.7500\n",
      "Epoch 533/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 597603000314.4114 - mae: 486860.8438 - val_loss: 1605828061996.5337 - val_mae: 746663.6250\n",
      "Epoch 534/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 597103913604.8776 - mae: 485965.1562 - val_loss: 1605834658324.9072 - val_mae: 743232.8125\n",
      "Epoch 535/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 595328412648.9473 - mae: 485377.0625 - val_loss: 1581955014621.6704 - val_mae: 744287.6250\n",
      "Epoch 536/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 597081444156.8003 - mae: 485419.7500 - val_loss: 1587857592324.3911 - val_mae: 742914.5625\n",
      "Epoch 537/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 598835332068.3567 - mae: 486957.8125 - val_loss: 1615506630911.2764 - val_mae: 746990.1875\n",
      "Epoch 538/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 593391027680.1154 - mae: 484410.5938 - val_loss: 1593168954282.8745 - val_mae: 744371.5625\n",
      "Epoch 539/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 593115815655.7747 - mae: 484731.5625 - val_loss: 1607659513839.8333 - val_mae: 745838.1250\n",
      "Epoch 540/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 596888053601.7246 - mae: 485686.2500 - val_loss: 1563780722330.7327 - val_mae: 735221.6250\n",
      "Epoch 541/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 595363657957.3297 - mae: 485847.0938 - val_loss: 1684400509494.4385 - val_mae: 761088.0625\n",
      "Epoch 542/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 592612958710.1702 - mae: 484012.4375 - val_loss: 1636890039823.6179 - val_mae: 754298.8125\n",
      "Epoch 543/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 596213692121.6038 - mae: 484968.7812 - val_loss: 1613946244758.7410 - val_mae: 744111.3125\n",
      "Epoch 544/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 590222543290.8917 - mae: 483549.5312 - val_loss: 1629287590662.2122 - val_mae: 745470.2500\n",
      "Epoch 545/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 589747759086.2365 - mae: 482883.7500 - val_loss: 1625065454038.4353 - val_mae: 745019.7500\n",
      "Epoch 546/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 593937356638.9303 - mae: 484652.6875 - val_loss: 1636028087761.1460 - val_mae: 747512.7500\n",
      "Epoch 547/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 593458504416.8888 - mae: 484657.2188 - val_loss: 1621763132815.2812 - val_mae: 747549.8750\n",
      "Epoch 548/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 589346191354.6111 - mae: 482728.9688 - val_loss: 1605055949967.0068 - val_mae: 742882.5000\n",
      "Epoch 549/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 589829561407.5697 - mae: 483921.6875 - val_loss: 1634109866221.9121 - val_mae: 754541.3125\n",
      "Epoch 550/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 588106095157.4403 - mae: 482435.8750 - val_loss: 1602656364234.0356 - val_mae: 743373.4375\n",
      "Epoch 551/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 590144634969.4166 - mae: 482973.3125 - val_loss: 1598433772632.9177 - val_mae: 741997.3125\n",
      "Epoch 552/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 589024531526.9545 - mae: 482176.2500 - val_loss: 1661523881388.8206 - val_mae: 754759.8125\n",
      "Epoch 553/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 589237823205.1799 - mae: 482402.1875 - val_loss: 1617969915752.1116 - val_mae: 744449.2500\n",
      "Epoch 554/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 590276647372.0565 - mae: 482249.5000 - val_loss: 1615838362742.6567 - val_mae: 745849.7500\n",
      "Epoch 555/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 586343508208.9060 - mae: 482111.8438 - val_loss: 1661550361234.8489 - val_mae: 763082.0625\n",
      "Epoch 556/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 587829466154.2134 - mae: 481798.7812 - val_loss: 1621201596250.3398 - val_mae: 747448.3750\n",
      "Epoch 557/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 585506210860.2094 - mae: 481057.9062 - val_loss: 1625990515589.1519 - val_mae: 744306.8750\n",
      "Epoch 558/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 585503245436.0457 - mae: 480999.3750 - val_loss: 1641764188605.8855 - val_mae: 752754.4375\n",
      "Epoch 559/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 583479857149.2057 - mae: 480165.2812 - val_loss: 1625391087681.4658 - val_mae: 746301.6875\n",
      "Epoch 560/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 582833598525.0747 - mae: 479966.1562 - val_loss: 1648721573548.8955 - val_mae: 755174.1875\n",
      "Epoch 561/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 586815124728.9894 - mae: 480904.8750 - val_loss: 1587444267617.3503 - val_mae: 741344.1250\n",
      "Epoch 562/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 586854092331.7603 - mae: 481523.9062 - val_loss: 1609629383271.7373 - val_mae: 744870.6875\n",
      "Epoch 563/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 585907345066.5004 - mae: 480606.1875 - val_loss: 1643140709502.6404 - val_mae: 755133.1875\n",
      "Epoch 564/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 583248729995.6387 - mae: 479870.1875 - val_loss: 1634629543159.7917 - val_mae: 749475.4375\n",
      "Epoch 565/1000\n",
      "164176/164176 [==============================] - 16s 97us/step - loss: 582352066097.1492 - mae: 479754.7188 - val_loss: 1598328354882.4636 - val_mae: 744269.4375\n",
      "Epoch 566/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 579777965074.8613 - mae: 478694.6875 - val_loss: 1641539002490.1494 - val_mae: 759406.5000\n",
      "Epoch 567/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 579962770666.6189 - mae: 479777.1875 - val_loss: 1610655976583.8215 - val_mae: 746352.6250\n",
      "Epoch 568/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 582711208885.8521 - mae: 480263.8750 - val_loss: 1660908691290.7390 - val_mae: 763824.1250\n",
      "Epoch 569/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 578951610443.0461 - mae: 478341.5312 - val_loss: 1650088217798.0938 - val_mae: 762588.6250\n",
      "Epoch 570/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 583501726703.5338 - mae: 480888.2188 - val_loss: 1647223464671.2920 - val_mae: 756135.9375\n",
      "Epoch 571/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 577503446593.5157 - mae: 478243.6875 - val_loss: 1628930909369.5195 - val_mae: 746830.3750\n",
      "Epoch 572/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 580007403129.6007 - mae: 478790.3438 - val_loss: 1607992825886.5374 - val_mae: 744490.8125\n",
      "Epoch 573/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 577426171103.8409 - mae: 478017.9375 - val_loss: 1605231785073.2678 - val_mae: 743357.5000\n",
      "Epoch 574/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 575344636182.2294 - mae: 477943.9062 - val_loss: 1614130480978.5557 - val_mae: 746844.3125\n",
      "Epoch 575/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 574219915826.4465 - mae: 477105.5625 - val_loss: 1636716469531.9177 - val_mae: 752442.4375\n",
      "Epoch 576/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 576609929242.1464 - mae: 478107.5625 - val_loss: 1619065729936.1294 - val_mae: 748413.1875\n",
      "Epoch 577/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 572696097078.1639 - mae: 476433.0938 - val_loss: 1593480840585.8923 - val_mae: 741171.4375\n",
      "Epoch 578/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 574433254873.9281 - mae: 477171.2812 - val_loss: 1646145178291.9810 - val_mae: 757673.5000\n",
      "Epoch 579/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 577768485249.2100 - mae: 478422.8438 - val_loss: 1600292130079.5105 - val_mae: 744251.7500\n",
      "Epoch 580/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 576420989869.0701 - mae: 477560.3438 - val_loss: 1601680339658.1355 - val_mae: 746243.8750\n",
      "Epoch 581/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 573588790721.9772 - mae: 476004.5625 - val_loss: 1694025756547.8545 - val_mae: 770480.5000\n",
      "Epoch 582/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 573638942257.7480 - mae: 475652.9375 - val_loss: 1619289692864.3555 - val_mae: 747246.0625\n",
      "Epoch 583/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 574825978035.0328 - mae: 476343.8125 - val_loss: 1647151247754.7903 - val_mae: 751653.0000\n",
      "Epoch 584/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 572478793114.6578 - mae: 476205.3125 - val_loss: 1615373015896.0444 - val_mae: 745455.5000\n",
      "Epoch 585/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 573357569820.6663 - mae: 476009.0000 - val_loss: 1633891599963.0632 - val_mae: 754264.4375\n",
      "Epoch 586/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 573990903117.4161 - mae: 476569.2500 - val_loss: 1630160767870.7651 - val_mae: 753726.1250\n",
      "Epoch 587/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 573014036834.0739 - mae: 475499.7500 - val_loss: 1600145454399.2454 - val_mae: 747196.7500\n",
      "Epoch 588/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 570064591253.7678 - mae: 475237.6562 - val_loss: 1636978129251.4709 - val_mae: 747282.3750\n",
      "Epoch 589/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 571762805559.7108 - mae: 475533.3750 - val_loss: 1640795674113.2476 - val_mae: 752238.0000\n",
      "Epoch 590/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 566077236655.4152 - mae: 473647.9375 - val_loss: 1618757746221.7561 - val_mae: 748666.6250\n",
      "Epoch 591/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 575616334158.7136 - mae: 476903.5625 - val_loss: 1664249891463.2727 - val_mae: 762604.0000\n",
      "Epoch 592/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 572824550293.5184 - mae: 475627.0938 - val_loss: 1714243717018.4084 - val_mae: 775310.2500\n",
      "Epoch 593/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 571304342060.8580 - mae: 474958.4062 - val_loss: 1663899374150.4055 - val_mae: 759419.7500\n",
      "Epoch 594/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 570461646589.7297 - mae: 474594.4062 - val_loss: 1644420404015.5276 - val_mae: 751441.6250\n",
      "Epoch 595/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 563501594441.3745 - mae: 472968.5312 - val_loss: 1642021927815.1479 - val_mae: 751371.3750\n",
      "Epoch 596/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 570614427934.2130 - mae: 474672.3125 - val_loss: 1621547831343.5027 - val_mae: 748998.3750\n",
      "Epoch 597/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 564518801549.7095 - mae: 472912.8438 - val_loss: 1628367353911.2866 - val_mae: 748264.0625\n",
      "Epoch 598/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 566989928475.3439 - mae: 473981.9688 - val_loss: 1635746466578.1877 - val_mae: 755091.5000\n",
      "Epoch 599/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 566891137343.9438 - mae: 473503.0625 - val_loss: 1658909861263.4807 - val_mae: 751752.1250\n",
      "Epoch 600/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 565001614707.9373 - mae: 472408.4062 - val_loss: 1632807909817.6941 - val_mae: 749814.3750\n",
      "Epoch 601/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 566046827960.9956 - mae: 472889.7500 - val_loss: 1612228296726.3542 - val_mae: 747437.3750\n",
      "Epoch 602/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 564009375366.5741 - mae: 472178.3750 - val_loss: 1650727617313.5562 - val_mae: 752230.8750\n",
      "Epoch 603/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 565510175273.4650 - mae: 472760.1875 - val_loss: 1653377786077.1465 - val_mae: 759533.9375\n",
      "Epoch 604/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 561291049598.0914 - mae: 471317.3750 - val_loss: 1657771584937.6272 - val_mae: 756479.8125\n",
      "Epoch 605/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 564012090902.4041 - mae: 473004.7188 - val_loss: 1644987609709.5254 - val_mae: 753702.3750\n",
      "Epoch 606/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 562977649960.8911 - mae: 471748.6562 - val_loss: 1613575543335.0698 - val_mae: 746067.8125\n",
      "Epoch 607/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 562884004824.1816 - mae: 472424.5000 - val_loss: 1640785816728.5872 - val_mae: 748030.1250\n",
      "Epoch 608/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 559874952687.0847 - mae: 471218.8750 - val_loss: 1646722593672.1458 - val_mae: 758382.1875\n",
      "Epoch 609/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 564314825083.2223 - mae: 472645.8125 - val_loss: 1636104351955.1670 - val_mae: 750274.8750\n",
      "Epoch 610/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 561788225022.4531 - mae: 471074.2188 - val_loss: 1638650312475.0696 - val_mae: 756136.4375\n",
      "Epoch 611/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 562166782919.8152 - mae: 471114.4375 - val_loss: 1616377893173.3655 - val_mae: 747481.3125\n",
      "Epoch 612/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 562775994263.0153 - mae: 470918.5000 - val_loss: 1609211475010.9626 - val_mae: 747165.7500\n",
      "Epoch 613/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 557578995489.0573 - mae: 470184.5938 - val_loss: 1629174983201.8806 - val_mae: 750692.1875\n",
      "Epoch 614/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 558711493760.4365 - mae: 470445.8125 - val_loss: 1623226249192.4482 - val_mae: 749935.6875\n",
      "Epoch 615/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 560014773921.9180 - mae: 470448.6250 - val_loss: 1662891215423.4199 - val_mae: 758226.8125\n",
      "Epoch 616/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 560025814871.3458 - mae: 470049.3750 - val_loss: 1627244699315.0828 - val_mae: 749570.5000\n",
      "Epoch 617/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 558183195094.2356 - mae: 470185.9375 - val_loss: 1644113075501.4817 - val_mae: 754403.0000\n",
      "Epoch 618/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 561963846387.8500 - mae: 471334.5000 - val_loss: 1680798678078.7712 - val_mae: 759676.2500\n",
      "Epoch 619/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 554525343526.0470 - mae: 468449.8125 - val_loss: 1655741361243.9116 - val_mae: 756064.5625\n",
      "Epoch 620/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 553406298406.7954 - mae: 467873.5938 - val_loss: 1679267380278.7876 - val_mae: 762869.2500\n",
      "Epoch 621/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 557113967556.4221 - mae: 469639.4688 - val_loss: 1646607719851.6230 - val_mae: 753599.7500\n",
      "Epoch 622/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 557942938494.3658 - mae: 469723.3438 - val_loss: 1675904291190.8313 - val_mae: 758980.6875\n",
      "Epoch 623/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 556459124081.1429 - mae: 469271.9688 - val_loss: 1646224172247.8574 - val_mae: 755534.0000\n",
      "Epoch 624/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 556768378135.9259 - mae: 469574.0625 - val_loss: 1687927493235.1140 - val_mae: 770181.5625\n",
      "Epoch 625/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 553005052643.6831 - mae: 468340.0938 - val_loss: 1654686792758.7876 - val_mae: 758036.9375\n",
      "Epoch 626/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 555760823373.5410 - mae: 467718.9062 - val_loss: 1620349880456.3206 - val_mae: 751327.7500\n",
      "Epoch 627/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 551983440556.1969 - mae: 466825.9375 - val_loss: 1626199436878.0898 - val_mae: 746524.6875\n",
      "Epoch 628/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 555631785957.8536 - mae: 468673.2500 - val_loss: 1659509523985.6138 - val_mae: 757448.8125\n",
      "Epoch 629/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 554907998272.0686 - mae: 467540.0312 - val_loss: 1657545941411.1404 - val_mae: 753440.4375\n",
      "Epoch 630/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 552396248600.3999 - mae: 467471.7812 - val_loss: 1609576445071.6055 - val_mae: 748325.8750\n",
      "Epoch 631/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 552313912187.9708 - mae: 467385.6875 - val_loss: 1666850631603.7563 - val_mae: 758664.6250\n",
      "Epoch 632/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 552401772153.6007 - mae: 467457.2500 - val_loss: 1641884097495.7825 - val_mae: 758775.1875\n",
      "Epoch 633/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 554869646068.0496 - mae: 468073.0000 - val_loss: 1633721113452.8018 - val_mae: 746955.8750\n",
      "Epoch 634/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 553586965924.4377 - mae: 467678.0312 - val_loss: 1644231598695.5376 - val_mae: 751355.5000\n",
      "Epoch 635/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 552133752997.4606 - mae: 466867.1562 - val_loss: 1639432525089.7058 - val_mae: 756713.1250\n",
      "Epoch 636/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 550008366931.3540 - mae: 466818.5625 - val_loss: 1695099970810.9854 - val_mae: 767607.3125\n",
      "Epoch 637/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 550451839868.5696 - mae: 466696.4688 - val_loss: 1656403584065.7651 - val_mae: 754539.5000\n",
      "Epoch 638/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 549667142270.0914 - mae: 466186.8438 - val_loss: 1625141614866.8364 - val_mae: 753478.6250\n",
      "Epoch 639/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 548690881382.0158 - mae: 466425.5000 - val_loss: 1620103617487.7988 - val_mae: 751804.6875\n",
      "Epoch 640/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 545772329967.7334 - mae: 465027.0312 - val_loss: 1648622135407.5713 - val_mae: 754236.3125\n",
      "Epoch 641/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 546477415208.1426 - mae: 465335.5000 - val_loss: 1639168015291.8398 - val_mae: 754954.4375\n",
      "Epoch 642/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 547502460387.9076 - mae: 465472.2188 - val_loss: 1638216817025.0105 - val_mae: 753086.3750\n",
      "Epoch 643/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 547320059478.6723 - mae: 465239.3438 - val_loss: 1631244516865.0479 - val_mae: 750039.5625\n",
      "Epoch 644/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 545837395859.0235 - mae: 464475.9062 - val_loss: 1708229416485.9722 - val_mae: 767814.5000\n",
      "Epoch 645/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 545939279781.3858 - mae: 465169.2500 - val_loss: 1632598529304.1255 - val_mae: 750175.7500\n",
      "Epoch 646/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 545929455692.4432 - mae: 464491.8438 - val_loss: 1639567671051.7009 - val_mae: 753330.5000\n",
      "Epoch 647/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 547153358961.7667 - mae: 464729.9062 - val_loss: 1643860384058.7544 - val_mae: 760664.4375\n",
      "Epoch 648/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 547877451097.5913 - mae: 465054.3125 - val_loss: 1652103851546.9946 - val_mae: 757536.2500\n",
      "Epoch 649/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 545273671725.1075 - mae: 464355.1562 - val_loss: 1671802324649.3027 - val_mae: 760406.1875\n",
      "Epoch 650/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 545258315605.9487 - mae: 463462.5000 - val_loss: 1647307806028.8174 - val_mae: 754178.8750\n",
      "Epoch 651/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 543553772522.9432 - mae: 463695.2188 - val_loss: 1626851121976.1099 - val_mae: 749715.0625\n",
      "Epoch 652/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 546004434995.8936 - mae: 464130.1562 - val_loss: 1632276493958.5740 - val_mae: 749860.7500\n",
      "Epoch 653/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 542815734552.5746 - mae: 463897.1562 - val_loss: 1654426183020.3528 - val_mae: 757534.5625\n",
      "Epoch 654/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 542161122273.5624 - mae: 463226.2812 - val_loss: 1642703598235.8303 - val_mae: 754119.4375\n",
      "Epoch 655/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 548004542886.5335 - mae: 465154.7188 - val_loss: 1630790581713.1460 - val_mae: 756358.0000\n",
      "Epoch 656/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 543026190288.2978 - mae: 462747.8438 - val_loss: 1627951183532.8955 - val_mae: 751812.8750\n",
      "Epoch 657/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 541212657916.8814 - mae: 462781.1875 - val_loss: 1642764928001.0977 - val_mae: 751403.2500\n",
      "Epoch 658/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 543209386862.5982 - mae: 463956.7188 - val_loss: 1667452094486.9529 - val_mae: 758751.8750\n",
      "Epoch 659/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 539388836273.7106 - mae: 462095.0312 - val_loss: 1667770071384.2939 - val_mae: 759734.6250\n",
      "Epoch 660/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 541289805360.3508 - mae: 463376.7500 - val_loss: 1700840007334.9077 - val_mae: 767493.5625\n",
      "Epoch 661/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 536386417628.7723 - mae: 461556.2500 - val_loss: 1648000684868.2849 - val_mae: 756396.0000\n",
      "Epoch 662/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 542366645903.1564 - mae: 463017.0625 - val_loss: 1625871488308.8667 - val_mae: 751587.9375\n",
      "Epoch 663/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 539017744235.1053 - mae: 462078.8750 - val_loss: 1684744324980.0869 - val_mae: 759980.6875\n",
      "Epoch 664/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 542377971700.4237 - mae: 463323.8750 - val_loss: 1641793476242.5496 - val_mae: 754794.7500\n",
      "Epoch 665/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 536791848208.1419 - mae: 461444.5312 - val_loss: 1681405414773.7336 - val_mae: 760621.1250\n",
      "Epoch 666/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 536950972362.5098 - mae: 460913.1562 - val_loss: 1644852199987.6440 - val_mae: 752008.6250\n",
      "Epoch 667/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 539321889373.5582 - mae: 462018.9375 - val_loss: 1666641132760.0569 - val_mae: 759993.7500\n",
      "Epoch 668/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 538004508527.0972 - mae: 461124.6250 - val_loss: 1658287705693.2588 - val_mae: 755417.6250\n",
      "Epoch 669/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 536973887645.0779 - mae: 461581.3438 - val_loss: 1662785807412.9912 - val_mae: 762464.4375\n",
      "Epoch 670/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 536622719534.0555 - mae: 461036.2188 - val_loss: 1675091893151.8970 - val_mae: 759525.1250\n",
      "Epoch 671/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 536345867960.0725 - mae: 460469.4688 - val_loss: 1646107480351.0115 - val_mae: 754815.5625\n",
      "Epoch 672/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 539201942868.0027 - mae: 461297.4375 - val_loss: 1665143104893.9167 - val_mae: 757619.6875\n",
      "Epoch 673/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 539325043683.9575 - mae: 461468.0938 - val_loss: 1680582499372.4089 - val_mae: 758191.9375\n",
      "Epoch 674/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 535389448261.2579 - mae: 460247.7188 - val_loss: 1661446487907.5210 - val_mae: 755043.5000\n",
      "Epoch 675/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 536466776464.7781 - mae: 460604.1875 - val_loss: 1655440942431.5791 - val_mae: 752936.9375\n",
      "Epoch 676/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 534240095231.3014 - mae: 459871.0938 - val_loss: 1673073189519.4558 - val_mae: 759412.1250\n",
      "Epoch 677/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 533630122831.2625 - mae: 459686.3750 - val_loss: 1656266771179.8662 - val_mae: 755536.5625\n",
      "Epoch 678/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 534200640947.4070 - mae: 459652.5938 - val_loss: 1677356423546.6235 - val_mae: 761298.9375\n",
      "Epoch 679/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 531781479140.6810 - mae: 458950.3438 - val_loss: 1704950457495.9883 - val_mae: 772840.0000\n",
      "Epoch 680/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 535694240331.3953 - mae: 460364.7500 - val_loss: 1662633434878.1289 - val_mae: 775495.0000\n",
      "Epoch 681/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 531105089699.4647 - mae: 458468.4688 - val_loss: 1658437095621.5947 - val_mae: 767507.3750\n",
      "Epoch 682/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 533880497401.3885 - mae: 459447.6250 - val_loss: 1647020901051.3657 - val_mae: 755665.3125\n",
      "Epoch 683/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 528488887307.0773 - mae: 457842.8750 - val_loss: 1652014213499.8210 - val_mae: 753950.9375\n",
      "Epoch 684/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 532111715199.5634 - mae: 458594.9688 - val_loss: 1653342281168.3477 - val_mae: 756965.5625\n",
      "Epoch 685/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 529893414224.0109 - mae: 458281.0312 - val_loss: 1642218379770.3616 - val_mae: 755602.0625\n",
      "Epoch 686/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 529302835785.5991 - mae: 457749.0000 - val_loss: 1628819186887.8901 - val_mae: 751797.3125\n",
      "Epoch 687/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 528745722406.4711 - mae: 457145.9062 - val_loss: 1679606274912.0281 - val_mae: 773311.8125\n",
      "Epoch 688/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 529384479851.4796 - mae: 458072.0000 - val_loss: 1646884116727.6921 - val_mae: 752902.6875\n",
      "Epoch 689/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 531407019178.5503 - mae: 457854.8750 - val_loss: 1665799255554.0459 - val_mae: 757022.1875\n",
      "Epoch 690/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 530983601996.1688 - mae: 458265.6562 - val_loss: 1652986997304.2346 - val_mae: 756894.4375\n",
      "Epoch 691/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 530340639675.2410 - mae: 457928.1875 - val_loss: 1675868619718.0188 - val_mae: 760016.7500\n",
      "Epoch 692/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 526382143568.0359 - mae: 456523.9688 - val_loss: 1631895121484.4932 - val_mae: 748528.6875\n",
      "Epoch 693/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 531240404657.2864 - mae: 458672.9062 - val_loss: 1667777030903.8418 - val_mae: 758211.8750\n",
      "Epoch 694/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 529642962064.2042 - mae: 457821.8125 - val_loss: 1640821033658.3679 - val_mae: 752882.8750\n",
      "Epoch 695/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 527224040143.9236 - mae: 456522.4375 - val_loss: 1674082431867.3721 - val_mae: 758894.8750\n",
      "Epoch 696/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 529376203906.0333 - mae: 457865.3750 - val_loss: 1653847667604.9197 - val_mae: 752726.3750\n",
      "Epoch 697/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 524842724517.3608 - mae: 456081.9688 - val_loss: 1725153123967.9875 - val_mae: 768739.5625\n",
      "Epoch 698/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 525855865461.3094 - mae: 456462.7500 - val_loss: 1688449610384.6533 - val_mae: 765192.9375\n",
      "Epoch 699/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 527009322219.4172 - mae: 456949.9062 - val_loss: 1696035692490.5098 - val_mae: 763794.2500\n",
      "Epoch 700/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 528013805047.2679 - mae: 457051.1250 - val_loss: 1676782520406.8220 - val_mae: 761180.0625\n",
      "Epoch 701/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 525698707889.1118 - mae: 455145.5312 - val_loss: 1682494680722.7490 - val_mae: 760789.5000\n",
      "Epoch 702/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 525190624046.7292 - mae: 455504.3125 - val_loss: 1650309891545.7285 - val_mae: 754893.6875\n",
      "Epoch 703/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 523728210497.0167 - mae: 455634.5312 - val_loss: 1632793777143.1182 - val_mae: 750526.5625\n",
      "Epoch 704/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 521941223788.6522 - mae: 454133.2812 - val_loss: 1631622701131.8445 - val_mae: 752367.5000\n",
      "Epoch 705/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 525124642429.3928 - mae: 455432.1562 - val_loss: 1689022509950.5654 - val_mae: 767114.4375\n",
      "Epoch 706/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 523271422819.8203 - mae: 455318.8438 - val_loss: 1677243948640.0530 - val_mae: 758883.8750\n",
      "Epoch 707/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 525367667527.6780 - mae: 455313.5000 - val_loss: 1634483579095.3584 - val_mae: 749932.6875\n",
      "Epoch 708/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 526631545694.0322 - mae: 456069.6875 - val_loss: 1670150271049.8486 - val_mae: 763484.4375\n",
      "Epoch 709/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 521550537468.0331 - mae: 454368.4375 - val_loss: 1634162756539.4407 - val_mae: 746970.7500\n",
      "Epoch 710/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 521681138548.9851 - mae: 454548.9062 - val_loss: 1663907309920.6768 - val_mae: 754344.6250\n",
      "Epoch 711/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 520950818876.4760 - mae: 454023.8438 - val_loss: 1671253290748.1328 - val_mae: 760967.8125\n",
      "Epoch 712/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 522111730366.5593 - mae: 454312.9688 - val_loss: 1638775004177.2646 - val_mae: 749930.3750\n",
      "Epoch 713/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 519614737737.0253 - mae: 452952.5312 - val_loss: 1674246258716.7410 - val_mae: 756434.0000\n",
      "Epoch 714/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 517293700488.6947 - mae: 452917.7500 - val_loss: 1651068212076.2031 - val_mae: 754141.2500\n",
      "Epoch 715/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 519764954824.2393 - mae: 453588.3438 - val_loss: 1700588655334.0781 - val_mae: 770358.6875\n",
      "Epoch 716/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 520393015699.9716 - mae: 453483.2188 - val_loss: 1660603926606.8384 - val_mae: 757301.1875\n",
      "Epoch 717/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 518977650205.7889 - mae: 452380.6250 - val_loss: 1658959561106.3748 - val_mae: 756648.3750\n",
      "Epoch 718/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 519729994421.1784 - mae: 453407.4688 - val_loss: 1673746437641.2310 - val_mae: 760106.3125\n",
      "Epoch 719/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 521476887947.0897 - mae: 453367.9062 - val_loss: 1729320140677.0522 - val_mae: 777562.6250\n",
      "Epoch 720/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 515964057430.1483 - mae: 452058.5625 - val_loss: 1667748661569.5405 - val_mae: 758919.9375\n",
      "Epoch 721/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 520852176586.2352 - mae: 453834.4375 - val_loss: 1678002634526.8618 - val_mae: 759235.2500\n",
      "Epoch 722/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 519025528419.6457 - mae: 452928.9062 - val_loss: 1657903469147.1631 - val_mae: 752381.0000\n",
      "Epoch 723/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 518973832542.1818 - mae: 452866.4062 - val_loss: 1688253856077.1169 - val_mae: 766271.6250\n",
      "Epoch 724/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 520147933497.1578 - mae: 454206.0625 - val_loss: 1654661731085.9963 - val_mae: 755069.9375\n",
      "Epoch 725/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 515393731667.5287 - mae: 451279.5312 - val_loss: 1697331794992.6003 - val_mae: 767995.6250\n",
      "Epoch 726/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 515066121853.1932 - mae: 450995.0000 - val_loss: 1721150793906.6335 - val_mae: 766397.5000\n",
      "Epoch 727/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 519534660432.4600 - mae: 453477.6250 - val_loss: 1674251400958.3284 - val_mae: 760955.8125\n",
      "Epoch 728/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 516155339231.4168 - mae: 451537.0000 - val_loss: 1672833801961.3713 - val_mae: 756836.5625\n",
      "Epoch 729/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 516371091472.3664 - mae: 451876.5938 - val_loss: 1651248030942.2441 - val_mae: 753045.5000\n",
      "Epoch 730/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 515512137285.9066 - mae: 451884.0000 - val_loss: 1650827019950.0930 - val_mae: 756416.0625\n",
      "Epoch 731/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 513607374485.7429 - mae: 450777.1562 - val_loss: 1692122671633.9133 - val_mae: 764735.6250\n",
      "Epoch 732/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 515599453377.3035 - mae: 451123.7500 - val_loss: 1678441750725.1956 - val_mae: 764332.2500\n",
      "Epoch 733/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 514221382567.8807 - mae: 450969.0000 - val_loss: 1665720108896.9263 - val_mae: 755134.8750\n",
      "Epoch 734/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 515736705749.1129 - mae: 451430.0625 - val_loss: 1725192357971.6284 - val_mae: 773925.2500\n",
      "Epoch 735/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 513101017409.3409 - mae: 450562.2500 - val_loss: 1692993974708.3052 - val_mae: 765709.2500\n",
      "Epoch 736/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 513235751957.4559 - mae: 450712.3750 - val_loss: 1651131023149.2322 - val_mae: 754756.5000\n",
      "Epoch 737/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 511082009145.6318 - mae: 449928.2500 - val_loss: 1648557946935.2866 - val_mae: 753292.9375\n",
      "Epoch 738/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 511741296197.7071 - mae: 449964.6250 - val_loss: 1646553677854.3379 - val_mae: 754190.8750\n",
      "Epoch 739/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 513777239081.8143 - mae: 449846.7188 - val_loss: 1671971193165.7156 - val_mae: 761269.8125\n",
      "Epoch 740/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 510485586067.6971 - mae: 450411.0938 - val_loss: 1730736636553.8672 - val_mae: 774299.8125\n",
      "Epoch 741/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 511687825162.3038 - mae: 449933.3438 - val_loss: 1650754705569.4688 - val_mae: 754777.1250\n",
      "Epoch 742/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 515362563107.4273 - mae: 450970.0625 - val_loss: 1711155103760.5659 - val_mae: 769665.3750\n",
      "Epoch 743/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 510586441346.7818 - mae: 448917.1250 - val_loss: 1679796128092.6848 - val_mae: 765947.9375\n",
      "Epoch 744/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 510341147843.6987 - mae: 448946.8125 - val_loss: 1681553451317.5652 - val_mae: 762734.5000\n",
      "Epoch 745/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 511844473103.6429 - mae: 449871.5000 - val_loss: 1667072551553.6841 - val_mae: 758062.6250\n",
      "Epoch 746/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 510146065969.9476 - mae: 448686.7812 - val_loss: 1664390748599.2991 - val_mae: 759359.8750\n",
      "Epoch 747/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 512393886318.1241 - mae: 450807.7812 - val_loss: 1704219469673.7083 - val_mae: 769514.6250\n",
      "Epoch 748/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 507573697111.7700 - mae: 449019.0625 - val_loss: 1724412419209.0190 - val_mae: 772269.8125\n",
      "Epoch 749/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 510426155024.5660 - mae: 449121.1875 - val_loss: 1664512800741.8535 - val_mae: 757076.6250\n",
      "Epoch 750/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 506337636377.9468 - mae: 447489.3125 - val_loss: 1634818180861.9292 - val_mae: 751783.8125\n",
      "Epoch 751/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 507239917628.4760 - mae: 448325.2188 - val_loss: 1669126884077.4631 - val_mae: 756727.2500\n",
      "Epoch 752/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 508116438728.2393 - mae: 449175.9375 - val_loss: 1656566443088.7344 - val_mae: 756626.8750\n",
      "Epoch 753/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 508027497753.1235 - mae: 448415.3438 - val_loss: 1756595690877.3181 - val_mae: 781302.6250\n",
      "Epoch 754/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 508849194332.3855 - mae: 447966.1250 - val_loss: 1661502212987.5715 - val_mae: 757561.1875\n",
      "Epoch 755/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 506686841651.9185 - mae: 447769.7188 - val_loss: 1643398959453.2837 - val_mae: 749707.6875\n",
      "Epoch 756/1000\n",
      "164176/164176 [==============================] - 17s 102us/step - loss: 507929060017.7854 - mae: 448430.2812 - val_loss: 1699284289049.5974 - val_mae: 765271.4375\n",
      "Epoch 757/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 508407144170.9681 - mae: 448469.3750 - val_loss: 1680959993496.1379 - val_mae: 762183.9375\n",
      "Epoch 758/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 506413563401.0315 - mae: 447438.6875 - val_loss: 1703527202501.4451 - val_mae: 764488.3750\n",
      "Epoch 759/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 505997601831.2195 - mae: 447989.7812 - val_loss: 1656110220844.7583 - val_mae: 754551.2500\n",
      "Epoch 760/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 507431130601.2965 - mae: 447169.2500 - val_loss: 1706213955004.5881 - val_mae: 768982.3750\n",
      "Epoch 761/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 506772573199.2687 - mae: 447699.5625 - val_loss: 1639691761337.6692 - val_mae: 755122.4375\n",
      "Epoch 762/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 506931573317.6073 - mae: 448508.0625 - val_loss: 1647252891957.5652 - val_mae: 757302.0000\n",
      "Epoch 763/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 510138842205.1091 - mae: 449281.2188 - val_loss: 1660497453446.3994 - val_mae: 753787.5000\n",
      "Epoch 764/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 502699003499.6293 - mae: 445669.7812 - val_loss: 1660555568664.7991 - val_mae: 758268.3125\n",
      "Epoch 765/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 503357424744.8849 - mae: 446993.3750 - val_loss: 1655081183536.7749 - val_mae: 757218.6875\n",
      "Epoch 766/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 504382847957.3873 - mae: 446807.1562 - val_loss: 1668441040978.8301 - val_mae: 762430.1875\n",
      "Epoch 767/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 506165211505.2427 - mae: 447165.8750 - val_loss: 1742064018232.7087 - val_mae: 779508.1875\n",
      "Epoch 768/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 504953272039.1759 - mae: 446912.8438 - val_loss: 1659320812361.4744 - val_mae: 759571.6250\n",
      "Epoch 769/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 500729351594.4255 - mae: 444978.3438 - val_loss: 1710989640867.9639 - val_mae: 770055.4375\n",
      "Epoch 770/1000\n",
      "164176/164176 [==============================] - 16s 97us/step - loss: 503842768941.3071 - mae: 446424.5312 - val_loss: 1697632308586.5564 - val_mae: 766654.9375\n",
      "Epoch 771/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 505916802984.7789 - mae: 447280.2188 - val_loss: 1733411037491.3696 - val_mae: 780550.5000\n",
      "Epoch 772/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 503969615648.0593 - mae: 445968.5625 - val_loss: 1700795641251.2402 - val_mae: 770139.7500\n",
      "Epoch 773/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 497778916611.1685 - mae: 444011.9688 - val_loss: 1674652866919.4629 - val_mae: 759768.3750\n",
      "Epoch 774/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 498377396918.1764 - mae: 444591.3438 - val_loss: 1702642058365.6423 - val_mae: 765260.0625\n",
      "Epoch 775/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 502153161394.3842 - mae: 445736.6875 - val_loss: 1691712589748.5547 - val_mae: 763339.3750\n",
      "Epoch 776/1000\n",
      "164176/164176 [==============================] - 16s 97us/step - loss: 501679416347.4437 - mae: 445059.3750 - val_loss: 1666005896648.7632 - val_mae: 758155.2500\n",
      "Epoch 777/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 502059688818.1908 - mae: 445777.0000 - val_loss: 1693895625772.8081 - val_mae: 765785.3125\n",
      "Epoch 778/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 497383305090.2579 - mae: 443482.5625 - val_loss: 1685029795848.9817 - val_mae: 762746.6875\n",
      "Epoch 779/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 503335947623.3630 - mae: 445862.7812 - val_loss: 1666622805621.8083 - val_mae: 756943.5625\n",
      "Epoch 780/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 499394219895.0808 - mae: 443948.4062 - val_loss: 1659614937118.3379 - val_mae: 759358.3125\n",
      "Epoch 781/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 499654281824.1528 - mae: 445198.1875 - val_loss: 1703418085795.9387 - val_mae: 774267.7500\n",
      "Epoch 782/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 503132620577.0572 - mae: 446476.9375 - val_loss: 1692178512942.8040 - val_mae: 763059.6875\n",
      "Epoch 783/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 502182618388.5330 - mae: 445719.0625 - val_loss: 1678472059748.8184 - val_mae: 759025.6250\n",
      "Epoch 784/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 499220165371.6340 - mae: 445003.0938 - val_loss: 1670750086770.2158 - val_mae: 763375.0000\n",
      "Epoch 785/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 500514058492.6818 - mae: 444542.3438 - val_loss: 1659475570397.7952 - val_mae: 755903.8125\n",
      "Epoch 786/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 496873131689.9016 - mae: 444430.7812 - val_loss: 1676903798674.0256 - val_mae: 757188.7500\n",
      "Epoch 787/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 499062519430.5740 - mae: 444404.6250 - val_loss: 1659070523390.2036 - val_mae: 762885.4375\n",
      "Epoch 788/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 498766029403.5623 - mae: 444013.3750 - val_loss: 1679866157616.6501 - val_mae: 764322.8125\n",
      "Epoch 789/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 497626795203.6987 - mae: 443998.3438 - val_loss: 1730992974853.6882 - val_mae: 774802.8750\n",
      "Epoch 790/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 497522838773.9955 - mae: 443220.3125 - val_loss: 1697235195726.9631 - val_mae: 768423.8750\n",
      "Epoch 791/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 496471107716.3286 - mae: 443921.6562 - val_loss: 1728808411120.8311 - val_mae: 771223.8750\n",
      "Epoch 792/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 497000850482.5963 - mae: 443509.3438 - val_loss: 1684512903913.2717 - val_mae: 762615.7500\n",
      "Epoch 793/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 494258259550.2567 - mae: 441785.2812 - val_loss: 1727596130056.9067 - val_mae: 777221.8125\n",
      "Epoch 794/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 497228391433.8798 - mae: 444092.8125 - val_loss: 1711151989603.0220 - val_mae: 776716.1250\n",
      "Epoch 795/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 497550711032.4904 - mae: 443804.5000 - val_loss: 1685356452986.6484 - val_mae: 764141.1875\n",
      "Epoch 796/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 493613299530.9713 - mae: 442185.5625 - val_loss: 1677482453125.9253 - val_mae: 762584.8750\n",
      "Epoch 797/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 495896479383.7388 - mae: 443215.0000 - val_loss: 1715036320432.5879 - val_mae: 770512.1250\n",
      "Epoch 798/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 492209855514.8450 - mae: 441427.1250 - val_loss: 1704483622903.6172 - val_mae: 772941.2500\n",
      "Epoch 799/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 494428226222.5920 - mae: 442314.7188 - val_loss: 1715501075737.1235 - val_mae: 766656.6250\n",
      "Epoch 800/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 496414461597.1277 - mae: 443070.9062 - val_loss: 1678189483449.3948 - val_mae: 760879.5625\n",
      "Epoch 801/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 492629106677.6213 - mae: 441639.7500 - val_loss: 1662654534864.2729 - val_mae: 757839.8125\n",
      "Epoch 802/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 494224659681.8369 - mae: 441263.6562 - val_loss: 1687009639309.5347 - val_mae: 763987.2500\n",
      "Epoch 803/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 492011102782.0228 - mae: 441371.5312 - val_loss: 1634372661283.5271 - val_mae: 751255.4375\n",
      "Epoch 804/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 492454326917.4762 - mae: 441123.5000 - val_loss: 1753035221906.7241 - val_mae: 777819.1875\n",
      "Epoch 805/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 493776272458.7468 - mae: 441950.6562 - val_loss: 1688725156372.1086 - val_mae: 764030.3750\n",
      "Epoch 806/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 491241272623.2780 - mae: 441237.9688 - val_loss: 1721770836510.8867 - val_mae: 766871.0625\n",
      "Epoch 807/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 492194926162.0817 - mae: 440902.4688 - val_loss: 1696446797385.1001 - val_mae: 763030.3125\n",
      "Epoch 808/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 491097470648.0725 - mae: 440598.2812 - val_loss: 1704521617541.7258 - val_mae: 765049.4375\n",
      "Epoch 809/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 491625914679.3615 - mae: 441155.0312 - val_loss: 1707718899138.1768 - val_mae: 769394.3750\n",
      "Epoch 810/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 491185597830.4991 - mae: 441261.4062 - val_loss: 1734540713713.3550 - val_mae: 775447.0000\n",
      "Epoch 811/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 492144652869.6073 - mae: 440879.9375 - val_loss: 1688897828403.5442 - val_mae: 766566.0000\n",
      "Epoch 812/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 491956828327.0574 - mae: 440988.2812 - val_loss: 1737101222075.6152 - val_mae: 774313.5000\n",
      "Epoch 813/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 489199626500.0667 - mae: 439741.4375 - val_loss: 1733356102937.3230 - val_mae: 773878.6875\n",
      "Epoch 814/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 490479706700.3933 - mae: 440203.2812 - val_loss: 1705135372381.6079 - val_mae: 768402.8750\n",
      "Epoch 815/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 492936850052.8775 - mae: 440753.1562 - val_loss: 1702281800069.4014 - val_mae: 763575.2500\n",
      "Epoch 816/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 490455142426.7452 - mae: 440254.4688 - val_loss: 1691901736723.0859 - val_mae: 765673.5000\n",
      "Epoch 817/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 489949927109.6447 - mae: 440304.4688 - val_loss: 1717067361368.7180 - val_mae: 765758.8125\n",
      "Epoch 818/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 487556699894.3448 - mae: 438533.4688 - val_loss: 1689310753514.0701 - val_mae: 759624.7500\n",
      "Epoch 819/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 490526188392.6105 - mae: 440666.5938 - val_loss: 1682357274832.1731 - val_mae: 759911.4375\n",
      "Epoch 820/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 488661868142.4235 - mae: 439944.6250 - val_loss: 1717643615622.8984 - val_mae: 773142.0625\n",
      "Epoch 821/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 491007188689.4205 - mae: 441239.4062 - val_loss: 1713798266303.5820 - val_mae: 767344.7500\n",
      "Epoch 822/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 487676169998.4952 - mae: 438956.2500 - val_loss: 1671184516033.2288 - val_mae: 755542.8750\n",
      "Epoch 823/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 489859381035.0367 - mae: 440312.1562 - val_loss: 1696187403185.8604 - val_mae: 760941.3750\n",
      "Epoch 824/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 488789056313.7067 - mae: 439575.8750 - val_loss: 1689065491066.4988 - val_mae: 760567.0625\n",
      "Epoch 825/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 486304299686.0095 - mae: 439284.0312 - val_loss: 1674139093790.9614 - val_mae: 762375.4375\n",
      "Epoch 826/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 487979675937.4064 - mae: 439055.9062 - val_loss: 1685640112025.6099 - val_mae: 762061.5000\n",
      "Epoch 827/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 484599042759.7404 - mae: 438360.9688 - val_loss: 1727823627946.8994 - val_mae: 766727.0625\n",
      "Epoch 828/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 487515827622.4337 - mae: 439144.8125 - val_loss: 1669812615616.0811 - val_mae: 762214.6250\n",
      "Epoch 829/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 488267997075.4226 - mae: 439318.0000 - val_loss: 1644142036262.8953 - val_mae: 757191.5000\n",
      "Epoch 830/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 485730041072.0078 - mae: 438398.5000 - val_loss: 1706803537081.5195 - val_mae: 765735.1250\n",
      "Epoch 831/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 484179553379.8951 - mae: 437563.0625 - val_loss: 1684430590020.2600 - val_mae: 765597.0000\n",
      "Epoch 832/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 487942449198.1054 - mae: 438743.0312 - val_loss: 1725936673354.7966 - val_mae: 773295.2500\n",
      "Epoch 833/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 484596690918.0532 - mae: 438020.7500 - val_loss: 1720951217510.9639 - val_mae: 773063.6250\n",
      "Epoch 834/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 489470762633.5679 - mae: 440078.5000 - val_loss: 1692127902653.4365 - val_mae: 763678.0625\n",
      "Epoch 835/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 485976713597.4177 - mae: 438751.7188 - val_loss: 1695459515179.3362 - val_mae: 764707.7500\n",
      "Epoch 836/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 486232079740.5197 - mae: 439338.4062 - val_loss: 1687444455816.7944 - val_mae: 760394.1875\n",
      "Epoch 837/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 484933193836.9765 - mae: 438309.8750 - val_loss: 1693290842799.1907 - val_mae: 762020.9375\n",
      "Epoch 838/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 483288645691.0789 - mae: 437783.3125 - val_loss: 1672867068158.8772 - val_mae: 760896.3750\n",
      "Epoch 839/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 481749926632.0740 - mae: 436766.5938 - val_loss: 1707946190312.8975 - val_mae: 766669.0625\n",
      "Epoch 840/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 486305188167.9275 - mae: 437684.7188 - val_loss: 1741065653845.2751 - val_mae: 775170.5000\n",
      "Epoch 841/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 482750580909.0451 - mae: 437733.5000 - val_loss: 1716695955029.0754 - val_mae: 770283.5000\n",
      "Epoch 842/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 483853669363.9247 - mae: 438037.2812 - val_loss: 1715605575402.3694 - val_mae: 774957.0000\n",
      "Epoch 843/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 482411322920.1676 - mae: 436820.2188 - val_loss: 1722186178342.0469 - val_mae: 773039.1875\n",
      "Epoch 844/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 483766631157.8458 - mae: 437012.1250 - val_loss: 1720170554786.6414 - val_mae: 774055.3750\n",
      "Epoch 845/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 481709170020.5688 - mae: 436430.9688 - val_loss: 1682606229004.8237 - val_mae: 761982.4375\n",
      "Epoch 846/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 483131093405.9511 - mae: 437386.5625 - val_loss: 1719585298187.9006 - val_mae: 768965.2500\n",
      "Epoch 847/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 481704298815.9438 - mae: 436541.5938 - val_loss: 1689664834424.3782 - val_mae: 762210.2500\n",
      "Epoch 848/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 482164045419.4296 - mae: 436313.4688 - val_loss: 1706095362729.8018 - val_mae: 766908.0625\n",
      "Epoch 849/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 479326766403.6364 - mae: 436153.9375 - val_loss: 1736939628144.6191 - val_mae: 775874.1875\n",
      "Epoch 850/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 479429341315.3306 - mae: 436074.0625 - val_loss: 1672416613510.8235 - val_mae: 756892.0000\n",
      "Epoch 851/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 480469166764.7957 - mae: 436272.1562 - val_loss: 1694694430919.7903 - val_mae: 766801.9375\n",
      "Epoch 852/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 481539349077.3749 - mae: 436692.0000 - val_loss: 1688069287968.3337 - val_mae: 765069.2500\n",
      "Epoch 853/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 480142021482.6064 - mae: 435522.5000 - val_loss: 1718901472647.6968 - val_mae: 769558.0000\n",
      "Epoch 854/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 482594513483.5950 - mae: 437088.3750 - val_loss: 1680947025837.1699 - val_mae: 762475.7500\n",
      "Epoch 855/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 480948605321.9920 - mae: 436794.8750 - val_loss: 1678701516581.4482 - val_mae: 759395.6250\n",
      "Epoch 856/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 479549598399.9564 - mae: 435874.6562 - val_loss: 1695272231621.7444 - val_mae: 767730.6250\n",
      "Epoch 857/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 474949794088.3922 - mae: 434123.4062 - val_loss: 1734704977752.5435 - val_mae: 775835.3125\n",
      "Epoch 858/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 479326076482.5137 - mae: 435572.2500 - val_loss: 1673044600207.4807 - val_mae: 758035.5000\n",
      "Epoch 859/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 478558338366.2473 - mae: 435110.0312 - val_loss: 1717263666987.2363 - val_mae: 774985.9375\n",
      "Epoch 860/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 479669788006.9639 - mae: 435858.4062 - val_loss: 1732949024441.4697 - val_mae: 776486.6875\n",
      "Epoch 861/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 477282045549.2260 - mae: 435067.8750 - val_loss: 1713512924788.6108 - val_mae: 773048.3750\n",
      "Epoch 862/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 477591910579.8312 - mae: 435299.0938 - val_loss: 1669975256258.1021 - val_mae: 759187.7500\n",
      "Epoch 863/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 477573128740.4752 - mae: 435089.9688 - val_loss: 1686773286257.9414 - val_mae: 762691.6875\n",
      "Epoch 864/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 475597210943.6445 - mae: 434282.1250 - val_loss: 1683924489761.1819 - val_mae: 762246.9375\n",
      "Epoch 865/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 480180708591.8082 - mae: 435557.9062 - val_loss: 1713578892702.8491 - val_mae: 770018.7500\n",
      "Epoch 866/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 480739633109.4871 - mae: 435305.1250 - val_loss: 1701890974233.2981 - val_mae: 766140.3750\n",
      "Epoch 867/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 473426749460.5579 - mae: 434161.4375 - val_loss: 1675889664374.4321 - val_mae: 760577.0000\n",
      "Epoch 868/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 475996844200.0554 - mae: 434461.8125 - val_loss: 1715168400854.1357 - val_mae: 770520.5000\n",
      "Epoch 869/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 475423279412.7668 - mae: 434035.5000 - val_loss: 1698276364076.8330 - val_mae: 764050.6250\n",
      "Epoch 870/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 474322323272.1769 - mae: 433446.8438 - val_loss: 1694231025506.1238 - val_mae: 765986.3125\n",
      "Epoch 871/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 476091987355.8554 - mae: 433669.8438 - val_loss: 1719919235850.8027 - val_mae: 774586.9375\n",
      "Epoch 872/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 475943001168.3352 - mae: 434023.0625 - val_loss: 1714855819662.5825 - val_mae: 769983.0000\n",
      "Epoch 873/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 472522963867.1069 - mae: 432036.7500 - val_loss: 1707358936165.8911 - val_mae: 769530.3750\n",
      "Epoch 874/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 476438730754.5947 - mae: 434202.9062 - val_loss: 1738501165612.8579 - val_mae: 772778.0000\n",
      "Epoch 875/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 472628056887.5112 - mae: 432790.2812 - val_loss: 1711311401385.1282 - val_mae: 774130.6250\n",
      "Epoch 876/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 473896620825.9717 - mae: 432945.8750 - val_loss: 1699979864183.5549 - val_mae: 762480.0000\n",
      "Epoch 877/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 474121380907.4109 - mae: 433352.0938 - val_loss: 1712090240674.5166 - val_mae: 771802.8125\n",
      "Epoch 878/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 473011233392.3196 - mae: 432850.8125 - val_loss: 1781949569927.9463 - val_mae: 784537.0000\n",
      "Epoch 879/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 476174893562.1619 - mae: 434225.5938 - val_loss: 1736120596835.4709 - val_mae: 772504.8750\n",
      "Epoch 880/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 473322219590.0563 - mae: 432826.9062 - val_loss: 1696968577350.9294 - val_mae: 766126.6875\n",
      "Epoch 881/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 470764362575.2625 - mae: 432553.3125 - val_loss: 1728851807858.0161 - val_mae: 772430.6250\n",
      "Epoch 882/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 472382443960.2970 - mae: 432029.9062 - val_loss: 1693072291863.3521 - val_mae: 762976.6250\n",
      "Epoch 883/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 472772572396.2156 - mae: 432710.3438 - val_loss: 1739022141863.7310 - val_mae: 769857.7500\n",
      "Epoch 884/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 475130431075.0469 - mae: 433135.1250 - val_loss: 1707572280499.3323 - val_mae: 773926.1250\n",
      "Epoch 885/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 470981321156.6717 - mae: 432377.3438 - val_loss: 1714317756705.6062 - val_mae: 770985.1875\n",
      "Epoch 886/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 474002696023.3458 - mae: 433124.0625 - val_loss: 1686779215033.7192 - val_mae: 762575.7500\n",
      "Epoch 887/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 473850378806.2388 - mae: 433276.5000 - val_loss: 1687091759500.5867 - val_mae: 763368.5000\n",
      "Epoch 888/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 469723614191.3342 - mae: 431542.0938 - val_loss: 1695066080485.4294 - val_mae: 767402.3125\n",
      "Epoch 889/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 474208700633.7535 - mae: 432071.5625 - val_loss: 1704783416463.8052 - val_mae: 767612.1250\n",
      "Epoch 890/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 470527209335.0808 - mae: 431950.1562 - val_loss: 1705165268734.7275 - val_mae: 769083.3750\n",
      "Epoch 891/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 468026886394.4863 - mae: 430683.5000 - val_loss: 1716281962386.2251 - val_mae: 769628.1875\n",
      "Epoch 892/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 468583669620.2865 - mae: 430717.7500 - val_loss: 1723708219170.8535 - val_mae: 773284.6250\n",
      "Epoch 893/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 471451422368.3212 - mae: 431842.1875 - val_loss: 1687556094272.3430 - val_mae: 762375.3125\n",
      "Epoch 894/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 466556866924.4525 - mae: 430509.6562 - val_loss: 1694893530290.5339 - val_mae: 763950.8125\n",
      "Epoch 895/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 468370101019.5685 - mae: 431307.0312 - val_loss: 1732615098581.5620 - val_mae: 774167.9375\n",
      "Epoch 896/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 469145790251.4359 - mae: 430910.7500 - val_loss: 1693673789957.5386 - val_mae: 773801.9375\n",
      "Epoch 897/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 470420348429.0233 - mae: 430950.8125 - val_loss: 1707249552954.1309 - val_mae: 767208.8125\n",
      "Epoch 898/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 467004827575.9478 - mae: 429908.8125 - val_loss: 1743291783228.8752 - val_mae: 775789.9375\n",
      "Epoch 899/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 468236215662.7479 - mae: 430735.0000 - val_loss: 1687885326161.7573 - val_mae: 760477.0625\n",
      "Epoch 900/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 468720297360.2791 - mae: 430252.4062 - val_loss: 1700775328156.9531 - val_mae: 764064.8125\n",
      "Epoch 901/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 468642787871.2858 - mae: 430448.5312 - val_loss: 1730383963872.3899 - val_mae: 775435.3125\n",
      "Epoch 902/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 466835006338.9564 - mae: 429839.2500 - val_loss: 1703410479730.3157 - val_mae: 767689.3125\n",
      "Epoch 903/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 471191902857.2685 - mae: 430709.2188 - val_loss: 1705395406321.6794 - val_mae: 768693.6875\n",
      "Epoch 904/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 462970113882.3397 - mae: 428570.5312 - val_loss: 1710901519978.2322 - val_mae: 769514.5625\n",
      "Epoch 905/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 466377206741.7866 - mae: 429897.1875 - val_loss: 1688360543735.2678 - val_mae: 761937.8125\n",
      "Epoch 906/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 469166298228.4611 - mae: 430938.3750 - val_loss: 1726373429088.3274 - val_mae: 770160.3125\n",
      "Epoch 907/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 464137263988.7855 - mae: 429132.6562 - val_loss: 1736611064708.3535 - val_mae: 781279.8125\n",
      "Epoch 908/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 463473236038.4554 - mae: 428736.7500 - val_loss: 1729302048180.8042 - val_mae: 779918.3750\n",
      "Epoch 909/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 467580920714.1417 - mae: 430758.5000 - val_loss: 1732600327708.5913 - val_mae: 773864.1250\n",
      "Epoch 910/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 469691254453.2782 - mae: 429840.6875 - val_loss: 1692089209390.4548 - val_mae: 766279.5625\n",
      "Epoch 911/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 465892098304.2744 - mae: 429304.8750 - val_loss: 1731899932450.0552 - val_mae: 772906.3750\n",
      "Epoch 912/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 465791739761.6919 - mae: 429040.9375 - val_loss: 1680807527987.1450 - val_mae: 761680.1250\n",
      "Epoch 913/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 470120403745.1570 - mae: 431807.7188 - val_loss: 1679623799507.3167 - val_mae: 760140.1875\n",
      "Epoch 914/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 468150323378.6337 - mae: 429874.7188 - val_loss: 1712062082701.9590 - val_mae: 768462.5625\n",
      "Epoch 915/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 462498649139.7938 - mae: 428664.4375 - val_loss: 1720431326765.1575 - val_mae: 769589.8125\n",
      "Epoch 916/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 467570111942.1686 - mae: 429708.8125 - val_loss: 1673654796230.5178 - val_mae: 758024.1875\n",
      "Epoch 917/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 463841281380.3691 - mae: 428032.4375 - val_loss: 1711385096639.4824 - val_mae: 769744.2500\n",
      "Epoch 918/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 465536999305.8423 - mae: 429647.0938 - val_loss: 1703211126378.0325 - val_mae: 768882.2500\n",
      "Epoch 919/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 461591983484.7192 - mae: 427208.0938 - val_loss: 1757747367712.0593 - val_mae: 781940.3125\n",
      "Epoch 920/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 465143484607.2078 - mae: 429569.9375 - val_loss: 1680423997583.3062 - val_mae: 763119.1250\n",
      "Epoch 921/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 466354610600.7290 - mae: 429009.1562 - val_loss: 1685056253008.7344 - val_mae: 762881.1250\n",
      "Epoch 922/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 464120526053.8287 - mae: 428183.4375 - val_loss: 1702108094298.7390 - val_mae: 769186.6875\n",
      "Epoch 923/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 462180129356.5930 - mae: 427590.7500 - val_loss: 1745922444663.3303 - val_mae: 781184.9375\n",
      "Epoch 924/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 462347386373.8381 - mae: 427657.4688 - val_loss: 1754368490563.5615 - val_mae: 778838.3125\n",
      "Epoch 925/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 464104330797.9557 - mae: 427546.1875 - val_loss: 1704539373678.1741 - val_mae: 767861.7500\n",
      "Epoch 926/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 463092975504.8280 - mae: 427780.3125 - val_loss: 1751762774502.8018 - val_mae: 774806.9375\n",
      "Epoch 927/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 463534759021.0763 - mae: 427654.6875 - val_loss: 1681378388949.7866 - val_mae: 763128.9375\n",
      "Epoch 928/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 461512237098.3132 - mae: 427799.9688 - val_loss: 1746140912946.2720 - val_mae: 781341.8750\n",
      "Epoch 929/1000\n",
      "164176/164176 [==============================] - 16s 101us/step - loss: 462608992767.5510 - mae: 427657.3125 - val_loss: 1692564727646.5312 - val_mae: 766424.8750\n",
      "Epoch 930/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 464467011425.7246 - mae: 427992.7812 - val_loss: 1688752226866.0474 - val_mae: 762016.5000\n",
      "Epoch 931/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 460163988912.4131 - mae: 427273.9688 - val_loss: 1732983791152.1514 - val_mae: 771062.2500\n",
      "Epoch 932/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 459861350082.4512 - mae: 426986.6875 - val_loss: 1709542168830.0789 - val_mae: 774665.0000\n",
      "Epoch 933/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 463029582405.4076 - mae: 427308.7812 - val_loss: 1711065167029.9270 - val_mae: 767906.0000\n",
      "Epoch 934/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 461248399283.1576 - mae: 427252.4375 - val_loss: 1733833050607.5837 - val_mae: 772964.1250\n",
      "Epoch 935/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 463625354364.2452 - mae: 427437.2188 - val_loss: 1752805323668.9197 - val_mae: 779978.0625\n",
      "Epoch 936/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 460982882773.8365 - mae: 426677.8125 - val_loss: 1706673515905.7090 - val_mae: 764540.1875\n",
      "Epoch 937/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 460052641917.5426 - mae: 426302.6562 - val_loss: 1714763563867.0383 - val_mae: 770823.0625\n",
      "Epoch 938/1000\n",
      "164176/164176 [==============================] - 16s 97us/step - loss: 461361848690.4402 - mae: 426670.7500 - val_loss: 1739081884024.0288 - val_mae: 776976.8750\n",
      "Epoch 939/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 458859313491.5038 - mae: 426814.9375 - val_loss: 1703237041123.7578 - val_mae: 766136.0000\n",
      "Epoch 940/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 458964441125.7227 - mae: 425969.0312 - val_loss: 1730891420565.7180 - val_mae: 771904.1875\n",
      "Epoch 941/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 460051378321.7012 - mae: 426719.0312 - val_loss: 1716254673108.9634 - val_mae: 768887.8750\n",
      "Epoch 942/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 460495487759.1938 - mae: 426406.9375 - val_loss: 1733875765913.4353 - val_mae: 777438.5000\n",
      "Epoch 943/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 458698776983.1650 - mae: 425547.7812 - val_loss: 1737366054795.2395 - val_mae: 781528.6250\n",
      "Epoch 944/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 457219085501.9105 - mae: 425853.5312 - val_loss: 1704532169910.7253 - val_mae: 770825.6875\n",
      "Epoch 945/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 459555308385.4252 - mae: 426115.1875 - val_loss: 1733031100971.6604 - val_mae: 776631.7500\n",
      "Epoch 946/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 457010014055.3131 - mae: 425477.0000 - val_loss: 1782318465033.5803 - val_mae: 786250.8750\n",
      "Epoch 947/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 457520656188.0020 - mae: 425823.1250 - val_loss: 1744179939041.1882 - val_mae: 773621.0000\n",
      "Epoch 948/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 457138361981.7921 - mae: 425351.5625 - val_loss: 1715425254947.3774 - val_mae: 768466.1250\n",
      "Epoch 949/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 454405716610.8816 - mae: 424467.4062 - val_loss: 1734374153499.9177 - val_mae: 773665.3125\n",
      "Epoch 950/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 455363363586.4200 - mae: 424964.4062 - val_loss: 1740491456895.2141 - val_mae: 773750.4375\n",
      "Epoch 951/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 457421709361.4984 - mae: 424854.8750 - val_loss: 1703795285342.9802 - val_mae: 768133.1250\n",
      "Epoch 952/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 455772859065.5695 - mae: 425027.4062 - val_loss: 1734909553093.2705 - val_mae: 771272.1875\n",
      "Epoch 953/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 459254726782.1413 - mae: 426475.9062 - val_loss: 1718053666729.7769 - val_mae: 768521.9375\n",
      "Epoch 954/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 456993992628.5547 - mae: 424670.8438 - val_loss: 1721887990947.1653 - val_mae: 773538.7500\n",
      "Epoch 955/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 455521017342.6528 - mae: 425020.5625 - val_loss: 1690920024502.5007 - val_mae: 764151.0625\n",
      "Epoch 956/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 454096983602.7460 - mae: 424466.1562 - val_loss: 1713704961433.8594 - val_mae: 768714.8750\n",
      "Epoch 957/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 453274928919.4768 - mae: 424352.7812 - val_loss: 1733027914514.3872 - val_mae: 772917.9375\n",
      "Epoch 958/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 456007636787.8187 - mae: 424756.0312 - val_loss: 1721809983544.0850 - val_mae: 771295.5625\n",
      "Epoch 959/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 455521746281.3589 - mae: 425352.2500 - val_loss: 1713844326107.4001 - val_mae: 768373.0625\n",
      "Epoch 960/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 454846024379.1661 - mae: 423979.4688 - val_loss: 1703770882996.4548 - val_mae: 767517.8750\n",
      "Epoch 961/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 454761569657.3262 - mae: 424838.1562 - val_loss: 1764910459783.5471 - val_mae: 784655.3750\n",
      "Epoch 962/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 458874513018.1994 - mae: 426500.5938 - val_loss: 1710005849832.1738 - val_mae: 766479.3750\n",
      "Epoch 963/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 458376013585.0900 - mae: 425454.7812 - val_loss: 1739719630507.0991 - val_mae: 774898.4375\n",
      "Epoch 964/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 455349659240.6354 - mae: 424356.0312 - val_loss: 1726202256056.3718 - val_mae: 772859.7500\n",
      "Epoch 965/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 454876577423.9548 - mae: 423851.8750 - val_loss: 1692003939575.3926 - val_mae: 764662.1875\n",
      "Epoch 966/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 456705309674.1448 - mae: 425243.5625 - val_loss: 1722609292093.8979 - val_mae: 768468.0625\n",
      "Epoch 967/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 450311938829.5972 - mae: 422296.3750 - val_loss: 1721247313233.8071 - val_mae: 769522.9375\n",
      "Epoch 968/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 457846496791.4020 - mae: 425442.6562 - val_loss: 1732030440435.5256 - val_mae: 772875.0625\n",
      "Epoch 969/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 455507795099.3813 - mae: 424189.8438 - val_loss: 1759787468254.5186 - val_mae: 776794.3750\n",
      "Epoch 970/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 454032994008.2066 - mae: 423344.7188 - val_loss: 1716808183216.0139 - val_mae: 771961.1250\n",
      "Epoch 971/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 451861160743.1447 - mae: 423150.0625 - val_loss: 1701624056400.9839 - val_mae: 764476.6250\n",
      "Epoch 972/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 453856399504.9028 - mae: 423626.4375 - val_loss: 1767664472545.8120 - val_mae: 782976.5625\n",
      "Epoch 973/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 449790952103.4066 - mae: 423056.0625 - val_loss: 1707670906213.9658 - val_mae: 769758.1875\n",
      "Epoch 974/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 452474091993.6287 - mae: 423413.7188 - val_loss: 1756264656035.7642 - val_mae: 778036.4375\n",
      "Epoch 975/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 454068008671.9907 - mae: 423446.6562 - val_loss: 1724060115000.6838 - val_mae: 769021.9375\n",
      "Epoch 976/1000\n",
      "164176/164176 [==============================] - 16s 97us/step - loss: 450880318991.8176 - mae: 422627.2812 - val_loss: 1738842300790.4321 - val_mae: 774880.2500\n",
      "Epoch 977/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 450841937057.5687 - mae: 422913.5000 - val_loss: 1770902186929.0618 - val_mae: 779458.0625\n",
      "Epoch 978/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 449720979628.0472 - mae: 422461.4375 - val_loss: 1744444593665.2476 - val_mae: 773896.9375\n",
      "Epoch 979/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 450270205995.3112 - mae: 422155.8125 - val_loss: 1714386010638.9194 - val_mae: 770860.4375\n",
      "Epoch 980/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 449024099913.4992 - mae: 422522.0938 - val_loss: 1728126433681.7761 - val_mae: 772291.8750\n",
      "Epoch 981/1000\n",
      "164176/164176 [==============================] - 16s 98us/step - loss: 453131181362.2719 - mae: 422561.4062 - val_loss: 1714012584863.0986 - val_mae: 772784.5000\n",
      "Epoch 982/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 447968646524.7192 - mae: 421505.9375 - val_loss: 1747116480149.7429 - val_mae: 778307.8125\n",
      "Epoch 983/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 450784167851.1740 - mae: 422120.4688 - val_loss: 1712117796994.5322 - val_mae: 769460.1875\n",
      "Epoch 984/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 450371278191.9454 - mae: 422441.8750 - val_loss: 1732342592159.0239 - val_mae: 773543.5625\n",
      "Epoch 985/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 449839412168.0148 - mae: 421701.4375 - val_loss: 1722757458598.4087 - val_mae: 770534.0000\n",
      "Epoch 986/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 451180844237.8778 - mae: 422618.4062 - val_loss: 1738475236911.7520 - val_mae: 776942.9375\n",
      "Epoch 987/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 448554946623.4698 - mae: 421187.1875 - val_loss: 1716551354561.8025 - val_mae: 770818.5000\n",
      "Epoch 988/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 451002778935.6609 - mae: 422337.1250 - val_loss: 1726147349687.7231 - val_mae: 773667.7500\n",
      "Epoch 989/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 451420275447.2429 - mae: 422532.6875 - val_loss: 1717747383253.3875 - val_mae: 769400.3750\n",
      "Epoch 990/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 450214868696.0070 - mae: 422166.0938 - val_loss: 1740920748556.7239 - val_mae: 777976.6250\n",
      "Epoch 991/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 450235250887.7903 - mae: 421543.4688 - val_loss: 1781628116754.8862 - val_mae: 784601.8750\n",
      "Epoch 992/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 449587385458.0660 - mae: 422299.6562 - val_loss: 1760868964128.6580 - val_mae: 780112.0000\n",
      "Epoch 993/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 445815190616.5185 - mae: 420742.1875 - val_loss: 1727738852545.1040 - val_mae: 769954.2500\n",
      "Epoch 994/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 448770427512.9020 - mae: 421406.5000 - val_loss: 1735741881069.8623 - val_mae: 776095.9375\n",
      "Epoch 995/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 449121772807.7591 - mae: 420963.0938 - val_loss: 1722412565904.2791 - val_mae: 772154.0000\n",
      "Epoch 996/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 446356788068.2194 - mae: 420631.0625 - val_loss: 1777858950369.0386 - val_mae: 784206.4375\n",
      "Epoch 997/1000\n",
      "164176/164176 [==============================] - 16s 100us/step - loss: 450066693244.2452 - mae: 422312.5312 - val_loss: 1713756640213.5869 - val_mae: 768206.5625\n",
      "Epoch 998/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 448067501042.9268 - mae: 421404.2500 - val_loss: 1702063744725.2126 - val_mae: 766079.3750\n",
      "Epoch 999/1000\n",
      "164176/164176 [==============================] - 17s 101us/step - loss: 451493459212.1501 - mae: 422149.2188 - val_loss: 1737667081212.7068 - val_mae: 775922.6250\n",
      "Epoch 1000/1000\n",
      "164176/164176 [==============================] - 16s 99us/step - loss: 450885100298.4036 - mae: 421679.4375 - val_loss: 1711815438409.8486 - val_mae: 768312.7500\n"
     ]
    }
   ],
   "source": [
    "deep_model = Sequential()\n",
    "\n",
    "# La idea es que los modelos mas profundos pueden abstraerse mejor de los datos con lo que fueron entrenados\n",
    "deep_model.add(Dense(50, activation = 'relu', input_shape = (43,)))\n",
    "deep_model.add(Dense(100, activation = 'relu'))\n",
    "deep_model.add(Dense(150, activation = 'relu'))\n",
    "deep_model.add(Dense(200, activation = 'relu'))\n",
    "deep_model.add(Dense(150, activation = 'relu'))\n",
    "deep_model.add(Dense(100, activation = 'relu'))\n",
    "deep_model.add(Dense(50, activation = 'relu'))\n",
    "deep_model.add(Dense(1))\n",
    "\n",
    "deep_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "training_history = deep_model.fit(train_x, train_y, validation_split=0.2, epochs=1000, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/testModified.csv')\n",
    "test_df.antiguedad = scaler.fit_transform(np.array(test_df.antiguedad).reshape(-1, 1))\n",
    "test_df.metroscubiertos = scaler.fit_transform(np.array(test_df.metroscubiertos).reshape(-1, 1))\n",
    "test_df.metrostotales = scaler.fit_transform(np.array(test_df.metrostotales).reshape(-1, 1))\n",
    "\n",
    "#deep_model_prediction = deep_model.predict(test_df.drop(columns=['id']))\n",
    "res = pd.DataFrame(deep_model, index=test_df.id, columns=['precio'])\n",
    "res = res.rename({'precio':'target'}, axis=1)\n",
    "res.to_csv(\"predictions/deep_model_prediction.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6756809.  ],\n",
       "       [ 497338.22],\n",
       "       [2102467.  ],\n",
       "       ...,\n",
       "       [1133866.1 ],\n",
       "       [1253792.1 ],\n",
       "       [2763418.2 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8a8a712a640d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/testModified.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantiguedad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantiguedad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetroscubiertos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetroscubiertos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrostotales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrostotales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/testModified.csv')\n",
    "test_df.antiguedad = scaler.fit_transform(np.array(test_df.antiguedad).reshape(-1, 1))\n",
    "test_df.metroscubiertos = scaler.fit_transform(np.array(test_df.metroscubiertos).reshape(-1, 1))\n",
    "test_df.metrostotales = scaler.fit_transform(np.array(test_df.metrostotales).reshape(-1, 1))\n",
    "\n",
    "wide_model_prediction = wide_model.predict(test_df.drop(columns=['id']))\n",
    "res = pd.DataFrame(wide_model, index=test_df.id, columns=['precio'])\n",
    "res = res.rename({'precio':'target'}, axis=1)\n",
    "res.to_csv(\"predictions/wide_model_prediction.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wide_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-122d2297510d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwide_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wide_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(wide_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
