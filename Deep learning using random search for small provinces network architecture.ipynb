{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 15)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "train_df = pd.read_csv('data/features_small_provinces.csv')\n",
    "\n",
    "train_df = train_df[train_df.metroscubiertos <= train_df.metrostotales]\n",
    "\n",
    "train_x = train_df.drop(columns=['precio', 'usosmultiples'])\n",
    "train_y = train_df.precio\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "coordinates_scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "\n",
    "train_x.antiguedad = scaler.fit_transform(np.array(train_x.antiguedad).reshape(-1, 1))\n",
    "train_x.metroscubiertos = scaler.fit_transform(np.array(train_x.metroscubiertos).reshape(-1, 1))\n",
    "train_x.metrostotales = scaler.fit_transform(np.array(train_x.metrostotales).reshape(-1, 1))\n",
    "train_x.habitaciones = scaler.fit_transform(np.array(train_x.habitaciones).reshape(-1, 1))\n",
    "train_x.garages = scaler.fit_transform(np.array(train_x.garages).reshape(-1, 1))\n",
    "train_x.banos = scaler.fit_transform(np.array(train_x.banos).reshape(-1, 1))\n",
    "\n",
    "n_cols = train_x.shape[0]\n",
    "\n",
    "train_x = train_x.to_numpy()\n",
    "train_y = train_y.to_numpy()\n",
    "#train_x.head()\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units_1', min_value=64, max_value=100, step=16), activation = 'relu', input_shape=(15,)))\n",
    "    for i in range(hp.Int('num_layers', 2, 5)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=64,\n",
    "                                            max_value=100,\n",
    "                                            step=16),\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Search space summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Default search space size: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">units_1 (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 16</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">num_layers (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">units_0 (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 16</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">learning_rate (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: [0.01, 0.001, 0.0001]</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 348 samples\n",
      "Epoch 1/20\n",
      "348/348 [==============================] - ETA: 5s - loss: 3511062364160.0000 - mae: 1294906.250 - 1s 2ms/sample - loss: 2750228642898.3911 - mae: 1239739.8750 - val_loss: 3060524803083.7700 - val_mae: 1316727.0000\n",
      "Epoch 2/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1747195396096.0000 - mae: 1094405.250 - 0s 203us/sample - loss: 2750223809806.7129 - mae: 1239738.0000 - val_loss: 3060513877403.9541 - val_mae: 1316723.2500\n",
      "Epoch 3/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1349067472896.0000 - mae: 936870.62 - 0s 189us/sample - loss: 2750202720773.8848 - mae: 1239729.8750 - val_loss: 3060466456458.2993 - val_mae: 1316706.6250\n",
      "Epoch 4/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 4166227066880.0000 - mae: 1546447.500 - 0s 197us/sample - loss: 2750120154453.3335 - mae: 1239696.8750 - val_loss: 3060275446889.9312 - val_mae: 1316640.5000\n",
      "Epoch 5/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2410450255872.0000 - mae: 1292610.625 - 0s 184us/sample - loss: 2749787657933.9771 - mae: 1239572.7500 - val_loss: 3059590747853.9771 - val_mae: 1316404.7500\n",
      "Epoch 6/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2977495515136.0000 - mae: 1230576.000 - 0s 200us/sample - loss: 2748694975452.6895 - mae: 1239159.0000 - val_loss: 3057451273157.1494 - val_mae: 1315672.8750\n",
      "Epoch 7/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1776063479808.0000 - mae: 1134161.250 - 0s 199us/sample - loss: 2745580171405.2412 - mae: 1237962.2500 - val_loss: 3051528647738.8506 - val_mae: 1313653.2500\n",
      "Epoch 8/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2837402091520.0000 - mae: 1362822.250 - 0s 205us/sample - loss: 2737032131642.8506 - mae: 1234842.6250 - val_loss: 3036807219223.5400 - val_mae: 1308637.3750\n",
      "Epoch 9/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3873553514496.0000 - mae: 1499042.500 - 0s 197us/sample - loss: 2717271953455.0806 - mae: 1227374.3750 - val_loss: 3003152815445.3335 - val_mae: 1297116.2500\n",
      "Epoch 10/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1479473364992.0000 - mae: 993938.00 - 0s 194us/sample - loss: 2671887571391.2642 - mae: 1210960.8750 - val_loss: 2932467092986.1147 - val_mae: 1272626.7500\n",
      "Epoch 11/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 5862938640384.0000 - mae: 1529235.375 - 0s 192us/sample - loss: 2589397822499.3105 - mae: 1177493.8750 - val_loss: 2796631734625.1040 - val_mae: 1224316.7500\n",
      "Epoch 12/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1947276935168.0000 - mae: 1005229.750 - 0s 188us/sample - loss: 2427285897592.6436 - mae: 1113629.7500 - val_loss: 2558957997185.4712 - val_mae: 1135244.2500\n",
      "Epoch 13/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 629844475904.0000 - mae: 643349.500 - 0s 194us/sample - loss: 2157016199685.8850 - mae: 1000509.5000 - val_loss: 2191114172204.1379 - val_mae: 982993.9375\n",
      "Epoch 14/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 942700953600.0000 - mae: 655066.375 - 0s 179us/sample - loss: 1768076640632.6436 - mae: 823850.5625 - val_loss: 1707282473807.4480 - val_mae: 780519.9375\n",
      "Epoch 15/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3581271343104.0000 - mae: 1015622.500 - 0s 205us/sample - loss: 1374300179020.5059 - mae: 664419.5000 - val_loss: 1227171926063.0806 - val_mae: 648403.2500\n",
      "Epoch 16/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 403911540736.0000 - mae: 421401.218 - 0s 194us/sample - loss: 1068380632687.8162 - mae: 630373.3125 - val_loss: 1001793776110.3448 - val_mae: 663783.5625\n",
      "Epoch 17/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 510569873408.0000 - mae: 518944.468 - 0s 154us/sample - loss: 995529979692.1379 - mae: 686535.0625 - val_loss: 962678305662.5287 - val_mae: 692514.4375\n",
      "Epoch 18/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 749066387456.0000 - mae: 601643.750 - 0s 179us/sample - loss: 976792885506.9425 - mae: 685608.8125 - val_loss: 943462164409.3793 - val_mae: 659491.1875\n",
      "Epoch 19/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 517838077952.0000 - mae: 583171.625 - 0s 175us/sample - loss: 950362303334.9885 - mae: 650221.6875 - val_loss: 928659633587.4943 - val_mae: 643746.2500\n",
      "Epoch 20/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1149357785088.0000 - mae: 748286.62 - 0s 166us/sample - loss: 934662710848.7357 - mae: 645206.0625 - val_loss: 907083500673.4713 - val_mae: 641490.5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-num_layers: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 630373.3125</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 348 samples\n",
      "Epoch 1/20\n",
      "348/348 [==============================] - ETA: 3s - loss: 2785682653184.0000 - mae: 1265729.750 - 0s 1ms/sample - loss: 2750229105416.8276 - mae: 1239740.0000 - val_loss: 3060526056553.9312 - val_mae: 1316727.5000\n",
      "Epoch 2/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1457184178176.0000 - mae: 990780.75 - 0s 225us/sample - loss: 2750225812044.5059 - mae: 1239738.7500 - val_loss: 3060518719535.0806 - val_mae: 1316725.0000\n",
      "Epoch 3/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2200380112896.0000 - mae: 1078184.500 - 0s 213us/sample - loss: 2750208492461.6089 - mae: 1239732.2500 - val_loss: 3060471548680.8276 - val_mae: 1316708.6250\n",
      "Epoch 4/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2532711071744.0000 - mae: 1274575.500 - 0s 190us/sample - loss: 2750092076420.4141 - mae: 1239691.2500 - val_loss: 3060191958545.6553 - val_mae: 1316613.2500\n",
      "Epoch 5/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2431842516992.0000 - mae: 1211609.750 - 0s 205us/sample - loss: 2749523870260.9653 - mae: 1239476.7500 - val_loss: 3058808335395.3101 - val_mae: 1316143.2500\n",
      "Epoch 6/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1651201540096.0000 - mae: 1087610.750 - 0s 197us/sample - loss: 2746777850091.4023 - mae: 1238503.8750 - val_loss: 3052953319023.8159 - val_mae: 1314159.2500\n",
      "Epoch 7/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2682779598848.0000 - mae: 1211546.750 - 0s 188us/sample - loss: 2736886179204.4141 - mae: 1234713.8750 - val_loss: 3031796930253.9771 - val_mae: 1306988.3750\n",
      "Epoch 8/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1805648396288.0000 - mae: 1081036.000 - 0s 202us/sample - loss: 2702209178800.5518 - mae: 1222113.6250 - val_loss: 2965008787385.3794 - val_mae: 1284145.0000\n",
      "Epoch 9/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2501559189504.0000 - mae: 1169649.750 - 0s 208us/sample - loss: 2603336725562.8506 - mae: 1183882.7500 - val_loss: 2780775248201.5630 - val_mae: 1219117.0000\n",
      "Epoch 10/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1897611919360.0000 - mae: 1110316.000 - 0s 195us/sample - loss: 2356632339750.2529 - mae: 1082417.1250 - val_loss: 2354644795580.3218 - val_mae: 1054611.6250\n",
      "Epoch 11/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 4543223955456.0000 - mae: 1262219.250 - 0s 221us/sample - loss: 1856127955273.5632 - mae: 852469.8750 - val_loss: 1612198871416.6438 - val_mae: 745626.7500\n",
      "Epoch 12/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1231761965056.0000 - mae: 732877.12 - 0s 229us/sample - loss: 1212996830878.8967 - mae: 640210.9375 - val_loss: 990506536442.1150 - val_mae: 661550.0000\n",
      "Epoch 13/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2077868163072.0000 - mae: 945638.50 - 0s 177us/sample - loss: 1049005150820.0460 - mae: 756613.8125 - val_loss: 959646389965.9772 - val_mae: 726445.0000\n",
      "Epoch 14/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 798275600384.0000 - mae: 692219.062 - 0s 185us/sample - loss: 962862780086.4369 - mae: 673594.1875 - val_loss: 942070615169.4713 - val_mae: 628160.8125\n",
      "Epoch 15/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1381652496384.0000 - mae: 767497.68 - 0s 180us/sample - loss: 930960023269.5173 - mae: 617565.5000 - val_loss: 913039760372.2299 - val_mae: 619718.3750\n",
      "Epoch 16/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 956214214656.0000 - mae: 680667.687 - 0s 206us/sample - loss: 907018404216.6437 - mae: 623575.1875 - val_loss: 872176494521.3793 - val_mae: 631159.1875\n",
      "Epoch 17/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 651497570304.0000 - mae: 617815.062 - 0s 182us/sample - loss: 886222999787.4022 - mae: 628511.2500 - val_loss: 851867071546.8506 - val_mae: 621632.6250\n",
      "Epoch 18/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 963936190464.0000 - mae: 643388.750 - 0s 174us/sample - loss: 872036396255.6322 - mae: 618973.0000 - val_loss: 834830685148.6897 - val_mae: 606820.8750\n",
      "Epoch 19/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 666724990976.0000 - mae: 630921.625 - 0s 161us/sample - loss: 862590877319.3563 - mae: 620043.6250 - val_loss: 809249114323.8621 - val_mae: 615437.0000\n",
      "Epoch 20/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 338279137280.0000 - mae: 505287.937 - 0s 209us/sample - loss: 847233498571.0344 - mae: 593707.9375 - val_loss: 813616747484.6897 - val_mae: 576164.4375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-num_layers: 5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_4: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 593707.9375</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 348 samples\n",
      "Epoch 1/20\n",
      "348/348 [==============================] - ETA: 3s - loss: 3103273779200.0000 - mae: 1310906.250 - 0s 1ms/sample - loss: 2750229209370.4829 - mae: 1239740.0000 - val_loss: 3060526903248.9194 - val_mae: 1316727.7500\n",
      "Epoch 2/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1816526848000.0000 - mae: 1149093.500 - 0s 190us/sample - loss: 2750228883950.3447 - mae: 1239739.8750 - val_loss: 3060526806828.1377 - val_mae: 1316727.6250\n",
      "Epoch 3/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3028806795264.0000 - mae: 1243843.500 - 0s 156us/sample - loss: 2750228698641.6553 - mae: 1239739.8750 - val_loss: 3060526568789.3335 - val_mae: 1316727.6250\n",
      "Epoch 4/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3079782531072.0000 - mae: 1302624.750 - 0s 191us/sample - loss: 2750228489227.7700 - mae: 1239739.7500 - val_loss: 3060526306645.3335 - val_mae: 1316727.5000\n",
      "Epoch 5/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 4626605670400.0000 - mae: 1330067.250 - 0s 200us/sample - loss: 2750228276800.7358 - mae: 1239739.6250 - val_loss: 3060525866725.5171 - val_mae: 1316727.5000\n",
      "Epoch 6/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2050624716800.0000 - mae: 1169685.000 - 0s 180us/sample - loss: 2750227800723.1265 - mae: 1239739.5000 - val_loss: 3060525375582.1611 - val_mae: 1316727.2500\n",
      "Epoch 7/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2789897142272.0000 - mae: 1314624.250 - 0s 181us/sample - loss: 2750227407507.1265 - mae: 1239739.3750 - val_loss: 3060524733781.3335 - val_mae: 1316727.0000\n",
      "Epoch 8/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2219004395520.0000 - mae: 1194217.750 - 0s 197us/sample - loss: 2750226652713.1953 - mae: 1239739.1250 - val_loss: 3060524052809.5630 - val_mae: 1316726.6250\n",
      "Epoch 9/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 5087986450432.0000 - mae: 1460717.500 - 0s 174us/sample - loss: 2750225870800.9194 - mae: 1239738.8750 - val_loss: 3060523073536.0000 - val_mae: 1316726.3750\n",
      "Epoch 10/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1738282631168.0000 - mae: 1164220.625 - 0s 189us/sample - loss: 2750225025612.5059 - mae: 1239738.5000 - val_loss: 3060521868276.2300 - val_mae: 1316726.1250\n",
      "Epoch 11/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1545826467840.0000 - mae: 1032966.875 - 0s 191us/sample - loss: 2750223802273.8389 - mae: 1239738.0000 - val_loss: 3060520424977.6553 - val_mae: 1316725.5000\n",
      "Epoch 12/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2362022035456.0000 - mae: 1229810.000 - 0s 185us/sample - loss: 2750222208317.7935 - mae: 1239737.5000 - val_loss: 3060518647219.4946 - val_mae: 1316724.8750\n",
      "Epoch 13/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3382070738944.0000 - mae: 1433309.250 - 0s 188us/sample - loss: 2750220335645.4253 - mae: 1239736.7500 - val_loss: 3060516227660.5054 - val_mae: 1316724.0000\n",
      "Epoch 14/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1979793539072.0000 - mae: 1157246.250 - 0s 177us/sample - loss: 2750218092355.6782 - mae: 1239735.8750 - val_loss: 3060513268747.7700 - val_mae: 1316723.0000\n",
      "Epoch 15/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3497532588032.0000 - mae: 1397276.625 - 0s 201us/sample - loss: 2750215264514.9424 - mae: 1239734.7500 - val_loss: 3060509556547.6782 - val_mae: 1316721.8750\n",
      "Epoch 16/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2787261284352.0000 - mae: 1389493.750 - 0s 165us/sample - loss: 2750211552314.8506 - mae: 1239733.5000 - val_loss: 3060505142283.7700 - val_mae: 1316720.3750\n",
      "Epoch 17/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2433580269568.0000 - mae: 1151558.500 - 0s 183us/sample - loss: 2750207049163.0347 - mae: 1239731.6250 - val_loss: 3060499387168.3677 - val_mae: 1316718.2500\n",
      "Epoch 18/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 5352594079744.0000 - mae: 1805956.875 - 0s 188us/sample - loss: 2750201666171.5859 - mae: 1239729.6250 - val_loss: 3060492456924.6899 - val_mae: 1316715.8750\n",
      "Epoch 19/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2552768757760.0000 - mae: 1280540.000 - 0s 180us/sample - loss: 2750194634987.4023 - mae: 1239727.1250 - val_loss: 3060483986961.6553 - val_mae: 1316713.1250\n",
      "Epoch 20/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1949994582016.0000 - mae: 1202457.000 - 0s 177us/sample - loss: 2750185902880.3677 - mae: 1239724.0000 - val_loss: 3060473799503.4482 - val_mae: 1316709.3750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-num_layers: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 96</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 96</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_4: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 1239724.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 348 samples\n",
      "Epoch 1/20\n",
      "348/348 [==============================] - ETA: 2s - loss: 3079885553664.0000 - mae: 1295951.125 - 0s 1ms/sample - loss: 2750228457589.7012 - mae: 1239739.7500 - val_loss: 3060524935662.3447 - val_mae: 1316727.1250\n",
      "Epoch 2/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 4290623045632.0000 - mae: 1417686.500 - 0s 153us/sample - loss: 2750225513742.7129 - mae: 1239738.6250 - val_loss: 3060519924794.8506 - val_mae: 1316725.3750\n",
      "Epoch 3/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3213655277568.0000 - mae: 1461784.000 - 0s 151us/sample - loss: 2750218431334.9883 - mae: 1239735.8750 - val_loss: 3060507230396.3218 - val_mae: 1316721.0000\n",
      "Epoch 4/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2346401136640.0000 - mae: 1193961.750 - 0s 175us/sample - loss: 2750200130971.9541 - mae: 1239729.2500 - val_loss: 3060475532064.3677 - val_mae: 1316710.1250\n",
      "Epoch 5/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3749425709056.0000 - mae: 1492982.875 - 0s 177us/sample - loss: 2750158630864.9194 - mae: 1239713.3750 - val_loss: 3060404388593.2876 - val_mae: 1316685.8750\n",
      "Epoch 6/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3362836447232.0000 - mae: 1445560.250 - 0s 162us/sample - loss: 2750064412695.5400 - mae: 1239679.1250 - val_loss: 3060256575535.0806 - val_mae: 1316636.0000\n",
      "Epoch 7/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2772394049536.0000 - mae: 1377550.000 - 0s 196us/sample - loss: 2749883564973.6089 - mae: 1239611.1250 - val_loss: 3059969063830.0688 - val_mae: 1316539.2500\n",
      "Epoch 8/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3181601357824.0000 - mae: 1328527.875 - 0s 148us/sample - loss: 2749535727003.9541 - mae: 1239484.7500 - val_loss: 3059455101893.1494 - val_mae: 1316366.1250\n",
      "Epoch 9/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2202611482624.0000 - mae: 1161599.000 - 0s 154us/sample - loss: 2748923884414.5288 - mae: 1239267.7500 - val_loss: 3058594926074.1147 - val_mae: 1316077.0000\n",
      "Epoch 10/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2114858516480.0000 - mae: 1100195.125 - 0s 175us/sample - loss: 2747966444132.0459 - mae: 1238911.5000 - val_loss: 3057238855162.1147 - val_mae: 1315621.3750\n",
      "Epoch 11/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2510352547840.0000 - mae: 1260549.500 - 0s 171us/sample - loss: 2746539865523.4941 - mae: 1238367.6250 - val_loss: 3055211454558.1611 - val_mae: 1314939.6250\n",
      "Epoch 12/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 3384188338176.0000 - mae: 1357732.625 - 0s 198us/sample - loss: 2744334887971.3105 - mae: 1237580.3750 - val_loss: 3052354859337.5630 - val_mae: 1313977.8750\n",
      "Epoch 13/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2861082083328.0000 - mae: 1273393.750 - 0s 169us/sample - loss: 2741303762096.5518 - mae: 1236471.8750 - val_loss: 3048365181328.1841 - val_mae: 1312633.6250\n",
      "Epoch 14/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 5537408221184.0000 - mae: 1462899.500 - 0s 186us/sample - loss: 2737161980304.1841 - mae: 1234938.0000 - val_loss: 3042979240006.6206 - val_mae: 1310817.3750\n",
      "Epoch 15/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2648921604096.0000 - mae: 1278359.750 - 0s 163us/sample - loss: 2731697058309.8848 - mae: 1232896.5000 - val_loss: 3035793442086.2529 - val_mae: 1308391.1250\n",
      "Epoch 16/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1920264962048.0000 - mae: 1133842.375 - 0s 176us/sample - loss: 2724471019778.9424 - mae: 1230198.1250 - val_loss: 3026637728426.6665 - val_mae: 1305289.8750\n",
      "Epoch 17/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1359681028096.0000 - mae: 987077.93 - 0s 188us/sample - loss: 2715131314176.0000 - mae: 1226801.0000 - val_loss: 3015136015795.4946 - val_mae: 1301387.1250\n",
      "Epoch 18/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2270040162304.0000 - mae: 1067789.250 - 0s 145us/sample - loss: 2703607644724.9653 - mae: 1222512.2500 - val_loss: 3000736393145.3794 - val_mae: 1296486.1250\n",
      "Epoch 19/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1593004130304.0000 - mae: 1030827.625 - 0s 154us/sample - loss: 2689657461936.5518 - mae: 1217179.8750 - val_loss: 2982976254658.2070 - val_mae: 1290415.2500\n",
      "Epoch 20/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2785631535104.0000 - mae: 1197520.125 - 0s 168us/sample - loss: 2672017469769.5630 - mae: 1210682.2500 - val_loss: 2961957250436.4136 - val_mae: 1283194.5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-num_layers: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 96</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 96</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_4: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 1210682.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 348 samples\n",
      "Epoch 1/20\n",
      "348/348 [==============================] - ETA: 2s - loss: 3617795342336.0000 - mae: 1510312.500 - 0s 1ms/sample - loss: 2750074823126.8047 - mae: 1239682.2500 - val_loss: 3059425741765.1494 - val_mae: 1316354.6250\n",
      "Epoch 2/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1332635369472.0000 - mae: 971863.75 - 0s 153us/sample - loss: 2742592445428.2300 - mae: 1236805.6250 - val_loss: 3026674931782.6206 - val_mae: 1305280.8750\n",
      "Epoch 3/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 2572941262848.0000 - mae: 1163648.000 - 0s 158us/sample - loss: 2632653001692.6895 - mae: 1194948.5000 - val_loss: 2708442437325.9771 - val_mae: 1192601.1250\n",
      "Epoch 4/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1740095094784.0000 - mae: 1117771.250 - 0s 203us/sample - loss: 1969196570506.2988 - mae: 920879.5625 - val_loss: 1392145341804.8735 - val_mae: 681839.7500\n",
      "Epoch 5/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1629679910912.0000 - mae: 733260.87 - 0s 211us/sample - loss: 1086628777842.7587 - mae: 734647.9375 - val_loss: 998114455881.5632 - val_mae: 774988.8750\n",
      "Epoch 6/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1045504131072.0000 - mae: 834222.06 - 0s 185us/sample - loss: 935538973684.2299 - mae: 665539.6875 - val_loss: 913095778586.4828 - val_mae: 590503.0625\n",
      "Epoch 7/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 519802847232.0000 - mae: 529229.375 - 0s 192us/sample - loss: 893003875622.2528 - mae: 578326.8125 - val_loss: 804601166365.4252 - val_mae: 598462.3750\n",
      "Epoch 8/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 457619046400.0000 - mae: 611416.687 - 0s 164us/sample - loss: 827171236710.9885 - mae: 612097.6250 - val_loss: 749755907907.6781 - val_mae: 580938.3125\n",
      "Epoch 9/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 749571080192.0000 - mae: 614195.875 - 0s 182us/sample - loss: 790859175253.3333 - mae: 563960.5625 - val_loss: 724046472580.4138 - val_mae: 531101.8750\n",
      "Epoch 10/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 276989149184.0000 - mae: 449175.000 - 0s 183us/sample - loss: 766245074732.1379 - mae: 547485.0000 - val_loss: 674500085030.2529 - val_mae: 545171.2500\n",
      "Epoch 11/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 949194391552.0000 - mae: 638849.500 - 0s 189us/sample - loss: 742684854860.5057 - mae: 527780.9375 - val_loss: 665431070413.9772 - val_mae: 496109.0625\n",
      "Epoch 12/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 194500771840.0000 - mae: 378255.437 - 0s 178us/sample - loss: 731163827564.8735 - mae: 522574.4688 - val_loss: 631687842003.8621 - val_mae: 527638.6875\n",
      "Epoch 13/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 584221720576.0000 - mae: 522958.593 - 0s 202us/sample - loss: 712588228396.1379 - mae: 499216.9688 - val_loss: 623099465775.0804 - val_mae: 481842.0312\n",
      "Epoch 14/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 1038473166848.0000 - mae: 543626.50 - 0s 174us/sample - loss: 700534987764.2299 - mae: 500327.9062 - val_loss: 605940355495.7241 - val_mae: 478038.4688\n",
      "Epoch 15/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 515481075712.0000 - mae: 482597.312 - 0s 193us/sample - loss: 689742489906.0229 - mae: 490410.4688 - val_loss: 587124402482.0229 - val_mae: 480619.8750\n",
      "Epoch 16/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 549184372736.0000 - mae: 457047.687 - 0s 175us/sample - loss: 672534868485.8850 - mae: 473187.0312 - val_loss: 577900632134.6207 - val_mae: 464769.2500\n",
      "Epoch 17/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 383809716224.0000 - mae: 465373.437 - 0s 186us/sample - loss: 669140531176.4597 - mae: 476159.8125 - val_loss: 564508455429.8850 - val_mae: 459514.9062\n",
      "Epoch 18/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 533386887168.0000 - mae: 508672.500 - 0s 167us/sample - loss: 659783931597.9771 - mae: 476861.1562 - val_loss: 558806916460.8735 - val_mae: 451384.0938\n",
      "Epoch 19/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 230001655808.0000 - mae: 318513.406 - 0s 184us/sample - loss: 653022184177.2874 - mae: 456063.7812 - val_loss: 549620365288.4598 - val_mae: 452321.0000\n",
      "Epoch 20/20\n",
      "348/348 [==============================] - ETA: 0s - loss: 464397205504.0000 - mae: 409619.000 - 0s 143us/sample - loss: 656025254806.0690 - mae: 482858.7500 - val_loss: 550889051665.6552 - val_mae: 438585.4375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-num_layers: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_4: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 456063.78125</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Results summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Results in models/deep_model_small_provinces/deep_learning_with_keras_tuner</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Showing 10 best trials</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='mae', direction='min') Score: 456063.78125</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='mae', direction='min') Score: 593707.9375</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='mae', direction='min') Score: 630373.3125</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='mae', direction='min') Score: 1210682.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='mae', direction='min') Score: 1239724.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='mae',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory='models/deep_model_small_provinces',\n",
    "    project_name='deep_learning_with_keras_tuner')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(x=train_x,\n",
    "             y=train_y,\n",
    "             epochs=20,\n",
    "             validation_split=0.5,\n",
    "            callbacks = [tensorboard_callback])\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 16994), started 0:02:46 ago. (Use '!kill 16994' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-35617d0e2dd27e66\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-35617d0e2dd27e66\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1 = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_2 = models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 348 samples\n",
      "Epoch 1/500\n",
      "348/348 [==============================] - ETA: 2s - loss: 602702086144.0000 - mae: 545211.500 - 0s 1ms/sample - loss: 654142857875.1265 - mae: 470493.2500 - val_loss: 544743107430.9886 - val_mae: 454627.6875\n",
      "Epoch 2/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 643926523904.0000 - mae: 482232.343 - 0s 159us/sample - loss: 653365209205.7012 - mae: 453365.4688 - val_loss: 541728347289.0114 - val_mae: 441220.5938\n",
      "Epoch 3/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 228768481280.0000 - mae: 340328.750 - 0s 161us/sample - loss: 643301802360.6437 - mae: 458692.1875 - val_loss: 539559504554.6667 - val_mae: 450061.8750\n",
      "Epoch 4/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 548564828160.0000 - mae: 454175.968 - 0s 126us/sample - loss: 639703818710.8046 - mae: 462374.5625 - val_loss: 536173393896.4598 - val_mae: 430849.4375\n",
      "Epoch 5/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 229810864128.0000 - mae: 354600.812 - 0s 149us/sample - loss: 639320737062.2529 - mae: 441351.6250 - val_loss: 533905910819.3104 - val_mae: 440443.7812\n",
      "Epoch 6/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1975217422336.0000 - mae: 526843.50 - 0s 164us/sample - loss: 643266855100.3218 - mae: 474894.9062 - val_loss: 532790827266.9425 - val_mae: 434623.7188\n",
      "Epoch 7/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 372917731328.0000 - mae: 423597.187 - 0s 149us/sample - loss: 636058689818.4828 - mae: 442023.1875 - val_loss: 528308593110.8046 - val_mae: 429623.3438\n",
      "Epoch 8/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 674681716736.0000 - mae: 555879.312 - 0s 177us/sample - loss: 642013132776.4597 - mae: 464429.2812 - val_loss: 529155578491.5862 - val_mae: 422025.0938\n",
      "Epoch 9/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 102014648320.0000 - mae: 244816.312 - 0s 167us/sample - loss: 636107604344.6437 - mae: 435633.2812 - val_loss: 525052797587.1265 - val_mae: 428858.5312\n",
      "Epoch 10/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 272138043392.0000 - mae: 349586.187 - 0s 177us/sample - loss: 632753934194.7587 - mae: 459170.9062 - val_loss: 524287461587.8620 - val_mae: 424666.1562\n",
      "Epoch 11/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 603780284416.0000 - mae: 529887.562 - 0s 162us/sample - loss: 626308939776.0000 - mae: 442940.4062 - val_loss: 522826350403.6782 - val_mae: 430472.2188\n",
      "Epoch 12/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1029314707456.0000 - mae: 601894.12 - 0s 157us/sample - loss: 630145974648.6437 - mae: 459089.5938 - val_loss: 520674411814.2529 - val_mae: 426545.6875\n",
      "Epoch 13/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 232247214080.0000 - mae: 344953.312 - 0s 143us/sample - loss: 628100692509.4253 - mae: 436230.3125 - val_loss: 524388885892.4138 - val_mae: 418998.1562\n",
      "Epoch 14/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1207102210048.0000 - mae: 657068.31 - 0s 173us/sample - loss: 656807440901.8850 - mae: 470344.2188 - val_loss: 528793037859.3104 - val_mae: 418071.3438\n",
      "Epoch 15/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 297555656704.0000 - mae: 383449.468 - 0s 166us/sample - loss: 636861035013.8850 - mae: 437304.2812 - val_loss: 519668793532.3218 - val_mae: 426712.6875\n",
      "Epoch 16/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 201088106496.0000 - mae: 310717.812 - 0s 154us/sample - loss: 621467611653.8850 - mae: 444334.0312 - val_loss: 517795192172.8735 - val_mae: 422654.2188\n",
      "Epoch 17/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 418741616640.0000 - mae: 357835.187 - 0s 166us/sample - loss: 622511094113.1035 - mae: 450225.1875 - val_loss: 518287301431.9080 - val_mae: 423196.5000\n",
      "Epoch 18/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 104083070976.0000 - mae: 236035.656 - 0s 184us/sample - loss: 645343509774.7126 - mae: 441895.9062 - val_loss: 517937447288.6437 - val_mae: 430191.1875\n",
      "Epoch 19/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 642755592192.0000 - mae: 559486.062 - 0s 188us/sample - loss: 621469016534.8046 - mae: 446510.4688 - val_loss: 515370570905.0114 - val_mae: 427758.9375\n",
      "Epoch 20/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 223451545600.0000 - mae: 338230.375 - 0s 189us/sample - loss: 622116970872.6437 - mae: 432287.0938 - val_loss: 516036830408.0920 - val_mae: 416316.5938\n",
      "Epoch 21/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 229150326784.0000 - mae: 286518.437 - 0s 161us/sample - loss: 614983959281.2874 - mae: 443449.0938 - val_loss: 514239446286.7126 - val_mae: 425137.7500\n",
      "Epoch 22/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 455710375936.0000 - mae: 400323.656 - 0s 180us/sample - loss: 613468853589.3334 - mae: 438022.6250 - val_loss: 512942891290.4828 - val_mae: 419875.7188\n",
      "Epoch 23/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 297792569344.0000 - mae: 407537.500 - 0s 185us/sample - loss: 631025763433.9310 - mae: 431610.1250 - val_loss: 518526283352.2759 - val_mae: 434007.2188\n",
      "Epoch 24/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1035066277888.0000 - mae: 663968.43 - 0s 181us/sample - loss: 622229635825.2874 - mae: 454130.7188 - val_loss: 514109856379.5862 - val_mae: 417668.1875\n",
      "Epoch 25/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 863755436032.0000 - mae: 561136.750 - 0s 164us/sample - loss: 613029989128.8276 - mae: 437863.0312 - val_loss: 512835221480.4598 - val_mae: 421096.3750\n",
      "Epoch 26/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 639934529536.0000 - mae: 481940.187 - 0s 146us/sample - loss: 617791025481.5632 - mae: 445404.5000 - val_loss: 514880646179.3104 - val_mae: 414067.1250\n",
      "Epoch 27/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 157266984960.0000 - mae: 280742.218 - 0s 172us/sample - loss: 613378568333.2413 - mae: 431837.4375 - val_loss: 511801164717.6092 - val_mae: 421862.9375\n",
      "Epoch 28/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 222509137920.0000 - mae: 349823.500 - 0s 140us/sample - loss: 614753231507.1265 - mae: 428547.5000 - val_loss: 511054011756.8735 - val_mae: 412976.9062\n",
      "Epoch 29/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 730997456896.0000 - mae: 518388.437 - 0s 183us/sample - loss: 677611853517.9771 - mae: 501020.1250 - val_loss: 516886115951.8161 - val_mae: 413406.7500\n",
      "Epoch 30/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 792487919616.0000 - mae: 456372.031 - 0s 142us/sample - loss: 643424254870.0690 - mae: 426474.4688 - val_loss: 519671696890.1149 - val_mae: 411973.0938\n",
      "Epoch 31/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 3443530924032.0000 - mae: 1026673.500 - 0s 160us/sample - loss: 648761225168.9196 - mae: 463047.2188 - val_loss: 510818675558.9886 - val_mae: 416581.6875\n",
      "Epoch 32/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 529465671680.0000 - mae: 449467.500 - 0s 160us/sample - loss: 617988300140.8735 - mae: 428021.0625 - val_loss: 509248201174.8046 - val_mae: 414424.1250\n",
      "Epoch 33/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 175533080576.0000 - mae: 325287.843 - 0s 149us/sample - loss: 607632879192.2759 - mae: 442686.3438 - val_loss: 510835450326.8046 - val_mae: 418179.5312\n",
      "Epoch 34/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 272582049792.0000 - mae: 381107.000 - 0s 138us/sample - loss: 616358679705.0115 - mae: 425087.2500 - val_loss: 513743070784.7357 - val_mae: 410259.2188\n",
      "Epoch 35/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 753450745856.0000 - mae: 530027.562 - 0s 164us/sample - loss: 638514271385.0115 - mae: 474972.5625 - val_loss: 512063375383.5402 - val_mae: 422375.3438\n",
      "Epoch 36/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 358170624000.0000 - mae: 372237.093 - 0s 166us/sample - loss: 606430347452.3218 - mae: 417033.5312 - val_loss: 516325762813.0575 - val_mae: 408633.5938\n",
      "Epoch 37/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 491063214080.0000 - mae: 446551.500 - 0s 161us/sample - loss: 607244937710.3448 - mae: 434761.4688 - val_loss: 513620006794.2988 - val_mae: 425723.8125\n",
      "Epoch 38/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 355075620864.0000 - mae: 405956.031 - 0s 163us/sample - loss: 622146248233.1954 - mae: 428153.5625 - val_loss: 506980800017.6552 - val_mae: 408507.0938\n",
      "Epoch 39/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 889640714240.0000 - mae: 641150.000 - 0s 174us/sample - loss: 616911609385.1954 - mae: 457697.8438 - val_loss: 512826612159.2643 - val_mae: 426098.7188\n",
      "Epoch 40/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 310766796800.0000 - mae: 402522.375 - 0s 153us/sample - loss: 612963747675.2184 - mae: 422494.4688 - val_loss: 510825272849.6552 - val_mae: 408913.6875\n",
      "Epoch 41/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 220074868736.0000 - mae: 331482.906 - 0s 172us/sample - loss: 606259930500.4138 - mae: 438733.3750 - val_loss: 517417289563.2184 - val_mae: 430652.5625\n",
      "Epoch 42/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 381665411072.0000 - mae: 384811.750 - 0s 162us/sample - loss: 595818315022.7126 - mae: 439059.6250 - val_loss: 513577898784.3678 - val_mae: 409217.6875\n",
      "Epoch 43/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 2094153990144.0000 - mae: 598781.50 - 0s 160us/sample - loss: 604980835763.4943 - mae: 424603.0938 - val_loss: 509249003049.1954 - val_mae: 410425.7812\n",
      "Epoch 44/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 351987531776.0000 - mae: 361768.750 - 0s 174us/sample - loss: 597639887766.0690 - mae: 421335.0312 - val_loss: 506352682843.2184 - val_mae: 415311.1250\n",
      "Epoch 45/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 869386223616.0000 - mae: 643120.812 - 0s 149us/sample - loss: 607096911848.4597 - mae: 448931.8750 - val_loss: 504596809751.5402 - val_mae: 410774.7500\n",
      "Epoch 46/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 391382171648.0000 - mae: 392827.062 - 0s 162us/sample - loss: 606262028782.3448 - mae: 414944.0000 - val_loss: 506642516556.5057 - val_mae: 404228.2188\n",
      "Epoch 47/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 154814824448.0000 - mae: 274835.562 - 0s 134us/sample - loss: 596729686322.0231 - mae: 437526.6562 - val_loss: 525075865694.1609 - val_mae: 440824.6562\n",
      "Epoch 48/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 584625094656.0000 - mae: 457753.250 - 0s 140us/sample - loss: 597023547486.1609 - mae: 428773.3438 - val_loss: 507894559991.1724 - val_mae: 406403.8125\n",
      "Epoch 49/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 2291158482944.0000 - mae: 669886.81 - 0s 176us/sample - loss: 592672928261.8850 - mae: 424822.5625 - val_loss: 506724065185.8391 - val_mae: 411929.3750\n",
      "Epoch 50/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 347158937600.0000 - mae: 419721.343 - 0s 131us/sample - loss: 594795575649.1035 - mae: 427969.4375 - val_loss: 508608404656.5517 - val_mae: 417466.7188\n",
      "Epoch 51/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 204123799552.0000 - mae: 315836.937 - 0s 179us/sample - loss: 591367118188.8735 - mae: 420671.5000 - val_loss: 507306979469.2414 - val_mae: 406161.0000\n",
      "Epoch 52/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 770541879296.0000 - mae: 430480.593 - 0s 154us/sample - loss: 597454528229.5172 - mae: 426960.1250 - val_loss: 505767216045.6092 - val_mae: 408016.1875\n",
      "Epoch 53/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 245612380160.0000 - mae: 331437.000 - 0s 161us/sample - loss: 594878912205.9769 - mae: 424241.3750 - val_loss: 506825782330.8506 - val_mae: 410456.9062\n",
      "Epoch 54/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 199188152320.0000 - mae: 279270.062 - 0s 159us/sample - loss: 589945810331.9540 - mae: 422044.1250 - val_loss: 508026028220.3218 - val_mae: 410981.1562\n",
      "Epoch 55/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 177601839104.0000 - mae: 289152.187 - 0s 151us/sample - loss: 590935917673.9310 - mae: 427220.4688 - val_loss: 507901468012.8735 - val_mae: 411253.1875\n",
      "Epoch 56/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 145341480960.0000 - mae: 238957.750 - 0s 161us/sample - loss: 596728268823.5403 - mae: 415866.8438 - val_loss: 511335507144.0920 - val_mae: 401046.6562\n",
      "Epoch 57/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 553277718528.0000 - mae: 417539.062 - 0s 133us/sample - loss: 599257052595.4943 - mae: 434400.6562 - val_loss: 509908739083.7701 - val_mae: 418259.8750\n",
      "Epoch 58/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 489230794752.0000 - mae: 433436.062 - 0s 150us/sample - loss: 585574668358.6207 - mae: 420520.3750 - val_loss: 506662267939.3104 - val_mae: 407511.6250\n",
      "Epoch 59/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 203104665600.0000 - mae: 322954.750 - 0s 155us/sample - loss: 606087098509.2413 - mae: 417210.4062 - val_loss: 509228387457.4713 - val_mae: 415779.1250\n",
      "Epoch 60/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 380672081920.0000 - mae: 341461.875 - 0s 172us/sample - loss: 588414261883.5862 - mae: 436948.4688 - val_loss: 510266527979.4023 - val_mae: 413515.8750\n",
      "Epoch 61/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 304061579264.0000 - mae: 402917.343 - 0s 165us/sample - loss: 596845284481.4712 - mae: 419094.0312 - val_loss: 507444114490.8506 - val_mae: 411554.3438\n",
      "Epoch 62/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 319144067072.0000 - mae: 349202.500 - 0s 131us/sample - loss: 582336244959.6322 - mae: 421069.9375 - val_loss: 507938493404.6896 - val_mae: 411877.8750\n",
      "Epoch 63/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 431479586816.0000 - mae: 400510.250 - 0s 144us/sample - loss: 583200895305.5632 - mae: 422773.4688 - val_loss: 509467507606.0690 - val_mae: 412970.7188\n",
      "Epoch 64/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 275094175744.0000 - mae: 355327.562 - 0s 155us/sample - loss: 582734150550.0690 - mae: 416047.0938 - val_loss: 510336362802.0230 - val_mae: 401294.3125\n",
      "Epoch 65/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 516516741120.0000 - mae: 401216.687 - 0s 179us/sample - loss: 580197338618.1150 - mae: 410867.9688 - val_loss: 506532105910.4368 - val_mae: 405599.5938\n",
      "Epoch 66/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 417503182848.0000 - mae: 486285.843 - 0s 183us/sample - loss: 580729313162.2988 - mae: 422390.6562 - val_loss: 516129539554.5747 - val_mae: 421535.9688\n",
      "Epoch 67/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 360009793536.0000 - mae: 338584.812 - 0s 192us/sample - loss: 594366296393.5632 - mae: 418972.0000 - val_loss: 514269526557.4253 - val_mae: 416462.6250\n",
      "Epoch 68/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 452183457792.0000 - mae: 454686.218 - 0s 142us/sample - loss: 579175946875.5862 - mae: 424502.2500 - val_loss: 517052201689.7471 - val_mae: 422437.7812\n",
      "Epoch 69/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1960202469376.0000 - mae: 586658.25 - 0s 147us/sample - loss: 576001426443.7701 - mae: 423866.4375 - val_loss: 512701148760.2759 - val_mae: 405303.4375\n",
      "Epoch 70/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1917956259840.0000 - mae: 512190.62 - 0s 157us/sample - loss: 582756589473.8391 - mae: 414544.8125 - val_loss: 512392121850.1149 - val_mae: 412342.4062\n",
      "Epoch 71/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 313434505216.0000 - mae: 364635.437 - 0s 145us/sample - loss: 576703568507.5862 - mae: 420722.8125 - val_loss: 508192239568.9196 - val_mae: 407156.0000\n",
      "Epoch 72/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 518759710720.0000 - mae: 408654.500 - 0s 142us/sample - loss: 574205361046.0690 - mae: 410460.4688 - val_loss: 509907304636.3218 - val_mae: 408565.6562\n",
      "Epoch 73/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 695513120768.0000 - mae: 508713.218 - 0s 151us/sample - loss: 577737902291.8621 - mae: 423634.5312 - val_loss: 511572380424.8276 - val_mae: 405932.4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 245303803904.0000 - mae: 345672.187 - 0s 164us/sample - loss: 585021981048.6437 - mae: 411735.5938 - val_loss: 512620833262.3448 - val_mae: 411248.1875\n",
      "Epoch 75/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 593045684224.0000 - mae: 496622.375 - 0s 159us/sample - loss: 602042938238.5288 - mae: 453659.5938 - val_loss: 513360691011.6782 - val_mae: 402783.8750\n",
      "Epoch 76/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 746088693760.0000 - mae: 511718.531 - 0s 138us/sample - loss: 596871691346.3907 - mae: 411322.1250 - val_loss: 511528982786.9425 - val_mae: 399338.0312\n",
      "Epoch 77/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 2638007500800.0000 - mae: 704331.43 - 0s 168us/sample - loss: 613842033275.5862 - mae: 444246.7500 - val_loss: 514980932266.6667 - val_mae: 400511.8125\n",
      "Epoch 78/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 717434716160.0000 - mae: 458737.750 - 0s 153us/sample - loss: 581786508276.2299 - mae: 404930.7500 - val_loss: 510735589470.1609 - val_mae: 403303.4375\n",
      "Epoch 79/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 388841013248.0000 - mae: 406814.437 - 0s 137us/sample - loss: 567897162857.9310 - mae: 409942.4375 - val_loss: 519188411450.8506 - val_mae: 419690.0312\n",
      "Epoch 80/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 513988886528.0000 - mae: 407420.125 - 0s 156us/sample - loss: 574120348801.4712 - mae: 419131.5000 - val_loss: 513429398351.4483 - val_mae: 406885.7812\n",
      "Epoch 81/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 377549946880.0000 - mae: 386825.906 - 0s 147us/sample - loss: 574253141309.7931 - mae: 409915.5000 - val_loss: 517862134937.0114 - val_mae: 416365.0000\n",
      "Epoch 82/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 275288457216.0000 - mae: 357031.437 - 0s 187us/sample - loss: 586494287589.5172 - mae: 442118.9375 - val_loss: 515826761916.3218 - val_mae: 407499.7812\n",
      "Epoch 83/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 215489904640.0000 - mae: 304135.218 - 0s 168us/sample - loss: 603028333579.7701 - mae: 412385.8438 - val_loss: 518288788986.1149 - val_mae: 398579.9062\n",
      "Epoch 84/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 360992047104.0000 - mae: 374591.875 - 0s 158us/sample - loss: 561486822670.7126 - mae: 417343.6250 - val_loss: 528077925411.3104 - val_mae: 429061.1875\n",
      "Epoch 85/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 190175674368.0000 - mae: 267521.562 - 0s 132us/sample - loss: 570931190018.9425 - mae: 416482.3125 - val_loss: 521525330296.6437 - val_mae: 400500.9688\n",
      "Epoch 86/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 290432974848.0000 - mae: 398235.156 - 0s 157us/sample - loss: 569991167623.3563 - mae: 410229.1562 - val_loss: 531162732991.2643 - val_mae: 426726.9375\n",
      "Epoch 87/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 164155326464.0000 - mae: 296472.750 - 0s 152us/sample - loss: 568977765999.8160 - mae: 416021.7500 - val_loss: 518522029538.5747 - val_mae: 405016.3750\n",
      "Epoch 88/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 153530236928.0000 - mae: 260329.125 - 0s 154us/sample - loss: 560015951518.8965 - mae: 406489.0938 - val_loss: 525943855174.6207 - val_mae: 422187.6875\n",
      "Epoch 89/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 387820421120.0000 - mae: 436251.500 - 0s 133us/sample - loss: 569619855148.1379 - mae: 425697.0000 - val_loss: 522406889872.1839 - val_mae: 406918.8125\n",
      "Epoch 90/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 149831041024.0000 - mae: 281330.000 - 0s 144us/sample - loss: 586424073675.0345 - mae: 406992.1875 - val_loss: 520071572326.9886 - val_mae: 401019.9688\n",
      "Epoch 91/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1096639840256.0000 - mae: 633241.87 - 0s 155us/sample - loss: 576822680940.8735 - mae: 436438.5625 - val_loss: 535942442584.2759 - val_mae: 430451.9688\n",
      "Epoch 92/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 417363197952.0000 - mae: 361109.906 - 0s 158us/sample - loss: 586987901728.3678 - mae: 409394.1250 - val_loss: 523114468881.6552 - val_mae: 404078.4375\n",
      "Epoch 93/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 746243162112.0000 - mae: 505571.562 - 0s 173us/sample - loss: 591348513874.3907 - mae: 439487.7812 - val_loss: 537686053852.6896 - val_mae: 431491.0938\n",
      "Epoch 94/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 467038633984.0000 - mae: 414042.812 - 0s 153us/sample - loss: 585183482468.0460 - mae: 412058.6562 - val_loss: 529745799285.7012 - val_mae: 401463.8125\n",
      "Epoch 95/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 393277210624.0000 - mae: 339934.281 - 0s 165us/sample - loss: 562587435572.9655 - mae: 414633.0938 - val_loss: 542750179516.3218 - val_mae: 435145.5938\n",
      "Epoch 96/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 358294388736.0000 - mae: 345237.500 - 0s 168us/sample - loss: 552891388704.3678 - mae: 410774.3438 - val_loss: 527047126616.2759 - val_mae: 403999.0312\n",
      "Epoch 97/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 316445032448.0000 - mae: 323144.343 - 0s 160us/sample - loss: 566700964004.7816 - mae: 403189.8750 - val_loss: 522438483497.1954 - val_mae: 414463.8750\n",
      "Epoch 98/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 718945320960.0000 - mae: 478701.750 - 0s 149us/sample - loss: 559254074426.8506 - mae: 418448.4062 - val_loss: 526188114626.2069 - val_mae: 415466.0312\n",
      "Epoch 99/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 90368409600.0000 - mae: 192498.46 - 0s 165us/sample - loss: 556563188865.4712 - mae: 402301.6875 - val_loss: 525682061076.5977 - val_mae: 407929.8750\n",
      "Epoch 100/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 529386635264.0000 - mae: 425588.093 - 0s 152us/sample - loss: 552363082857.9310 - mae: 407953.9688 - val_loss: 529538191218.7586 - val_mae: 420848.6875\n",
      "Epoch 101/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 590710702080.0000 - mae: 494595.937 - 0s 155us/sample - loss: 567270359604.9655 - mae: 405734.3438 - val_loss: 524202209044.5977 - val_mae: 399916.3750\n",
      "Epoch 102/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 851244744704.0000 - mae: 506940.093 - 0s 156us/sample - loss: 598365892560.9196 - mae: 438233.5625 - val_loss: 526923346814.5287 - val_mae: 410533.6875\n",
      "Epoch 103/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 607575736320.0000 - mae: 456546.687 - 0s 161us/sample - loss: 572672699803.9540 - mae: 400824.8750 - val_loss: 527969804758.8046 - val_mae: 401369.3750\n",
      "Epoch 104/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 188329558016.0000 - mae: 288455.812 - 0s 145us/sample - loss: 543636518700.1379 - mae: 407752.0312 - val_loss: 547743859817.9310 - val_mae: 441824.4688\n",
      "Epoch 105/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 500855865344.0000 - mae: 440089.843 - 0s 169us/sample - loss: 555917489022.5288 - mae: 420623.6875 - val_loss: 530473768653.9770 - val_mae: 405983.5938\n",
      "Epoch 106/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 205511131136.0000 - mae: 322324.937 - 0s 151us/sample - loss: 555056445416.4597 - mae: 399772.2188 - val_loss: 523651517569.4713 - val_mae: 408901.5312\n",
      "Epoch 107/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 194150236160.0000 - mae: 265887.406 - 0s 163us/sample - loss: 564409591360.7356 - mae: 428862.1562 - val_loss: 531956223670.4368 - val_mae: 422371.1875\n",
      "Epoch 108/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 221800005632.0000 - mae: 356891.750 - 0s 162us/sample - loss: 563374411634.7587 - mae: 399035.5000 - val_loss: 526542554782.8965 - val_mae: 405130.1562\n",
      "Epoch 109/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 212853850112.0000 - mae: 281718.062 - 0s 150us/sample - loss: 548030739138.2068 - mae: 400541.7500 - val_loss: 527463053100.1380 - val_mae: 416691.9688\n",
      "Epoch 110/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 542514479104.0000 - mae: 470295.468 - 0s 161us/sample - loss: 548695653646.7126 - mae: 415501.9375 - val_loss: 535406221382.6207 - val_mae: 421064.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 241022582784.0000 - mae: 365630.875 - 0s 155us/sample - loss: 554609105719.9081 - mae: 403709.0000 - val_loss: 532444790784.0000 - val_mae: 412126.4688\n",
      "Epoch 112/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 406274244608.0000 - mae: 338792.562 - 0s 151us/sample - loss: 595588444489.5632 - mae: 445850.8438 - val_loss: 533407539105.8391 - val_mae: 410525.5312\n",
      "Epoch 113/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 531658276864.0000 - mae: 465143.375 - 0s 154us/sample - loss: 572321892752.1840 - mae: 400359.8125 - val_loss: 530321344912.1839 - val_mae: 408145.5938\n",
      "Epoch 114/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 139600363520.0000 - mae: 260416.687 - 0s 162us/sample - loss: 543371890358.4368 - mae: 409121.8438 - val_loss: 535106067067.5862 - val_mae: 424634.7500\n",
      "Epoch 115/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 995863691264.0000 - mae: 602981.125 - 0s 171us/sample - loss: 552221963981.9769 - mae: 416599.6875 - val_loss: 533437623707.9540 - val_mae: 407297.6875\n",
      "Epoch 116/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 854982000640.0000 - mae: 540259.937 - 0s 145us/sample - loss: 577875207273.9310 - mae: 400892.6875 - val_loss: 528962673840.5517 - val_mae: 410886.8125\n",
      "Epoch 117/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 463149301760.0000 - mae: 359297.812 - 0s 160us/sample - loss: 558422500175.4484 - mae: 426235.8750 - val_loss: 534307533506.2069 - val_mae: 421666.3125\n",
      "Epoch 118/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 350325506048.0000 - mae: 427397.937 - 0s 147us/sample - loss: 560369824356.0460 - mae: 399861.6875 - val_loss: 534820640650.2988 - val_mae: 413648.8750\n",
      "Epoch 119/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 620669698048.0000 - mae: 534132.125 - 0s 158us/sample - loss: 545221416842.2989 - mae: 410380.6875 - val_loss: 542976767223.1724 - val_mae: 422462.1250\n",
      "Epoch 120/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 285226369024.0000 - mae: 363970.062 - 0s 153us/sample - loss: 543268241784.6437 - mae: 399665.7500 - val_loss: 531193183126.0690 - val_mae: 408190.2188\n",
      "Epoch 121/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 287286165504.0000 - mae: 369207.031 - 0s 166us/sample - loss: 545204351493.8851 - mae: 409919.0312 - val_loss: 540466439368.0920 - val_mae: 425036.7500\n",
      "Epoch 122/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 145582358528.0000 - mae: 272017.875 - 0s 162us/sample - loss: 550585590607.4484 - mae: 398170.4062 - val_loss: 533802567138.5747 - val_mae: 415150.6250\n",
      "Epoch 123/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1796216324096.0000 - mae: 526443.18 - 0s 175us/sample - loss: 553637355614.1609 - mae: 421418.4375 - val_loss: 538675843119.0804 - val_mae: 413941.0938\n",
      "Epoch 124/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 127590539264.0000 - mae: 253177.437 - 0s 175us/sample - loss: 572225835243.4022 - mae: 401829.8438 - val_loss: 534957875482.4828 - val_mae: 414994.0625\n",
      "Epoch 125/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 201927557120.0000 - mae: 267135.312 - 0s 153us/sample - loss: 537371927940.4138 - mae: 405351.5312 - val_loss: 534029963499.4023 - val_mae: 415196.0312\n",
      "Epoch 126/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 548048797696.0000 - mae: 424271.687 - 0s 181us/sample - loss: 543164097171.1264 - mae: 403904.4062 - val_loss: 542253336976.1839 - val_mae: 421720.0000\n",
      "Epoch 127/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 785270833152.0000 - mae: 551404.375 - 0s 187us/sample - loss: 541537417569.1035 - mae: 405113.3750 - val_loss: 541156763200.7357 - val_mae: 412530.2188\n",
      "Epoch 128/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 141780975616.0000 - mae: 266028.437 - 0s 180us/sample - loss: 549239702633.9310 - mae: 394682.8438 - val_loss: 536230261253.8851 - val_mae: 415276.0938\n",
      "Epoch 129/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1966075674624.0000 - mae: 604547.75 - 0s 182us/sample - loss: 580124601944.2759 - mae: 431950.4062 - val_loss: 541960161115.2184 - val_mae: 408041.0938\n",
      "Epoch 130/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 405396324352.0000 - mae: 294244.875 - 0s 162us/sample - loss: 548731502027.0345 - mae: 396139.2500 - val_loss: 544760821736.4598 - val_mae: 422780.0312\n",
      "Epoch 131/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 303058780160.0000 - mae: 377974.312 - 0s 178us/sample - loss: 539258270590.5287 - mae: 397946.8125 - val_loss: 541469417566.1609 - val_mae: 421895.1875\n",
      "Epoch 132/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 344525897728.0000 - mae: 426309.812 - 0s 151us/sample - loss: 535359733948.3218 - mae: 404778.0312 - val_loss: 542912504090.4828 - val_mae: 420317.4375\n",
      "Epoch 133/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 424271478784.0000 - mae: 414047.500 - 0s 160us/sample - loss: 537174284923.5862 - mae: 403180.3750 - val_loss: 548534590452.2299 - val_mae: 422286.7500\n",
      "Epoch 134/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 357700337664.0000 - mae: 344106.187 - 0s 163us/sample - loss: 534723788423.3563 - mae: 396986.2500 - val_loss: 544121377756.6896 - val_mae: 410947.0938\n",
      "Epoch 135/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 201764945920.0000 - mae: 257463.687 - 0s 174us/sample - loss: 533236660765.4253 - mae: 396731.7812 - val_loss: 548285213731.3104 - val_mae: 422939.9688\n",
      "Epoch 136/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 184665341952.0000 - mae: 327555.375 - 0s 147us/sample - loss: 535701242985.9310 - mae: 409817.2500 - val_loss: 548082513331.4943 - val_mae: 416367.5312\n",
      "Epoch 137/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 444679127040.0000 - mae: 409347.562 - 0s 154us/sample - loss: 539607473140.2299 - mae: 400018.4375 - val_loss: 548761259514.1149 - val_mae: 425686.0625\n",
      "Epoch 138/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 369536139264.0000 - mae: 333179.968 - 0s 175us/sample - loss: 527955564802.9425 - mae: 399139.4062 - val_loss: 542887979502.3448 - val_mae: 410062.6250\n",
      "Epoch 139/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 272617144320.0000 - mae: 366382.156 - 0s 145us/sample - loss: 539924829713.6552 - mae: 400266.1562 - val_loss: 543723619245.6092 - val_mae: 415145.6875\n",
      "Epoch 140/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 478902714368.0000 - mae: 440307.937 - 0s 145us/sample - loss: 529986599170.9425 - mae: 393593.7812 - val_loss: 550604700001.1034 - val_mae: 421435.0938\n",
      "Epoch 141/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 442576142336.0000 - mae: 426886.187 - 0s 157us/sample - loss: 536311149438.5287 - mae: 404874.2500 - val_loss: 548583327767.5402 - val_mae: 412915.5000\n",
      "Epoch 142/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 234230808576.0000 - mae: 313566.250 - 0s 161us/sample - loss: 527116304007.3563 - mae: 392205.2500 - val_loss: 550668378394.4828 - val_mae: 418114.3125\n",
      "Epoch 143/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 563852083200.0000 - mae: 462219.125 - 0s 152us/sample - loss: 525588792425.9310 - mae: 396221.2812 - val_loss: 557484530299.5862 - val_mae: 432246.4688\n",
      "Epoch 144/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 410039615488.0000 - mae: 468402.062 - 0s 153us/sample - loss: 531012929559.5402 - mae: 402662.4688 - val_loss: 554284019853.2413 - val_mae: 425844.1875\n",
      "Epoch 145/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 435586334720.0000 - mae: 435298.937 - 0s 159us/sample - loss: 528551416231.7242 - mae: 397702.9062 - val_loss: 558581513863.3563 - val_mae: 427820.7500\n",
      "Epoch 146/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 109150912512.0000 - mae: 226275.750 - 0s 147us/sample - loss: 538552730729.9310 - mae: 397130.5312 - val_loss: 555007336918.8046 - val_mae: 426544.8750\n",
      "Epoch 147/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 366699282432.0000 - mae: 374693.437 - 0s 155us/sample - loss: 532485560390.6207 - mae: 404021.5312 - val_loss: 563584029284.0460 - val_mae: 438495.9688\n",
      "Epoch 148/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 267691163648.0000 - mae: 367243.281 - 0s 165us/sample - loss: 531167689810.3908 - mae: 404586.3438 - val_loss: 554374278744.2759 - val_mae: 412741.4375\n",
      "Epoch 149/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 199158972416.0000 - mae: 287721.156 - 0s 165us/sample - loss: 544551961705.9310 - mae: 398257.6562 - val_loss: 576111444909.6093 - val_mae: 448090.9375\n",
      "Epoch 150/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 315340881920.0000 - mae: 387033.406 - 0s 154us/sample - loss: 528499418123.7701 - mae: 401957.0625 - val_loss: 551646438223.4482 - val_mae: 417155.9688\n",
      "Epoch 151/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 509984669696.0000 - mae: 458700.187 - 0s 159us/sample - loss: 533888241334.4368 - mae: 400868.5938 - val_loss: 557512341480.4597 - val_mae: 415822.7188\n",
      "Epoch 152/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 219538915328.0000 - mae: 313528.437 - 0s 159us/sample - loss: 524203430123.4023 - mae: 390897.8750 - val_loss: 560685567670.4368 - val_mae: 429270.2500\n",
      "Epoch 153/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 273144627200.0000 - mae: 355171.687 - 0s 165us/sample - loss: 540719838643.4943 - mae: 422750.8125 - val_loss: 558040818323.1265 - val_mae: 413927.5312\n",
      "Epoch 154/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 346164690944.0000 - mae: 339918.562 - 0s 162us/sample - loss: 529692116073.9310 - mae: 391260.1250 - val_loss: 556633802210.5747 - val_mae: 412772.0312\n",
      "Epoch 155/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 463988391936.0000 - mae: 432592.812 - 0s 143us/sample - loss: 539584846742.0690 - mae: 413786.4375 - val_loss: 559285930384.1840 - val_mae: 422984.5938\n",
      "Epoch 156/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 357159206912.0000 - mae: 347219.000 - 0s 157us/sample - loss: 525591384864.3679 - mae: 391811.1250 - val_loss: 556825814027.7701 - val_mae: 412507.5312\n",
      "Epoch 157/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 88466948096.0000 - mae: 194941.50 - 0s 164us/sample - loss: 527899186893.9770 - mae: 405752.1250 - val_loss: 565879986999.9081 - val_mae: 432422.9062\n",
      "Epoch 158/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 2109285072896.0000 - mae: 712734.37 - 0s 157us/sample - loss: 521696117489.2874 - mae: 395505.1875 - val_loss: 562431417143.9081 - val_mae: 412769.8750\n",
      "Epoch 159/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 272026435584.0000 - mae: 373874.531 - 0s 155us/sample - loss: 520236185953.1035 - mae: 393924.0000 - val_loss: 574615072155.9540 - val_mae: 444449.0625\n",
      "Epoch 160/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 446715920384.0000 - mae: 458414.625 - 0s 180us/sample - loss: 531215842009.7471 - mae: 407692.2812 - val_loss: 562910448581.1494 - val_mae: 426169.9375\n",
      "Epoch 161/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 586620469248.0000 - mae: 402751.343 - 0s 156us/sample - loss: 536288360989.4253 - mae: 412616.3750 - val_loss: 567476353000.4597 - val_mae: 416782.3125\n",
      "Epoch 162/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 244402765824.0000 - mae: 263543.562 - 0s 163us/sample - loss: 529390423922.7587 - mae: 393489.6875 - val_loss: 557444182345.5632 - val_mae: 416801.1562\n",
      "Epoch 163/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 583473561600.0000 - mae: 486350.750 - 0s 152us/sample - loss: 529278434009.7471 - mae: 402345.0000 - val_loss: 585562698057.5632 - val_mae: 452105.5312\n",
      "Epoch 164/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 212354105344.0000 - mae: 296233.593 - 0s 162us/sample - loss: 523586380623.4483 - mae: 398275.9688 - val_loss: 562877297157.8850 - val_mae: 417145.1562\n",
      "Epoch 165/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 235329994752.0000 - mae: 356787.156 - 0s 156us/sample - loss: 514782327866.8506 - mae: 391252.7812 - val_loss: 572261250483.4943 - val_mae: 436502.5625\n",
      "Epoch 166/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 252929081344.0000 - mae: 359530.500 - 0s 166us/sample - loss: 517440736197.1494 - mae: 405932.8750 - val_loss: 570138876162.9425 - val_mae: 429131.3125\n",
      "Epoch 167/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 374038822912.0000 - mae: 394603.687 - 0s 173us/sample - loss: 526550649608.8276 - mae: 392999.3125 - val_loss: 568439081736.8276 - val_mae: 427548.8125\n",
      "Epoch 168/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 174779236352.0000 - mae: 262714.875 - 0s 180us/sample - loss: 529604134182.2529 - mae: 416542.7188 - val_loss: 571058633998.7126 - val_mae: 427222.5312\n",
      "Epoch 169/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 403229376512.0000 - mae: 398687.500 - 0s 163us/sample - loss: 541092216078.7126 - mae: 394293.8438 - val_loss: 564718207917.6093 - val_mae: 419728.6562\n",
      "Epoch 170/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 239886139392.0000 - mae: 292542.875 - 0s 179us/sample - loss: 517595044098.9425 - mae: 405918.0312 - val_loss: 571396869249.4713 - val_mae: 433921.0938\n",
      "Epoch 171/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 194582167552.0000 - mae: 309277.093 - 0s 161us/sample - loss: 516099368136.0919 - mae: 393492.5938 - val_loss: 571126824206.7126 - val_mae: 426167.2188\n",
      "Epoch 172/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 814887665664.0000 - mae: 517571.562 - 0s 153us/sample - loss: 519671033432.2758 - mae: 401310.3125 - val_loss: 568578782455.1724 - val_mae: 421551.2188\n",
      "Epoch 173/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 289726758912.0000 - mae: 338850.062 - 0s 186us/sample - loss: 518471013340.6896 - mae: 390033.7812 - val_loss: 567349786035.4943 - val_mae: 432925.9375\n",
      "Epoch 174/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 601086033920.0000 - mae: 453156.687 - 0s 160us/sample - loss: 524827282373.1494 - mae: 412625.4375 - val_loss: 573125688590.7126 - val_mae: 421237.8438\n",
      "Epoch 175/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 372069171200.0000 - mae: 403686.500 - 0s 169us/sample - loss: 537916549802.6666 - mae: 412069.8750 - val_loss: 592657536153.0115 - val_mae: 450635.7812\n",
      "Epoch 176/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 256987758592.0000 - mae: 334255.375 - 0s 179us/sample - loss: 537876738000.9196 - mae: 395361.7812 - val_loss: 570597327036.3219 - val_mae: 426814.6562\n",
      "Epoch 177/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 405595684864.0000 - mae: 377168.875 - 0s 181us/sample - loss: 513081957929.1954 - mae: 397572.0312 - val_loss: 576796525862.2529 - val_mae: 428307.8750\n",
      "Epoch 178/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 459787894784.0000 - mae: 443277.625 - 0s 167us/sample - loss: 521747645169.2874 - mae: 398197.1562 - val_loss: 579212240072.0919 - val_mae: 430630.3438\n",
      "Epoch 179/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 279076208640.0000 - mae: 345827.687 - 0s 157us/sample - loss: 532957909980.6896 - mae: 392127.9062 - val_loss: 573579778306.9425 - val_mae: 434940.8750\n",
      "Epoch 180/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 284335603712.0000 - mae: 393829.906 - 0s 137us/sample - loss: 524319029412.7816 - mae: 411175.6250 - val_loss: 568235351969.8391 - val_mae: 418031.7188\n",
      "Epoch 181/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 271224946688.0000 - mae: 297460.812 - 0s 158us/sample - loss: 511113932941.2413 - mae: 389803.1250 - val_loss: 581441254270.5287 - val_mae: 433221.7500\n",
      "Epoch 182/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1703371341824.0000 - mae: 544708.31 - 0s 161us/sample - loss: 516229204933.1494 - mae: 405259.8125 - val_loss: 573838449087.2644 - val_mae: 419593.8750\n",
      "Epoch 183/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - ETA: 0s - loss: 406304063488.0000 - mae: 370984.687 - 0s 157us/sample - loss: 516198669111.9081 - mae: 392814.7500 - val_loss: 571278790138.1150 - val_mae: 428777.4375\n",
      "Epoch 184/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 251631517696.0000 - mae: 322136.312 - 0s 167us/sample - loss: 514467408554.6666 - mae: 408109.1875 - val_loss: 576377517679.8160 - val_mae: 426433.5625\n",
      "Epoch 185/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 400992370688.0000 - mae: 387266.875 - 0s 159us/sample - loss: 517929827598.7126 - mae: 386650.8438 - val_loss: 574089065154.2069 - val_mae: 425623.0000\n",
      "Epoch 186/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 343011033088.0000 - mae: 411599.343 - 0s 157us/sample - loss: 507442134851.6782 - mae: 398143.6250 - val_loss: 587891107769.3793 - val_mae: 444765.2812\n",
      "Epoch 187/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 264014659584.0000 - mae: 330683.281 - 0s 141us/sample - loss: 505403215165.7932 - mae: 398073.3438 - val_loss: 578160486706.0229 - val_mae: 425670.7188\n",
      "Epoch 188/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1743023374336.0000 - mae: 535793.31 - 0s 166us/sample - loss: 506434279400.4598 - mae: 391289.5312 - val_loss: 578901309416.4597 - val_mae: 428348.6875\n",
      "Epoch 189/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 442823213056.0000 - mae: 381649.562 - 0s 147us/sample - loss: 521785962072.2758 - mae: 403061.3438 - val_loss: 588287322229.7012 - val_mae: 433698.9062\n",
      "Epoch 190/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1871458861056.0000 - mae: 600812.68 - 0s 166us/sample - loss: 506137559604.9655 - mae: 387189.0938 - val_loss: 577119751109.1494 - val_mae: 417009.7500\n",
      "Epoch 191/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 431096856576.0000 - mae: 371954.437 - 0s 154us/sample - loss: 509774447156.9655 - mae: 403999.0938 - val_loss: 595017665853.7931 - val_mae: 452138.1250\n",
      "Epoch 192/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 858107936768.0000 - mae: 628013.437 - 0s 147us/sample - loss: 502773319326.8965 - mae: 394683.8125 - val_loss: 575488625793.4713 - val_mae: 421772.6562\n",
      "Epoch 193/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 388294639616.0000 - mae: 390572.750 - 0s 170us/sample - loss: 505427377234.3908 - mae: 387968.0000 - val_loss: 576057188069.5172 - val_mae: 423857.3438\n",
      "Epoch 194/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 125473423360.0000 - mae: 248940.875 - 0s 170us/sample - loss: 501349664414.8965 - mae: 392305.2500 - val_loss: 584869942801.6552 - val_mae: 432796.8125\n",
      "Epoch 195/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 242739019776.0000 - mae: 302854.187 - 0s 159us/sample - loss: 508986623270.2529 - mae: 388314.9375 - val_loss: 580151057772.8735 - val_mae: 427396.1250\n",
      "Epoch 196/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 538245627904.0000 - mae: 424048.218 - 0s 184us/sample - loss: 501312398912.7356 - mae: 402300.4688 - val_loss: 594663606295.5403 - val_mae: 441305.4375\n",
      "Epoch 197/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 543617941504.0000 - mae: 449915.937 - 0s 175us/sample - loss: 500829394826.2989 - mae: 385880.6875 - val_loss: 579732835080.8276 - val_mae: 423877.5625\n",
      "Epoch 198/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 491551522816.0000 - mae: 425208.468 - 0s 184us/sample - loss: 498079238355.8621 - mae: 388551.7812 - val_loss: 585504595120.5518 - val_mae: 436556.7500\n",
      "Epoch 199/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1668578017280.0000 - mae: 537974.31 - 0s 134us/sample - loss: 544595941634.9425 - mae: 420013.8438 - val_loss: 597454158930.3907 - val_mae: 422537.6875\n",
      "Epoch 200/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 673372110848.0000 - mae: 503980.375 - 0s 164us/sample - loss: 513367483968.7356 - mae: 396916.2812 - val_loss: 612079522592.3678 - val_mae: 453463.5312\n",
      "Epoch 201/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 240780967936.0000 - mae: 332831.750 - 0s 140us/sample - loss: 502457896347.9540 - mae: 390874.6250 - val_loss: 584861765679.0804 - val_mae: 426101.3438\n",
      "Epoch 202/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 310052257792.0000 - mae: 322780.218 - 0s 161us/sample - loss: 515114014978.9425 - mae: 406177.5625 - val_loss: 587046285453.2413 - val_mae: 425368.5625\n",
      "Epoch 203/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 132442603520.0000 - mae: 226923.593 - 0s 160us/sample - loss: 516395870325.7011 - mae: 389118.0312 - val_loss: 588850894165.3334 - val_mae: 428948.9062\n",
      "Epoch 204/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 410921598976.0000 - mae: 386204.937 - 0s 163us/sample - loss: 517283812505.0115 - mae: 398327.0000 - val_loss: 603522506469.5172 - val_mae: 452693.1562\n",
      "Epoch 205/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 556876169216.0000 - mae: 475803.625 - 0s 166us/sample - loss: 506515078603.0345 - mae: 394530.2500 - val_loss: 581295899000.6437 - val_mae: 421793.1875\n",
      "Epoch 206/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 91317714944.0000 - mae: 205291.01 - 0s 181us/sample - loss: 500258580032.7356 - mae: 399946.1250 - val_loss: 595464956210.0229 - val_mae: 438185.7812\n",
      "Epoch 207/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 336378003456.0000 - mae: 383673.187 - 0s 166us/sample - loss: 501207785001.1954 - mae: 379636.6875 - val_loss: 581465353439.6322 - val_mae: 423581.8750\n",
      "Epoch 208/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 428882395136.0000 - mae: 420690.062 - 0s 181us/sample - loss: 490263498234.1149 - mae: 394410.6562 - val_loss: 604050666554.8506 - val_mae: 446069.6562\n",
      "Epoch 209/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 410662502400.0000 - mae: 405539.750 - 0s 177us/sample - loss: 506767569225.5632 - mae: 388974.4375 - val_loss: 600442286032.9196 - val_mae: 439440.3750\n",
      "Epoch 210/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 321731624960.0000 - mae: 423271.312 - 0s 176us/sample - loss: 499844572548.4138 - mae: 391971.2188 - val_loss: 594249658697.5632 - val_mae: 429488.2812\n",
      "Epoch 211/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 223400394752.0000 - mae: 319000.718 - 0s 148us/sample - loss: 495687844028.3218 - mae: 386826.6562 - val_loss: 589448377214.5287 - val_mae: 432332.5625\n",
      "Epoch 212/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 296953741312.0000 - mae: 355876.000 - 0s 170us/sample - loss: 504920441961.9310 - mae: 409529.9688 - val_loss: 597055853156.0460 - val_mae: 432880.5000\n",
      "Epoch 213/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 607571410944.0000 - mae: 409511.000 - 0s 164us/sample - loss: 513649800250.8506 - mae: 390534.1562 - val_loss: 590071811013.1494 - val_mae: 422885.6875\n",
      "Epoch 214/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 448513835008.0000 - mae: 417451.343 - 0s 193us/sample - loss: 491808309059.6782 - mae: 394353.9375 - val_loss: 597492413498.8506 - val_mae: 443662.8125\n",
      "Epoch 215/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 559684517888.0000 - mae: 465859.500 - 0s 175us/sample - loss: 500585915780.4138 - mae: 385404.7188 - val_loss: 587179299616.3678 - val_mae: 424733.5625\n",
      "Epoch 216/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 860600139776.0000 - mae: 569356.062 - 0s 133us/sample - loss: 514869677562.1149 - mae: 411436.9062 - val_loss: 596956358773.7012 - val_mae: 431125.4688\n",
      "Epoch 217/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 314670317568.0000 - mae: 384778.625 - 0s 160us/sample - loss: 556417257519.0804 - mae: 400754.2500 - val_loss: 583339271179.7701 - val_mae: 418829.6875\n",
      "Epoch 218/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 487624605696.0000 - mae: 390055.437 - 0s 177us/sample - loss: 500695372953.0115 - mae: 421524.1875 - val_loss: 627121865268.9656 - val_mae: 463011.5938\n",
      "Epoch 219/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 552809791488.0000 - mae: 489043.250 - 0s 172us/sample - loss: 480035621264.1839 - mae: 385489.2500 - val_loss: 608974928790.0690 - val_mae: 430033.5625\n",
      "Epoch 220/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1563228897280.0000 - mae: 475571.31 - 0s 168us/sample - loss: 494501474445.2413 - mae: 385862.7188 - val_loss: 600505634486.4368 - val_mae: 426297.6562\n",
      "Epoch 221/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 170108747776.0000 - mae: 272560.750 - 0s 140us/sample - loss: 491399640499.4943 - mae: 387433.5938 - val_loss: 601526574633.1954 - val_mae: 435102.2500\n",
      "Epoch 222/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 324693917696.0000 - mae: 400807.437 - 0s 187us/sample - loss: 482280803645.7932 - mae: 384191.6875 - val_loss: 597505637835.0344 - val_mae: 436775.2500\n",
      "Epoch 223/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 369235755008.0000 - mae: 359595.125 - 0s 167us/sample - loss: 479873191041.4713 - mae: 388789.6562 - val_loss: 594635021488.5518 - val_mae: 431160.3125\n",
      "Epoch 224/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 638988976128.0000 - mae: 423634.343 - 0s 160us/sample - loss: 485604330060.5057 - mae: 380111.0938 - val_loss: 608611207944.8276 - val_mae: 441430.6562\n",
      "Epoch 225/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 729121751040.0000 - mae: 515254.625 - 0s 183us/sample - loss: 484289845130.2989 - mae: 393885.9375 - val_loss: 608132181215.6322 - val_mae: 438951.6875\n",
      "Epoch 226/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 666240679936.0000 - mae: 491458.937 - 0s 200us/sample - loss: 480103553847.9081 - mae: 384664.7500 - val_loss: 606299148711.7241 - val_mae: 430129.3438\n",
      "Epoch 227/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 244301430784.0000 - mae: 339904.843 - 0s 154us/sample - loss: 488525692174.7126 - mae: 384680.5938 - val_loss: 615124725147.9540 - val_mae: 447832.5938\n",
      "Epoch 228/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 275288981504.0000 - mae: 261155.390 - 0s 165us/sample - loss: 503408842069.3334 - mae: 396606.7188 - val_loss: 597754598553.0115 - val_mae: 422277.0625\n",
      "Epoch 229/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1873481433088.0000 - mae: 548043.75 - 0s 148us/sample - loss: 530430123184.5517 - mae: 422287.7812 - val_loss: 608708174859.7701 - val_mae: 428677.9375\n",
      "Epoch 230/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 462539915264.0000 - mae: 384116.875 - 0s 165us/sample - loss: 537864062622.8965 - mae: 394161.9688 - val_loss: 589163691467.0344 - val_mae: 418040.0000\n",
      "Epoch 231/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 413945298944.0000 - mae: 339071.718 - 0s 177us/sample - loss: 528570627507.4943 - mae: 437562.5625 - val_loss: 610319409999.4482 - val_mae: 441464.9062\n",
      "Epoch 232/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 477339385856.0000 - mae: 497924.093 - 0s 146us/sample - loss: 516825028113.6552 - mae: 386652.8750 - val_loss: 606561934889.1954 - val_mae: 430952.2812\n",
      "Epoch 233/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 133080432640.0000 - mae: 255639.875 - 0s 141us/sample - loss: 508657214287.4483 - mae: 418724.5938 - val_loss: 616085528105.1954 - val_mae: 437215.8125\n",
      "Epoch 234/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 314448216064.0000 - mae: 321512.343 - 0s 145us/sample - loss: 487654839637.3334 - mae: 380994.7500 - val_loss: 598174084884.5977 - val_mae: 429053.7812\n",
      "Epoch 235/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 141425541120.0000 - mae: 256772.453 - 0s 153us/sample - loss: 470525710759.7242 - mae: 381779.5938 - val_loss: 622142054305.8391 - val_mae: 454205.5938\n",
      "Epoch 236/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 620583714816.0000 - mae: 452731.656 - 0s 166us/sample - loss: 486514141866.6666 - mae: 392317.1562 - val_loss: 612658599629.9771 - val_mae: 427774.4688\n",
      "Epoch 237/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 273686495232.0000 - mae: 343399.812 - 0s 155us/sample - loss: 474039319069.4253 - mae: 376404.7500 - val_loss: 607014406732.5057 - val_mae: 436193.1562\n",
      "Epoch 238/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 452124934144.0000 - mae: 416103.281 - 0s 163us/sample - loss: 487266197221.5172 - mae: 403298.0625 - val_loss: 620184444645.5172 - val_mae: 438042.0312\n",
      "Epoch 239/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 175003205632.0000 - mae: 274351.437 - 0s 150us/sample - loss: 476108391906.5747 - mae: 378020.2188 - val_loss: 599427061077.3334 - val_mae: 430414.1250\n",
      "Epoch 240/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 555404754944.0000 - mae: 418793.437 - 0s 155us/sample - loss: 470402339616.3679 - mae: 381560.5938 - val_loss: 631063140045.9772 - val_mae: 454968.2812\n",
      "Epoch 241/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 605930455040.0000 - mae: 447904.406 - 0s 144us/sample - loss: 478172177608.0919 - mae: 386750.4688 - val_loss: 612328448094.1609 - val_mae: 435605.4375\n",
      "Epoch 242/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1558853582848.0000 - mae: 501326.12 - 0s 180us/sample - loss: 470151114328.2758 - mae: 386299.0000 - val_loss: 621477682729.1954 - val_mae: 440448.0000\n",
      "Epoch 243/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 367762472960.0000 - mae: 400807.000 - 0s 171us/sample - loss: 475827571276.5057 - mae: 376282.3438 - val_loss: 600529105037.2413 - val_mae: 428100.4688\n",
      "Epoch 244/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 136091451392.0000 - mae: 275743.843 - 0s 154us/sample - loss: 464767411588.4138 - mae: 381488.0625 - val_loss: 620873031774.1609 - val_mae: 442899.7812\n",
      "Epoch 245/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 278004695040.0000 - mae: 360487.718 - 0s 157us/sample - loss: 471050846584.6437 - mae: 379087.5938 - val_loss: 633008552854.0690 - val_mae: 443378.0312\n",
      "Epoch 246/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 169674031104.0000 - mae: 288446.375 - 0s 175us/sample - loss: 470132194386.3908 - mae: 393382.3125 - val_loss: 628130274668.8735 - val_mae: 444013.8438\n",
      "Epoch 247/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 303864479744.0000 - mae: 416860.187 - 0s 141us/sample - loss: 495059693520.9196 - mae: 385440.4375 - val_loss: 610574826578.3907 - val_mae: 433664.4062\n",
      "Epoch 248/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 416034717696.0000 - mae: 423936.718 - 0s 170us/sample - loss: 483273685721.7471 - mae: 412290.9062 - val_loss: 636972578415.8160 - val_mae: 454254.9375\n",
      "Epoch 249/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 559076737024.0000 - mae: 450186.375 - 0s 159us/sample - loss: 512233615312.9196 - mae: 387610.3438 - val_loss: 611952166182.2529 - val_mae: 429384.0938\n",
      "Epoch 250/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 161069301760.0000 - mae: 268874.718 - 0s 172us/sample - loss: 476497117560.6437 - mae: 404311.9062 - val_loss: 649535836772.0460 - val_mae: 467412.3125\n",
      "Epoch 251/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 367115370496.0000 - mae: 402556.375 - 0s 154us/sample - loss: 482203584912.1839 - mae: 378781.5312 - val_loss: 611746913644.8735 - val_mae: 428261.3750\n",
      "Epoch 252/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 712330182656.0000 - mae: 538974.375 - 0s 168us/sample - loss: 483875760434.0230 - mae: 398227.6250 - val_loss: 620940745527.9081 - val_mae: 440812.6562\n",
      "Epoch 253/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 219887648768.0000 - mae: 269231.531 - 0s 152us/sample - loss: 461758294604.5057 - mae: 374504.3125 - val_loss: 620958813313.4713 - val_mae: 434710.4375\n",
      "Epoch 254/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 275716538368.0000 - mae: 258224.390 - 0s 156us/sample - loss: 464199472845.9770 - mae: 376021.2188 - val_loss: 630622162614.4368 - val_mae: 453723.2500\n",
      "Epoch 255/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - ETA: 0s - loss: 304593371136.0000 - mae: 329797.062 - 0s 138us/sample - loss: 462839405273.7471 - mae: 385104.8125 - val_loss: 626299315211.7701 - val_mae: 435756.4688\n",
      "Epoch 256/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 371557335040.0000 - mae: 399481.375 - 0s 165us/sample - loss: 457686281439.6321 - mae: 372054.3750 - val_loss: 621275215848.4597 - val_mae: 438565.6562\n",
      "Epoch 257/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 361683681280.0000 - mae: 379599.937 - 0s 167us/sample - loss: 457747174929.6552 - mae: 372259.9688 - val_loss: 637746058393.0115 - val_mae: 451574.8125\n",
      "Epoch 258/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 589804601344.0000 - mae: 489815.375 - 0s 149us/sample - loss: 461808426913.8391 - mae: 396851.4062 - val_loss: 636606531230.8966 - val_mae: 440668.5938\n",
      "Epoch 259/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 206998290432.0000 - mae: 275565.312 - 0s 174us/sample - loss: 476701368037.5172 - mae: 376676.6562 - val_loss: 636105311902.8966 - val_mae: 446519.3438\n",
      "Epoch 260/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 144024551424.0000 - mae: 231195.859 - 0s 177us/sample - loss: 464993784796.6896 - mae: 391404.0000 - val_loss: 631254377107.1263 - val_mae: 443765.7812\n",
      "Epoch 261/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1461625552896.0000 - mae: 539049.31 - 0s 150us/sample - loss: 457176994027.4023 - mae: 374423.3750 - val_loss: 626242321302.0690 - val_mae: 434368.0938\n",
      "Epoch 262/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 568621268992.0000 - mae: 454507.156 - 0s 158us/sample - loss: 462601921736.0919 - mae: 390821.0625 - val_loss: 646227120740.0460 - val_mae: 461828.5938\n",
      "Epoch 263/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 532252819456.0000 - mae: 489033.718 - 0s 179us/sample - loss: 470779693644.5057 - mae: 381445.8750 - val_loss: 620547983006.8966 - val_mae: 433835.3125\n",
      "Epoch 264/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 400998006784.0000 - mae: 346035.781 - 0s 144us/sample - loss: 480462656476.6896 - mae: 392770.5312 - val_loss: 636268026679.9081 - val_mae: 437662.8438\n",
      "Epoch 265/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1566161895424.0000 - mae: 527368.06 - 0s 172us/sample - loss: 458016276291.6782 - mae: 373836.2188 - val_loss: 626109198477.2413 - val_mae: 433508.3750\n",
      "Epoch 266/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 407123918848.0000 - mae: 297418.468 - 0s 154us/sample - loss: 466802217007.0804 - mae: 401772.2812 - val_loss: 640838144047.0804 - val_mae: 450547.4375\n",
      "Epoch 267/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 165348392960.0000 - mae: 313390.312 - 0s 152us/sample - loss: 453347664154.4828 - mae: 374804.0312 - val_loss: 627916141626.8506 - val_mae: 434517.7812\n",
      "Epoch 268/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 198613991424.0000 - mae: 310372.437 - 0s 161us/sample - loss: 469822192722.3908 - mae: 387271.9062 - val_loss: 651445630764.1379 - val_mae: 454825.0000\n",
      "Epoch 269/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 287783452672.0000 - mae: 317166.687 - 0s 171us/sample - loss: 472253411280.9196 - mae: 375770.3438 - val_loss: 619289857906.7587 - val_mae: 435407.5938\n",
      "Epoch 270/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 64805445632.0000 - mae: 203012.12 - 0s 153us/sample - loss: 452909261082.4828 - mae: 382057.0000 - val_loss: 647382477859.3103 - val_mae: 448922.3438\n",
      "Epoch 271/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 502334881792.0000 - mae: 496449.125 - 0s 160us/sample - loss: 451408345064.4598 - mae: 373066.5625 - val_loss: 631710566046.8966 - val_mae: 439094.3125\n",
      "Epoch 272/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1476747067392.0000 - mae: 529283.25 - 0s 152us/sample - loss: 465741461480.4598 - mae: 393962.6562 - val_loss: 644017309154.5748 - val_mae: 440104.8750\n",
      "Epoch 273/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1312552517632.0000 - mae: 445998.34 - 0s 171us/sample - loss: 461554308625.6552 - mae: 378034.8125 - val_loss: 642613626526.8966 - val_mae: 445979.5000\n",
      "Epoch 274/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 239478407168.0000 - mae: 355422.750 - 0s 162us/sample - loss: 448022814249.1954 - mae: 386880.5938 - val_loss: 640192322924.8737 - val_mae: 445407.0000\n",
      "Epoch 275/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 230946242560.0000 - mae: 340673.937 - 0s 164us/sample - loss: 446897267535.4483 - mae: 372773.8750 - val_loss: 645694447145.1954 - val_mae: 441488.6875\n",
      "Epoch 276/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 168957018112.0000 - mae: 299173.437 - 0s 160us/sample - loss: 457186920848.1839 - mae: 373150.1250 - val_loss: 655089339709.7931 - val_mae: 459269.5312\n",
      "Epoch 277/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 394185670656.0000 - mae: 401212.750 - 0s 146us/sample - loss: 449428430188.8736 - mae: 387368.6875 - val_loss: 641139344113.2874 - val_mae: 443029.5938\n",
      "Epoch 278/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 327956037632.0000 - mae: 403352.125 - 0s 157us/sample - loss: 443375957239.1724 - mae: 372559.1875 - val_loss: 647485801012.9656 - val_mae: 443649.1875\n",
      "Epoch 279/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 484234362880.0000 - mae: 428868.562 - 0s 143us/sample - loss: 439333218798.3448 - mae: 368203.8438 - val_loss: 655773689220.4138 - val_mae: 454844.7812\n",
      "Epoch 280/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 276729888768.0000 - mae: 348460.250 - 0s 153us/sample - loss: 447039952213.3334 - mae: 386020.4688 - val_loss: 674379386362.1150 - val_mae: 460122.7188\n",
      "Epoch 281/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 177319116800.0000 - mae: 276214.625 - 0s 149us/sample - loss: 436751727886.7126 - mae: 372590.1562 - val_loss: 648184748102.6207 - val_mae: 443773.2500\n",
      "Epoch 282/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 200001716224.0000 - mae: 261581.093 - 0s 152us/sample - loss: 439649318888.4598 - mae: 371808.5938 - val_loss: 648876343484.3219 - val_mae: 447881.8750\n",
      "Epoch 283/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1491218202624.0000 - mae: 575497.50 - 0s 149us/sample - loss: 440592461776.9196 - mae: 380660.5938 - val_loss: 655893417089.4713 - val_mae: 444248.8125\n",
      "Epoch 284/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 228932419584.0000 - mae: 305432.937 - 0s 170us/sample - loss: 448730701541.5172 - mae: 374287.8125 - val_loss: 670709926288.1840 - val_mae: 455081.5938\n",
      "Epoch 285/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 842176659456.0000 - mae: 559520.125 - 0s 176us/sample - loss: 430506767418.8506 - mae: 369108.3750 - val_loss: 650101192492.1379 - val_mae: 441577.5938\n",
      "Epoch 286/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 197272764416.0000 - mae: 283868.593 - 0s 146us/sample - loss: 437248000000.0000 - mae: 367674.1562 - val_loss: 656118487298.9425 - val_mae: 444281.5312\n",
      "Epoch 287/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 301828276224.0000 - mae: 315359.687 - 0s 177us/sample - loss: 437066425732.4138 - mae: 379548.5000 - val_loss: 658106389657.0115 - val_mae: 458172.3750\n",
      "Epoch 288/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 267862441984.0000 - mae: 324086.718 - 0s 158us/sample - loss: 432917354001.6552 - mae: 371560.0938 - val_loss: 667267559376.9196 - val_mae: 445172.2188\n",
      "Epoch 289/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 497385275392.0000 - mae: 409891.000 - 0s 156us/sample - loss: 438098595887.0804 - mae: 375003.4062 - val_loss: 654720501595.2184 - val_mae: 447213.2500\n",
      "Epoch 290/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 376582012928.0000 - mae: 352455.562 - 0s 153us/sample - loss: 439512197990.9885 - mae: 380525.5312 - val_loss: 668202281266.0228 - val_mae: 453178.3125\n",
      "Epoch 291/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 306856329216.0000 - mae: 342324.187 - 0s 171us/sample - loss: 436888479673.3793 - mae: 371002.0312 - val_loss: 677810255471.8160 - val_mae: 451122.0625\n",
      "Epoch 292/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 429886537728.0000 - mae: 428157.875 - 0s 160us/sample - loss: 430998600786.3908 - mae: 368886.6562 - val_loss: 655809177341.0575 - val_mae: 452514.3438\n",
      "Epoch 293/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 209245405184.0000 - mae: 307129.125 - 0s 167us/sample - loss: 429422892749.9770 - mae: 374114.5312 - val_loss: 675385469045.7012 - val_mae: 458967.9688\n",
      "Epoch 294/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 412717776896.0000 - mae: 427285.937 - 0s 144us/sample - loss: 427721352203.7701 - mae: 370387.2188 - val_loss: 690285202561.4713 - val_mae: 454840.2188\n",
      "Epoch 295/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 119347945472.0000 - mae: 217375.625 - 0s 173us/sample - loss: 437047666240.7356 - mae: 372775.4375 - val_loss: 670802958406.6207 - val_mae: 444264.5938\n",
      "Epoch 296/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 204331122688.0000 - mae: 345800.437 - 0s 168us/sample - loss: 436528311142.9885 - mae: 372329.7812 - val_loss: 655395105074.0228 - val_mae: 444508.8125\n",
      "Epoch 297/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 288385957888.0000 - mae: 327204.875 - 0s 161us/sample - loss: 434142530689.4713 - mae: 370908.0312 - val_loss: 685909935209.9310 - val_mae: 465233.9375\n",
      "Epoch 298/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1582316781568.0000 - mae: 659938.56 - 0s 144us/sample - loss: 432846894892.1379 - mae: 375415.5312 - val_loss: 654607261413.5172 - val_mae: 438830.4688\n",
      "Epoch 299/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 74193502208.0000 - mae: 203479.37 - 0s 136us/sample - loss: 449510385122.5747 - mae: 392040.7500 - val_loss: 677848224544.3678 - val_mae: 456621.3438\n",
      "Epoch 300/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 150189539328.0000 - mae: 309717.187 - 0s 146us/sample - loss: 457483962391.5402 - mae: 377515.1562 - val_loss: 655444775335.7240 - val_mae: 448001.5625\n",
      "Epoch 301/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 199466762240.0000 - mae: 308655.250 - 0s 124us/sample - loss: 467224184478.8965 - mae: 425925.2500 - val_loss: 677483227065.3793 - val_mae: 443226.4375\n",
      "Epoch 302/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 554559733760.0000 - mae: 476267.312 - 0s 171us/sample - loss: 444938868053.3334 - mae: 365361.0938 - val_loss: 663098572988.3219 - val_mae: 452283.1250\n",
      "Epoch 303/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 410963935232.0000 - mae: 352579.812 - 0s 163us/sample - loss: 442563676936.8276 - mae: 407642.0312 - val_loss: 688619906059.7701 - val_mae: 455528.2188\n",
      "Epoch 304/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 244346322944.0000 - mae: 342170.531 - 0s 152us/sample - loss: 441580105869.2413 - mae: 374513.9062 - val_loss: 672531081710.3448 - val_mae: 449012.2812\n",
      "Epoch 305/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 339265257472.0000 - mae: 364532.437 - 0s 164us/sample - loss: 432117083924.5977 - mae: 390306.5625 - val_loss: 676070685213.4252 - val_mae: 452912.5938\n",
      "Epoch 306/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 111284076544.0000 - mae: 228951.687 - 0s 144us/sample - loss: 431803041250.5747 - mae: 366013.1250 - val_loss: 677220811128.6437 - val_mae: 454087.5000\n",
      "Epoch 307/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1176340529152.0000 - mae: 447015.25 - 0s 157us/sample - loss: 431916708546.2068 - mae: 387227.1875 - val_loss: 659672052229.8850 - val_mae: 442337.7812\n",
      "Epoch 308/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 478962614272.0000 - mae: 449632.562 - 0s 138us/sample - loss: 417698546982.2529 - mae: 360414.4062 - val_loss: 666088901573.1494 - val_mae: 447241.0625\n",
      "Epoch 309/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 353371815936.0000 - mae: 374935.718 - 0s 174us/sample - loss: 420917835940.7816 - mae: 362576.8750 - val_loss: 688668269367.9081 - val_mae: 454657.0625\n",
      "Epoch 310/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 439901847552.0000 - mae: 429430.125 - 0s 142us/sample - loss: 424434164465.2874 - mae: 387774.8438 - val_loss: 678300976610.5748 - val_mae: 457581.4688\n",
      "Epoch 311/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 145981276160.0000 - mae: 271020.562 - 0s 150us/sample - loss: 436364406171.9540 - mae: 373366.1875 - val_loss: 668903007243.7701 - val_mae: 448737.1562\n",
      "Epoch 312/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 57940512768.0000 - mae: 188036.87 - 0s 162us/sample - loss: 427776257059.3104 - mae: 369705.5312 - val_loss: 703422682829.9772 - val_mae: 463321.0000\n",
      "Epoch 313/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 227250634752.0000 - mae: 270484.906 - 0s 164us/sample - loss: 407363980605.7932 - mae: 358507.9062 - val_loss: 670627895178.2988 - val_mae: 452591.7812\n",
      "Epoch 314/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1274850967552.0000 - mae: 474562.71 - 0s 148us/sample - loss: 410434013019.2184 - mae: 369557.5000 - val_loss: 694147543746.2069 - val_mae: 458562.2188\n",
      "Epoch 315/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 344270274560.0000 - mae: 407505.187 - 0s 157us/sample - loss: 406272759031.1724 - mae: 362481.7500 - val_loss: 690572315777.4713 - val_mae: 464051.6250\n",
      "Epoch 316/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 776906735616.0000 - mae: 626458.750 - 0s 159us/sample - loss: 415252500338.7587 - mae: 370667.7812 - val_loss: 711600073951.6322 - val_mae: 469932.4062\n",
      "Epoch 317/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 302159036416.0000 - mae: 343762.406 - 0s 146us/sample - loss: 394908840830.5287 - mae: 365016.5625 - val_loss: 695771233550.7126 - val_mae: 452515.6250\n",
      "Epoch 318/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 317119725568.0000 - mae: 254118.578 - 0s 159us/sample - loss: 412685478382.3448 - mae: 364305.1875 - val_loss: 695440404432.9196 - val_mae: 466707.9062\n",
      "Epoch 319/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 176094380032.0000 - mae: 275836.281 - 0s 151us/sample - loss: 400594158132.9655 - mae: 369741.1875 - val_loss: 700438666416.5518 - val_mae: 454978.5312\n",
      "Epoch 320/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 378282541056.0000 - mae: 390600.187 - 0s 149us/sample - loss: 406327300143.0804 - mae: 367791.0312 - val_loss: 730443966581.7012 - val_mae: 473290.4688\n",
      "Epoch 321/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 218377650176.0000 - mae: 282016.312 - 0s 149us/sample - loss: 417494207994.1149 - mae: 360616.1250 - val_loss: 672369810043.5862 - val_mae: 456062.9062\n",
      "Epoch 322/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 87841505280.0000 - mae: 206269.90 - 0s 138us/sample - loss: 411006821387.7701 - mae: 388960.1875 - val_loss: 696296443197.7931 - val_mae: 459781.1875\n",
      "Epoch 323/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 187249295360.0000 - mae: 289146.906 - 0s 149us/sample - loss: 399916699506.7587 - mae: 358020.4062 - val_loss: 714023813308.3219 - val_mae: 463431.0938\n",
      "Epoch 324/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 1102121009152.0000 - mae: 486978.68 - 0s 162us/sample - loss: 400330468952.2758 - mae: 375615.4375 - val_loss: 699073302951.7240 - val_mae: 452597.7500\n",
      "Epoch 325/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 279636344832.0000 - mae: 304820.156 - 0s 144us/sample - loss: 409078584661.3334 - mae: 360592.8125 - val_loss: 700518128063.2644 - val_mae: 477847.3125\n",
      "Epoch 326/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 475902181376.0000 - mae: 420825.687 - 0s 167us/sample - loss: 393554969329.2874 - mae: 372178.8125 - val_loss: 712894577558.0690 - val_mae: 462531.8750\n",
      "Epoch 327/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 400432234496.0000 - mae: 429780.187 - 0s 170us/sample - loss: 389692964675.6782 - mae: 354579.2188 - val_loss: 743521439979.4023 - val_mae: 472120.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 567953326080.0000 - mae: 455759.375 - 0s 147us/sample - loss: 394880134932.5977 - mae: 374996.9688 - val_loss: 719245709359.0804 - val_mae: 457565.2812\n",
      "Epoch 329/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 173579042816.0000 - mae: 279606.625 - 0s 145us/sample - loss: 399432801833.1954 - mae: 358421.0625 - val_loss: 732823782882.5748 - val_mae: 483618.5312\n",
      "Epoch 330/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 431487385600.0000 - mae: 425750.375 - 0s 169us/sample - loss: 399829586155.4023 - mae: 364321.5000 - val_loss: 731392761385.1954 - val_mae: 469575.6250\n",
      "Epoch 331/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 744793243648.0000 - mae: 577543.125 - 0s 150us/sample - loss: 383686149696.7356 - mae: 370618.7188 - val_loss: 719986462767.0804 - val_mae: 462335.1875\n",
      "Epoch 332/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 73820143616.0000 - mae: 206709.01 - 0s 152us/sample - loss: 377664055602.0230 - mae: 350979.7812 - val_loss: 715829418242.9425 - val_mae: 470984.5625\n",
      "Epoch 333/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 154083278848.0000 - mae: 282888.375 - 0s 163us/sample - loss: 380035510601.5632 - mae: 367769.0938 - val_loss: 737567197960.8275 - val_mae: 463113.8750\n",
      "Epoch 334/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 381174349824.0000 - mae: 443318.687 - 0s 166us/sample - loss: 372198088986.4827 - mae: 351259.5938 - val_loss: 753902749907.8621 - val_mae: 481598.6562\n",
      "Epoch 335/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 816321396736.0000 - mae: 411454.437 - 0s 154us/sample - loss: 374681103960.2758 - mae: 368110.8125 - val_loss: 739147179019.7701 - val_mae: 467833.6875\n",
      "Epoch 336/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 365883949056.0000 - mae: 385909.562 - 0s 167us/sample - loss: 378117666415.8161 - mae: 349240.0312 - val_loss: 705769291587.6781 - val_mae: 464076.3125\n",
      "Epoch 337/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 317103472640.0000 - mae: 376084.875 - 0s 152us/sample - loss: 394574032095.6321 - mae: 399877.6562 - val_loss: 790898413991.7240 - val_mae: 477786.0625\n",
      "Epoch 338/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 203900043264.0000 - mae: 302344.812 - 0s 168us/sample - loss: 399118137885.4253 - mae: 366546.8125 - val_loss: 790170590007.9081 - val_mae: 503776.8750\n",
      "Epoch 339/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 290593243136.0000 - mae: 383590.562 - 0s 172us/sample - loss: 365210356488.8276 - mae: 377145.3750 - val_loss: 736448349666.5748 - val_mae: 463099.5312\n",
      "Epoch 340/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 255737790464.0000 - mae: 309055.875 - 0s 158us/sample - loss: 378720304963.6782 - mae: 349751.0000 - val_loss: 772266559699.8621 - val_mae: 498947.1250\n",
      "Epoch 341/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 451184984064.0000 - mae: 472944.250 - 0s 167us/sample - loss: 374202405664.3678 - mae: 362819.1875 - val_loss: 741052163648.7356 - val_mae: 478624.5938\n",
      "Epoch 342/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 243868139520.0000 - mae: 350982.718 - 0s 178us/sample - loss: 362817332047.4482 - mae: 358928.1562 - val_loss: 738804377152.7356 - val_mae: 466097.2812\n",
      "Epoch 343/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 593390206976.0000 - mae: 491785.031 - 0s 174us/sample - loss: 352485242291.4943 - mae: 361994.0625 - val_loss: 795556191973.5172 - val_mae: 486666.9062\n",
      "Epoch 344/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 444573548544.0000 - mae: 461691.562 - 0s 195us/sample - loss: 348430090428.3218 - mae: 346864.5625 - val_loss: 790067147834.8506 - val_mae: 481272.6562\n",
      "Epoch 345/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 190254350336.0000 - mae: 271573.750 - 0s 171us/sample - loss: 334871771394.9425 - mae: 348695.0625 - val_loss: 754489033833.9310 - val_mae: 475109.0938\n",
      "Epoch 346/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 322495184896.0000 - mae: 359148.750 - 0s 162us/sample - loss: 353918461881.3793 - mae: 361459.6562 - val_loss: 767722057304.2760 - val_mae: 477979.7812\n",
      "Epoch 347/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 940194988032.0000 - mae: 468992.500 - 0s 163us/sample - loss: 356586729236.5977 - mae: 351503.2500 - val_loss: 832107557381.8850 - val_mae: 491915.2188\n",
      "Epoch 348/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 130639978496.0000 - mae: 281526.968 - 0s 163us/sample - loss: 340771192078.7126 - mae: 349876.3750 - val_loss: 765198381573.8850 - val_mae: 480221.8750\n",
      "Epoch 349/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 220472016896.0000 - mae: 338674.937 - 0s 159us/sample - loss: 326928227504.5518 - mae: 338300.0625 - val_loss: 806063751732.9656 - val_mae: 491885.5625\n",
      "Epoch 350/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 662667984896.0000 - mae: 340779.312 - 0s 165us/sample - loss: 343485771728.9196 - mae: 349134.1250 - val_loss: 832618934413.2413 - val_mae: 484424.3750\n",
      "Epoch 351/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 141676314624.0000 - mae: 265468.562 - 0s 158us/sample - loss: 322029955001.3793 - mae: 340539.0000 - val_loss: 881748942942.1609 - val_mae: 543206.5625\n",
      "Epoch 352/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 675291791360.0000 - mae: 480969.125 - 0s 149us/sample - loss: 388225385766.2529 - mae: 398742.4688 - val_loss: 871244550756.0460 - val_mae: 510685.5625\n",
      "Epoch 353/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 129460756480.0000 - mae: 248567.718 - 0s 149us/sample - loss: 355096639087.8161 - mae: 353313.6875 - val_loss: 899338128513.4713 - val_mae: 540749.6875\n",
      "Epoch 354/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 368776085504.0000 - mae: 389477.406 - 0s 173us/sample - loss: 339627268166.6207 - mae: 359046.2188 - val_loss: 776557821575.3563 - val_mae: 479626.9375\n",
      "Epoch 355/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 211636289536.0000 - mae: 327714.343 - 0s 166us/sample - loss: 317339815794.7586 - mae: 341379.8750 - val_loss: 803815104229.5172 - val_mae: 496141.6875\n",
      "Epoch 356/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 510738104320.0000 - mae: 474160.937 - 0s 190us/sample - loss: 315110348211.4943 - mae: 340783.2500 - val_loss: 893147846538.2988 - val_mae: 504489.8438\n",
      "Epoch 357/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 292298883072.0000 - mae: 353386.500 - 0s 171us/sample - loss: 324120420822.8046 - mae: 335153.3750 - val_loss: 831354752129.4713 - val_mae: 505210.8125\n",
      "Epoch 358/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 605497327616.0000 - mae: 405681.718 - 0s 146us/sample - loss: 325743227397.8851 - mae: 363044.8750 - val_loss: 869273336726.0690 - val_mae: 498479.6875\n",
      "Epoch 359/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 47195906048.0000 - mae: 162905.31 - 0s 167us/sample - loss: 339215534480.1839 - mae: 354574.5625 - val_loss: 835778223845.5172 - val_mae: 499048.7500\n",
      "Epoch 360/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 153690030080.0000 - mae: 235473.921 - 0s 180us/sample - loss: 315328776592.1839 - mae: 341349.7500 - val_loss: 870975821517.9772 - val_mae: 513684.2812\n",
      "Epoch 361/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 146502778880.0000 - mae: 283294.500 - 0s 131us/sample - loss: 303785477414.2529 - mae: 339348.5625 - val_loss: 850545919646.8966 - val_mae: 500357.7500\n",
      "Epoch 362/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 234861658112.0000 - mae: 291741.750 - 0s 169us/sample - loss: 301455250020.0460 - mae: 331683.9062 - val_loss: 838079143465.1954 - val_mae: 488577.7812\n",
      "Epoch 363/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 203996495872.0000 - mae: 308183.031 - 0s 167us/sample - loss: 290841145485.2414 - mae: 336171.4062 - val_loss: 839029584954.8506 - val_mae: 501943.7188\n",
      "Epoch 364/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 100320518144.0000 - mae: 229537.531 - 0s 162us/sample - loss: 287480267128.6437 - mae: 323072.2188 - val_loss: 917549103080.4597 - val_mae: 534351.5000\n",
      "Epoch 365/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 244986150912.0000 - mae: 329705.750 - 0s 160us/sample - loss: 287238943049.5632 - mae: 345700.5000 - val_loss: 923470856192.0000 - val_mae: 514317.1875\n",
      "Epoch 366/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 465779032064.0000 - mae: 404546.718 - 0s 175us/sample - loss: 296611439816.0920 - mae: 345567.6250 - val_loss: 955194736640.0000 - val_mae: 564378.8750\n",
      "Epoch 367/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 292458528768.0000 - mae: 336070.000 - 0s 153us/sample - loss: 352946116301.9770 - mae: 359483.7188 - val_loss: 846283429205.3334 - val_mae: 496863.7812\n",
      "Epoch 368/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 405228781568.0000 - mae: 372407.875 - 0s 159us/sample - loss: 337847827161.7471 - mae: 387355.5938 - val_loss: 1013068879189.3334 - val_mae: 542648.0000\n",
      "Epoch 369/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 169319923712.0000 - mae: 289055.500 - 0s 138us/sample - loss: 423410469829.1494 - mae: 406586.4062 - val_loss: 1071059586930.7587 - val_mae: 621720.9375\n",
      "Epoch 370/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 545080868864.0000 - mae: 408726.250 - 0s 163us/sample - loss: 337021217074.0230 - mae: 368213.4375 - val_loss: 879890943858.7587 - val_mae: 510091.0000\n",
      "Epoch 371/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 900586602496.0000 - mae: 495516.062 - 0s 173us/sample - loss: 317871865102.7126 - mae: 349277.3750 - val_loss: 968000361060.0460 - val_mae: 546937.5000\n",
      "Epoch 372/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 194632466432.0000 - mae: 299468.406 - 0s 146us/sample - loss: 271406604382.1609 - mae: 328003.1562 - val_loss: 880281268741.8850 - val_mae: 499913.0000\n",
      "Epoch 373/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 130068774912.0000 - mae: 262598.875 - 0s 153us/sample - loss: 262736060510.1609 - mae: 321893.3750 - val_loss: 894052987209.5632 - val_mae: 523628.4062\n",
      "Epoch 374/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 108539166720.0000 - mae: 259945.984 - 0s 164us/sample - loss: 260203079986.0230 - mae: 322392.5000 - val_loss: 896201013224.4597 - val_mae: 503081.9375\n",
      "Epoch 375/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 197410111488.0000 - mae: 305288.625 - 0s 190us/sample - loss: 253068784133.8851 - mae: 314242.6875 - val_loss: 924266111446.8046 - val_mae: 516415.4062\n",
      "Epoch 376/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 208437903360.0000 - mae: 306679.750 - 0s 151us/sample - loss: 250899964563.1265 - mae: 319643.8750 - val_loss: 954846577016.6437 - val_mae: 519084.0312\n",
      "Epoch 377/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 197172969472.0000 - mae: 299368.187 - 0s 168us/sample - loss: 255961736957.0575 - mae: 317399.9062 - val_loss: 927684451127.9081 - val_mae: 525607.5625\n",
      "Epoch 378/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 242162728960.0000 - mae: 354001.312 - 0s 168us/sample - loss: 249008067925.3333 - mae: 317843.0625 - val_loss: 966839295905.8391 - val_mae: 520608.2188\n",
      "Epoch 379/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 186709344256.0000 - mae: 294454.625 - 0s 163us/sample - loss: 263635839846.9885 - mae: 330243.4688 - val_loss: 961232906546.0228 - val_mae: 537765.3125\n",
      "Epoch 380/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 247749804032.0000 - mae: 362022.125 - 0s 146us/sample - loss: 256376440949.7011 - mae: 312852.2812 - val_loss: 928584512005.8850 - val_mae: 532038.5625\n",
      "Epoch 381/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 155913781248.0000 - mae: 307240.437 - 0s 164us/sample - loss: 241277783169.4713 - mae: 319678.4375 - val_loss: 1019610029609.1954 - val_mae: 537156.3125\n",
      "Epoch 382/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 147871088640.0000 - mae: 284371.687 - 0s 148us/sample - loss: 282112417321.1954 - mae: 339351.0312 - val_loss: 1054016804746.2988 - val_mae: 590909.1250\n",
      "Epoch 383/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 349225091072.0000 - mae: 498788.468 - 0s 139us/sample - loss: 271735523445.7011 - mae: 331027.3750 - val_loss: 927479366385.2874 - val_mae: 523607.4375\n",
      "Epoch 384/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 158374756352.0000 - mae: 265960.875 - 0s 154us/sample - loss: 225463578294.4368 - mae: 313220.5938 - val_loss: 1013421406372.7816 - val_mae: 527766.3750\n",
      "Epoch 385/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 278393356288.0000 - mae: 320629.375 - 0s 172us/sample - loss: 233334525622.4368 - mae: 310681.8750 - val_loss: 1057597221734.9885 - val_mae: 554653.3750\n",
      "Epoch 386/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 278107062272.0000 - mae: 372360.875 - 0s 155us/sample - loss: 216404042575.4483 - mae: 308704.1562 - val_loss: 983539965104.5518 - val_mae: 526140.4375\n",
      "Epoch 387/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 206094106624.0000 - mae: 278001.250 - 0s 171us/sample - loss: 243169760220.6897 - mae: 321575.6250 - val_loss: 977634173398.8046 - val_mae: 524705.6875\n",
      "Epoch 388/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 317302177792.0000 - mae: 395220.937 - 0s 149us/sample - loss: 255943822653.7931 - mae: 326384.3750 - val_loss: 1147976763792.1838 - val_mae: 622752.4375\n",
      "Epoch 389/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 278253305856.0000 - mae: 397853.562 - 0s 159us/sample - loss: 270268177207.9081 - mae: 347371.5938 - val_loss: 1058157788054.0690 - val_mae: 540075.0625\n",
      "Epoch 390/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 49102606336.0000 - mae: 161738.87 - 0s 174us/sample - loss: 227170440909.9770 - mae: 313354.2500 - val_loss: 937659074018.5748 - val_mae: 522219.9688\n",
      "Epoch 391/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 212072660992.0000 - mae: 329492.156 - 0s 136us/sample - loss: 207028378235.5862 - mae: 293782.4688 - val_loss: 1014128082567.3563 - val_mae: 546180.3125\n",
      "Epoch 392/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 105943318528.0000 - mae: 244590.328 - 0s 159us/sample - loss: 209709645635.6782 - mae: 305957.4688 - val_loss: 1045472278316.1379 - val_mae: 539112.1875\n",
      "Epoch 393/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 132826472448.0000 - mae: 263357.937 - 0s 163us/sample - loss: 247140795309.6092 - mae: 323334.8438 - val_loss: 1010858117143.5403 - val_mae: 556520.1875\n",
      "Epoch 394/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 291930112000.0000 - mae: 387343.468 - 0s 153us/sample - loss: 239253641098.2989 - mae: 315900.4688 - val_loss: 1047469080387.6781 - val_mae: 556914.8750\n",
      "Epoch 395/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 262239944704.0000 - mae: 324671.781 - 0s 183us/sample - loss: 232153375755.7701 - mae: 308747.5625 - val_loss: 976122530568.8275 - val_mae: 528581.4375\n",
      "Epoch 396/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 230142853120.0000 - mae: 290132.031 - 0s 152us/sample - loss: 201190788978.7586 - mae: 296383.8125 - val_loss: 1080535861271.5403 - val_mae: 544703.6250\n",
      "Epoch 397/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 229365252096.0000 - mae: 279959.968 - 0s 172us/sample - loss: 197490668226.2069 - mae: 294070.4062 - val_loss: 1032569936319.2644 - val_mae: 537577.1250\n",
      "Epoch 398/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 48887480320.0000 - mae: 161846.84 - 0s 145us/sample - loss: 198484910739.1265 - mae: 289423.0625 - val_loss: 1052153728506.1150 - val_mae: 550861.3125\n",
      "Epoch 399/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 85786992640.0000 - mae: 216854.21 - 0s 163us/sample - loss: 187245520260.4138 - mae: 284903.1875 - val_loss: 1088001549135.4482 - val_mae: 565908.3125\n",
      "Epoch 400/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - ETA: 0s - loss: 106103119872.0000 - mae: 231508.828 - 0s 158us/sample - loss: 186961302116.0460 - mae: 293470.4062 - val_loss: 1088377362690.9425 - val_mae: 549234.5625\n",
      "Epoch 401/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 238264614912.0000 - mae: 304171.875 - 0s 162us/sample - loss: 192226125000.0919 - mae: 290016.3125 - val_loss: 1064152206277.1494 - val_mae: 554868.4375\n",
      "Epoch 402/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 149830107136.0000 - mae: 259888.484 - 0s 145us/sample - loss: 181940899133.7931 - mae: 281197.7500 - val_loss: 1101449624470.0688 - val_mae: 554752.0000\n",
      "Epoch 403/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 79185895424.0000 - mae: 210359.95 - 0s 158us/sample - loss: 195548770727.7242 - mae: 287908.2188 - val_loss: 1094555713347.6781 - val_mae: 551205.3125\n",
      "Epoch 404/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 229792481280.0000 - mae: 265105.843 - 0s 159us/sample - loss: 221958804256.3678 - mae: 309312.7500 - val_loss: 1045086263743.2644 - val_mae: 563974.0625\n",
      "Epoch 405/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 119907999744.0000 - mae: 257607.687 - 0s 167us/sample - loss: 185988628668.3218 - mae: 280891.8750 - val_loss: 1120722324797.7932 - val_mae: 565528.0000\n",
      "Epoch 406/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 150446604288.0000 - mae: 283937.312 - 0s 157us/sample - loss: 180850552219.9540 - mae: 281560.2188 - val_loss: 1122800099987.1265 - val_mae: 570810.3125\n",
      "Epoch 407/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 226253078528.0000 - mae: 317483.250 - 0s 161us/sample - loss: 203042566661.8851 - mae: 292148.8438 - val_loss: 1212486262124.8735 - val_mae: 607315.3750\n",
      "Epoch 408/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 289336164352.0000 - mae: 361587.437 - 0s 149us/sample - loss: 198208887325.4253 - mae: 301694.0312 - val_loss: 1083380400881.2874 - val_mae: 555336.1875\n",
      "Epoch 409/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 194557689856.0000 - mae: 313112.156 - 0s 162us/sample - loss: 168659512731.9540 - mae: 278293.8750 - val_loss: 1133485981319.3562 - val_mae: 562240.8125\n",
      "Epoch 410/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 117142642688.0000 - mae: 235214.328 - 0s 157us/sample - loss: 172234820172.5057 - mae: 283789.2500 - val_loss: 1110832068584.4597 - val_mae: 559406.5000\n",
      "Epoch 411/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 133025652736.0000 - mae: 227213.843 - 0s 169us/sample - loss: 187808096726.8046 - mae: 290482.3125 - val_loss: 1205872426254.7126 - val_mae: 603402.0000\n",
      "Epoch 412/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 122822606848.0000 - mae: 255037.656 - 0s 149us/sample - loss: 168946274916.0460 - mae: 281523.4688 - val_loss: 1088792017249.1034 - val_mae: 554152.1875\n",
      "Epoch 413/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 141222232064.0000 - mae: 244878.593 - 0s 169us/sample - loss: 195826964550.6207 - mae: 291620.3438 - val_loss: 1089257807212.8737 - val_mae: 548253.4375\n",
      "Epoch 414/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 98058444800.0000 - mae: 220288.21 - 0s 162us/sample - loss: 170856116365.2414 - mae: 276512.5000 - val_loss: 1121650098364.3218 - val_mae: 562669.9375\n",
      "Epoch 415/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 63104315392.0000 - mae: 197641.87 - 0s 169us/sample - loss: 166778479251.1264 - mae: 269078.4062 - val_loss: 1102363255066.4827 - val_mae: 560305.4375\n",
      "Epoch 416/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 134052012032.0000 - mae: 258769.203 - 0s 163us/sample - loss: 183598660125.4253 - mae: 297662.8438 - val_loss: 1212763905012.2300 - val_mae: 600533.6250\n",
      "Epoch 417/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 248442535936.0000 - mae: 346748.250 - 0s 178us/sample - loss: 193507655115.0345 - mae: 303493.8750 - val_loss: 1151435954246.6206 - val_mae: 584289.7500\n",
      "Epoch 418/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 122659209216.0000 - mae: 272185.593 - 0s 152us/sample - loss: 155815211149.2414 - mae: 267810.8125 - val_loss: 1161050408606.8965 - val_mae: 570301.3125\n",
      "Epoch 419/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 95559008256.0000 - mae: 226258.53 - 0s 149us/sample - loss: 148718675520.7356 - mae: 262388.1875 - val_loss: 1099159738626.9425 - val_mae: 558440.9375\n",
      "Epoch 420/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 217521520640.0000 - mae: 318493.125 - 0s 154us/sample - loss: 147254598455.9081 - mae: 259694.1562 - val_loss: 1156883806749.4253 - val_mae: 569016.6250\n",
      "Epoch 421/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 109910425600.0000 - mae: 232606.781 - 0s 142us/sample - loss: 151537401291.0345 - mae: 260912.1875 - val_loss: 1197014172212.9656 - val_mae: 586962.0000\n",
      "Epoch 422/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 151232086016.0000 - mae: 259504.562 - 0s 172us/sample - loss: 157186880005.8851 - mae: 261872.5781 - val_loss: 1108079674850.5747 - val_mae: 564423.8125\n",
      "Epoch 423/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 127956779008.0000 - mae: 237220.859 - 0s 139us/sample - loss: 134773881655.9080 - mae: 250612.1562 - val_loss: 1171004585018.8506 - val_mae: 566716.9375\n",
      "Epoch 424/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 133611732992.0000 - mae: 246825.375 - 0s 177us/sample - loss: 133660881978.8506 - mae: 249161.4531 - val_loss: 1160892999421.0574 - val_mae: 567001.5000\n",
      "Epoch 425/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 170550919168.0000 - mae: 309639.812 - 0s 183us/sample - loss: 145057899343.4483 - mae: 263440.0312 - val_loss: 1149491359508.5977 - val_mae: 565340.1250\n",
      "Epoch 426/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 218035716096.0000 - mae: 323739.187 - 0s 166us/sample - loss: 136886820675.6781 - mae: 248664.2344 - val_loss: 1197629267497.1953 - val_mae: 579379.4375\n",
      "Epoch 427/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 173990772736.0000 - mae: 299431.062 - 0s 134us/sample - loss: 131194915781.1494 - mae: 246738.4531 - val_loss: 1163325281433.0115 - val_mae: 564649.8750\n",
      "Epoch 428/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 147262570496.0000 - mae: 304217.937 - 0s 137us/sample - loss: 133533807015.7241 - mae: 251234.8906 - val_loss: 1258937969510.9885 - val_mae: 605435.8125\n",
      "Epoch 429/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 132584677376.0000 - mae: 265924.656 - 0s 159us/sample - loss: 138431357775.4483 - mae: 257786.2812 - val_loss: 1135703725585.6553 - val_mae: 565217.9375\n",
      "Epoch 430/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 62250663936.0000 - mae: 196695.23 - 0s 154us/sample - loss: 131875756385.1035 - mae: 249359.7969 - val_loss: 1246005486980.4138 - val_mae: 585780.0625\n",
      "Epoch 431/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 190585831424.0000 - mae: 253044.718 - 0s 158us/sample - loss: 124966574433.1035 - mae: 239824.6719 - val_loss: 1240726236760.2759 - val_mae: 598656.8125\n",
      "Epoch 432/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 118546194432.0000 - mae: 270715.250 - 0s 147us/sample - loss: 133997106140.6897 - mae: 251665.7656 - val_loss: 1185008621826.9426 - val_mae: 571968.0625\n",
      "Epoch 433/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 176149069824.0000 - mae: 324314.250 - 0s 178us/sample - loss: 128564138596.0460 - mae: 245533.5625 - val_loss: 1254850923084.5059 - val_mae: 585124.6875\n",
      "Epoch 434/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 146460000256.0000 - mae: 243129.250 - 0s 175us/sample - loss: 131313997329.6552 - mae: 244835.1719 - val_loss: 1179993969817.0115 - val_mae: 570292.9375\n",
      "Epoch 435/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 248880463872.0000 - mae: 348627.656 - 0s 163us/sample - loss: 127340897856.7356 - mae: 241623.2031 - val_loss: 1230249520728.2759 - val_mae: 578074.9375\n",
      "Epoch 436/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 139155111936.0000 - mae: 237800.625 - 0s 160us/sample - loss: 122720587681.8391 - mae: 236076.6719 - val_loss: 1192137868958.8965 - val_mae: 571949.6250\n",
      "Epoch 437/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 136782782464.0000 - mae: 277392.875 - 0s 159us/sample - loss: 117872641694.8965 - mae: 236278.9219 - val_loss: 1242173127009.1035 - val_mae: 583762.3125\n",
      "Epoch 438/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 150877700096.0000 - mae: 282709.281 - 0s 146us/sample - loss: 118289608374.4368 - mae: 230864.5781 - val_loss: 1226707354164.9656 - val_mae: 579188.4375\n",
      "Epoch 439/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 80662528000.0000 - mae: 187903.03 - 0s 150us/sample - loss: 133430501093.5172 - mae: 248212.2031 - val_loss: 1204092601073.2874 - val_mae: 580307.6250\n",
      "Epoch 440/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 102443769856.0000 - mae: 190208.890 - 0s 164us/sample - loss: 125326616787.8621 - mae: 245357.0781 - val_loss: 1211369680848.9194 - val_mae: 572130.9375\n",
      "Epoch 441/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 108055461888.0000 - mae: 241694.859 - 0s 154us/sample - loss: 120495776779.7701 - mae: 241211.4531 - val_loss: 1262763602626.2068 - val_mae: 587703.1250\n",
      "Epoch 442/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 86062071808.0000 - mae: 224965.35 - 0s 148us/sample - loss: 128183602117.1494 - mae: 249042.3750 - val_loss: 1312683294343.3562 - val_mae: 609190.1875\n",
      "Epoch 443/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 125319421952.0000 - mae: 255080.343 - 0s 140us/sample - loss: 125355511855.0805 - mae: 241911.4062 - val_loss: 1256670954225.2874 - val_mae: 597560.1875\n",
      "Epoch 444/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 69637070848.0000 - mae: 197051.78 - 0s 155us/sample - loss: 116604257009.2874 - mae: 233128.5312 - val_loss: 1264479104600.2759 - val_mae: 589357.0000\n",
      "Epoch 445/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 154807091200.0000 - mae: 280381.062 - 0s 165us/sample - loss: 109827520959.2644 - mae: 225835.7188 - val_loss: 1255096145237.3333 - val_mae: 586416.9375\n",
      "Epoch 446/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 43295653888.0000 - mae: 148662.03 - 0s 141us/sample - loss: 116608882758.6207 - mae: 232889.6094 - val_loss: 1275260665479.3562 - val_mae: 585241.8125\n",
      "Epoch 447/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 92278210560.0000 - mae: 214608.56 - 0s 143us/sample - loss: 106706881995.0345 - mae: 221443.9062 - val_loss: 1251197425169.6553 - val_mae: 582019.6875\n",
      "Epoch 448/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 99052371968.0000 - mae: 233848.53 - 0s 154us/sample - loss: 119955552797.4253 - mae: 232267.2031 - val_loss: 1233745935630.7126 - val_mae: 580203.8750\n",
      "Epoch 449/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 34188859392.0000 - mae: 136329.93 - 0s 147us/sample - loss: 115513956234.2988 - mae: 232750.0000 - val_loss: 1332211786952.0920 - val_mae: 609583.0625\n",
      "Epoch 450/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 172521013248.0000 - mae: 312271.625 - 0s 166us/sample - loss: 124774575139.3103 - mae: 238095.2656 - val_loss: 1342497017338.1150 - val_mae: 626700.0625\n",
      "Epoch 451/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 101672558592.0000 - mae: 232642.312 - 0s 147us/sample - loss: 117761737080.6437 - mae: 241592.2812 - val_loss: 1254631002347.4023 - val_mae: 583871.6250\n",
      "Epoch 452/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 130224078848.0000 - mae: 224807.546 - 0s 160us/sample - loss: 110644837646.7126 - mae: 224330.8281 - val_loss: 1244661685612.8735 - val_mae: 576218.6875\n",
      "Epoch 453/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 161198112768.0000 - mae: 283134.656 - 0s 146us/sample - loss: 114427557205.3333 - mae: 229524.2969 - val_loss: 1195025903380.5977 - val_mae: 572330.6250\n",
      "Epoch 454/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 102901080064.0000 - mae: 230828.406 - 0s 175us/sample - loss: 106558970726.9885 - mae: 221655.0781 - val_loss: 1274378712122.8503 - val_mae: 582411.9375\n",
      "Epoch 455/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 167268417536.0000 - mae: 281972.250 - 0s 149us/sample - loss: 106669315319.1724 - mae: 220131.2344 - val_loss: 1248353973306.8506 - val_mae: 576477.8125\n",
      "Epoch 456/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 30098980864.0000 - mae: 132852.90 - 0s 155us/sample - loss: 104098492463.0805 - mae: 214203.1094 - val_loss: 1274692295103.2644 - val_mae: 584441.0000\n",
      "Epoch 457/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 97965047808.0000 - mae: 215845.87 - 0s 151us/sample - loss: 100168216434.7586 - mae: 212039.5156 - val_loss: 1284915349150.8965 - val_mae: 588847.0000\n",
      "Epoch 458/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 119229612032.0000 - mae: 226276.968 - 0s 142us/sample - loss: 99637073425.6552 - mae: 212022.9219 - val_loss: 1283946321802.2988 - val_mae: 590044.4375\n",
      "Epoch 459/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 69103321088.0000 - mae: 178676.62 - 0s 158us/sample - loss: 107045141774.7126 - mae: 222758.9844 - val_loss: 1260923598824.4597 - val_mae: 584832.2500\n",
      "Epoch 460/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 128187588608.0000 - mae: 211886.062 - 0s 169us/sample - loss: 101840091583.2644 - mae: 213929.2344 - val_loss: 1273143539311.8162 - val_mae: 582411.3125\n",
      "Epoch 461/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 56021778432.0000 - mae: 195203.71 - 0s 137us/sample - loss: 97589437828.4138 - mae: 210287.5469 - val_loss: 1313588962692.4138 - val_mae: 604465.3125\n",
      "Epoch 462/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 62860648448.0000 - mae: 204633.89 - 0s 154us/sample - loss: 103335122073.0115 - mae: 222805.2812 - val_loss: 1285709862417.6550 - val_mae: 586566.3125\n",
      "Epoch 463/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 36875726848.0000 - mae: 152388.93 - 0s 163us/sample - loss: 103767854021.1494 - mae: 221788.8438 - val_loss: 1282630274554.1150 - val_mae: 591730.5000\n",
      "Epoch 464/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 196044259328.0000 - mae: 290155.437 - 0s 155us/sample - loss: 101059508106.2988 - mae: 213282.2344 - val_loss: 1297713104731.2185 - val_mae: 587300.3750\n",
      "Epoch 465/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 149570977792.0000 - mae: 250583.375 - 0s 164us/sample - loss: 98877667210.2988 - mae: 210750.9375 - val_loss: 1270106121981.0574 - val_mae: 580641.6250\n",
      "Epoch 466/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 73289760768.0000 - mae: 182324.43 - 0s 151us/sample - loss: 100694556860.3219 - mae: 209372.0000 - val_loss: 1296606795281.6550 - val_mae: 589243.3750\n",
      "Epoch 467/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 66058964992.0000 - mae: 208517.85 - 0s 161us/sample - loss: 102558736007.3563 - mae: 219132.7812 - val_loss: 1272431363366.2527 - val_mae: 585893.0625\n",
      "Epoch 468/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 114645630976.0000 - mae: 235471.343 - 0s 163us/sample - loss: 102284504193.4713 - mae: 221816.9219 - val_loss: 1286928230141.0574 - val_mae: 599917.8125\n",
      "Epoch 469/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 150587998208.0000 - mae: 271229.812 - 0s 145us/sample - loss: 112030415624.8276 - mae: 235287.4219 - val_loss: 1258639492955.2185 - val_mae: 583118.8125\n",
      "Epoch 470/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 122549280768.0000 - mae: 230462.140 - 0s 149us/sample - loss: 104646801007.8161 - mae: 220480.8750 - val_loss: 1283673096945.2874 - val_mae: 582670.5625\n",
      "Epoch 471/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 161658978304.0000 - mae: 285681.125 - 0s 150us/sample - loss: 98308628762.4828 - mae: 216166.1562 - val_loss: 1298157485515.0344 - val_mae: 597374.3750\n",
      "Epoch 472/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - ETA: 0s - loss: 54470688768.0000 - mae: 181119.62 - 0s 145us/sample - loss: 99347021824.0000 - mae: 211997.1719 - val_loss: 1272740968989.4253 - val_mae: 588190.5000\n",
      "Epoch 473/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 181346729984.0000 - mae: 276044.250 - 0s 149us/sample - loss: 95377095950.7126 - mae: 211830.6250 - val_loss: 1295477897757.4253 - val_mae: 585752.4375\n",
      "Epoch 474/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 82503761920.0000 - mae: 184631.93 - 0s 147us/sample - loss: 91332135123.8621 - mae: 203436.8438 - val_loss: 1291686896275.1265 - val_mae: 586774.7500\n",
      "Epoch 475/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 117528846336.0000 - mae: 215857.218 - 0s 125us/sample - loss: 94837564286.5287 - mae: 209528.1094 - val_loss: 1274057765005.2415 - val_mae: 582954.9375\n",
      "Epoch 476/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 50308218880.0000 - mae: 159037.12 - 0s 156us/sample - loss: 107601972871.3563 - mae: 219875.4062 - val_loss: 1300046036427.0344 - val_mae: 585942.5000\n",
      "Epoch 477/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 101026283520.0000 - mae: 183236.328 - 0s 164us/sample - loss: 109463871582.1609 - mae: 225930.0625 - val_loss: 1264656500759.5403 - val_mae: 585468.6875\n",
      "Epoch 478/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 75661910016.0000 - mae: 182374.03 - 0s 128us/sample - loss: 111375936358.9885 - mae: 224472.7656 - val_loss: 1247458209179.9541 - val_mae: 584138.6250\n",
      "Epoch 479/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 141418332160.0000 - mae: 268178.125 - 0s 157us/sample - loss: 95064773408.3678 - mae: 212487.3125 - val_loss: 1392883775841.1035 - val_mae: 605173.6250\n",
      "Epoch 480/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 131396009984.0000 - mae: 218800.781 - 0s 154us/sample - loss: 95684726030.7126 - mae: 213557.3125 - val_loss: 1287852876058.4827 - val_mae: 592242.1875\n",
      "Epoch 481/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 81896816640.0000 - mae: 229023.90 - 0s 168us/sample - loss: 95713597369.3793 - mae: 211280.5938 - val_loss: 1305830559414.4368 - val_mae: 588090.5000\n",
      "Epoch 482/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 87587602432.0000 - mae: 178884.28 - 0s 180us/sample - loss: 87265402880.0000 - mae: 197805.5156 - val_loss: 1321533088968.0920 - val_mae: 602547.6875\n",
      "Epoch 483/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 88120639488.0000 - mae: 228810.48 - 0s 162us/sample - loss: 88503869722.4828 - mae: 207272.8438 - val_loss: 1294577021822.5288 - val_mae: 584356.2500\n",
      "Epoch 484/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 61688074240.0000 - mae: 167231.68 - 0s 152us/sample - loss: 91951526123.4023 - mae: 208211.5000 - val_loss: 1243287329403.5862 - val_mae: 582058.3125\n",
      "Epoch 485/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 78886273024.0000 - mae: 196085.37 - 0s 202us/sample - loss: 91208597739.4023 - mae: 202262.6875 - val_loss: 1268756800523.7700 - val_mae: 581777.0625\n",
      "Epoch 486/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 150281830400.0000 - mae: 248148.187 - 0s 149us/sample - loss: 88979922944.0000 - mae: 196249.4531 - val_loss: 1304340052321.1035 - val_mae: 588702.3125\n",
      "Epoch 487/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 101730992128.0000 - mae: 215179.031 - 0s 137us/sample - loss: 86852834174.5287 - mae: 196162.1406 - val_loss: 1328864193006.3450 - val_mae: 598278.3750\n",
      "Epoch 488/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 81585815552.0000 - mae: 212489.96 - 0s 150us/sample - loss: 84319319852.1379 - mae: 199272.0000 - val_loss: 1271279734642.7585 - val_mae: 584950.4375\n",
      "Epoch 489/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 103370489856.0000 - mae: 204868.890 - 0s 141us/sample - loss: 84998864519.3563 - mae: 192605.2812 - val_loss: 1310625582962.7585 - val_mae: 588014.5625\n",
      "Epoch 490/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 90622083072.0000 - mae: 165058.57 - 0s 147us/sample - loss: 86248617395.4942 - mae: 193188.9375 - val_loss: 1293522824792.2759 - val_mae: 586713.9375\n",
      "Epoch 491/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 40738066432.0000 - mae: 135469.25 - 0s 166us/sample - loss: 96707905465.3793 - mae: 206823.9531 - val_loss: 1350652210211.3103 - val_mae: 601735.6250\n",
      "Epoch 492/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 108230696960.0000 - mae: 230910.343 - 0s 172us/sample - loss: 87812162195.1264 - mae: 204160.5312 - val_loss: 1293657202217.1953 - val_mae: 588699.7500\n",
      "Epoch 493/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 70105522176.0000 - mae: 188489.43 - 0s 158us/sample - loss: 84314853988.0460 - mae: 200074.0625 - val_loss: 1287440354810.1150 - val_mae: 588625.6250\n",
      "Epoch 494/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 41170108416.0000 - mae: 141074.34 - 0s 168us/sample - loss: 85187118185.9310 - mae: 203565.4219 - val_loss: 1311804357149.4253 - val_mae: 593161.1250\n",
      "Epoch 495/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 53803991040.0000 - mae: 158011.96 - 0s 139us/sample - loss: 82023516277.7012 - mae: 190556.9375 - val_loss: 1326479737208.6438 - val_mae: 594236.6250\n",
      "Epoch 496/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 40981499904.0000 - mae: 141152.43 - 0s 163us/sample - loss: 86569743701.3333 - mae: 196719.8125 - val_loss: 1295992936895.2644 - val_mae: 589091.3750\n",
      "Epoch 497/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 48836890624.0000 - mae: 137863.32 - 0s 157us/sample - loss: 85784609238.8046 - mae: 195695.3594 - val_loss: 1344608627935.6321 - val_mae: 598891.8125\n",
      "Epoch 498/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 109816119296.0000 - mae: 231313.921 - 0s 143us/sample - loss: 86361849008.5517 - mae: 201604.7188 - val_loss: 1279040039218.0229 - val_mae: 586492.5625\n",
      "Epoch 499/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 63945105408.0000 - mae: 176968.89 - 0s 168us/sample - loss: 85556020706.5747 - mae: 199178.4844 - val_loss: 1301527547009.4712 - val_mae: 590810.0625\n",
      "Epoch 500/500\n",
      "348/348 [==============================] - ETA: 0s - loss: 74076340224.0000 - mae: 197243.45 - 0s 151us/sample - loss: 79360817787.5862 - mae: 189283.2031 - val_loss: 1301175990813.4253 - val_mae: 590361.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f05bc4e6950>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_1.fit(train_x, train_y, epochs = 500, validation_split = 0.5, callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 16994), started 0:05:41 ago. (Use '!kill 16994' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1bea827c0a75ca93\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1bea827c0a75ca93\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 348 samples\n",
      "Epoch 1/300\n",
      "348/348 [==============================] - ETA: 10s - loss: 3259642413056.0000 - mae: 1325656.25 - 1s 4ms/sample - loss: 2750226719002.4829 - mae: 1239739.1250 - val_loss: 3060518960587.0347 - val_mae: 1316725.0000\n",
      "Epoch 2/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2680418992128.0000 - mae: 1097371.250 - 0s 196us/sample - loss: 2750197235335.3564 - mae: 1239728.3750 - val_loss: 3060437271092.9653 - val_mae: 1316696.8750\n",
      "Epoch 3/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2158651113472.0000 - mae: 1174433.500 - 0s 186us/sample - loss: 2750007809177.0117 - mae: 1239653.7500 - val_loss: 3059986494899.4946 - val_mae: 1316542.7500\n",
      "Epoch 4/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2055072645120.0000 - mae: 1185436.250 - 0s 182us/sample - loss: 2749157909704.0918 - mae: 1239333.8750 - val_loss: 3058151559215.0806 - val_mae: 1315920.6250\n",
      "Epoch 5/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 3533973225472.0000 - mae: 1407558.250 - 0s 189us/sample - loss: 2746120177499.2183 - mae: 1238245.7500 - val_loss: 3052875013296.5518 - val_mae: 1314135.6250\n",
      "Epoch 6/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 3303798734848.0000 - mae: 1446127.750 - 0s 190us/sample - loss: 2739764973803.4023 - mae: 1235745.6250 - val_loss: 3042390991883.7700 - val_mae: 1310586.1250\n",
      "Epoch 7/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1731368845312.0000 - mae: 1072612.500 - 0s 194us/sample - loss: 2725674700658.7588 - mae: 1230724.7500 - val_loss: 3020297254511.8159 - val_mae: 1303083.1250\n",
      "Epoch 8/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 4510517297152.0000 - mae: 1303052.125 - 0s 183us/sample - loss: 2699768160161.8389 - mae: 1220757.3750 - val_loss: 2983631275678.8960 - val_mae: 1290541.7500\n",
      "Epoch 9/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1511332249600.0000 - mae: 940924.37 - 0s 175us/sample - loss: 2653507894731.0347 - mae: 1205236.8750 - val_loss: 2921462843792.1841 - val_mae: 1269002.5000\n",
      "Epoch 10/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 3120398598144.0000 - mae: 1369098.125 - 0s 192us/sample - loss: 2595759831393.1035 - mae: 1181278.7500 - val_loss: 2833328803133.7930 - val_mae: 1237852.1250\n",
      "Epoch 11/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1948571140096.0000 - mae: 1026971.687 - 0s 156us/sample - loss: 2499072488506.8506 - mae: 1142903.2500 - val_loss: 2707441758349.2412 - val_mae: 1192031.1250\n",
      "Epoch 12/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2495946424320.0000 - mae: 1151100.625 - 0s 177us/sample - loss: 2372387489438.8965 - mae: 1091442.1250 - val_loss: 2540657640177.2876 - val_mae: 1128653.2500\n",
      "Epoch 13/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1988618485760.0000 - mae: 1045316.812 - 0s 182us/sample - loss: 2195724562008.2759 - mae: 1017684.5000 - val_loss: 2319090230472.0918 - val_mae: 1038889.7500\n",
      "Epoch 14/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1619675054080.0000 - mae: 800266.37 - 0s 187us/sample - loss: 1990914085287.7241 - mae: 920327.3750 - val_loss: 2068924719763.1265 - val_mae: 930956.8750\n",
      "Epoch 15/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2200325324800.0000 - mae: 942007.37 - 0s 204us/sample - loss: 1728519584097.1033 - mae: 806920.5625 - val_loss: 1790320725121.4712 - val_mae: 815181.6250\n",
      "Epoch 16/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1941827485696.0000 - mae: 899568.12 - 0s 175us/sample - loss: 1514094879755.7700 - mae: 725591.3750 - val_loss: 1524859319166.5288 - val_mae: 718729.8750\n",
      "Epoch 17/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2918948012032.0000 - mae: 639908.25 - 0s 170us/sample - loss: 1326917143304.8276 - mae: 652973.6875 - val_loss: 1302471917003.0344 - val_mae: 658190.7500\n",
      "Epoch 18/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1558605070336.0000 - mae: 840867.50 - 0s 175us/sample - loss: 1173377709868.1379 - mae: 637663.8125 - val_loss: 1134551598939.2185 - val_mae: 632804.6875\n",
      "Epoch 19/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 971102486528.0000 - mae: 463562.250 - 0s 163us/sample - loss: 1090574003611.9540 - mae: 638268.0625 - val_loss: 1041513212774.9885 - val_mae: 634734.5000\n",
      "Epoch 20/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 837289508864.0000 - mae: 630707.500 - 0s 178us/sample - loss: 1019793181637.1494 - mae: 646388.5625 - val_loss: 995790069854.1609 - val_mae: 638206.7500\n",
      "Epoch 21/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 657041784832.0000 - mae: 590279.125 - 0s 190us/sample - loss: 1044098429069.2413 - mae: 650518.3125 - val_loss: 976166031783.7240 - val_mae: 635950.0000\n",
      "Epoch 22/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 568978571264.0000 - mae: 524731.312 - 0s 165us/sample - loss: 1004265268612.4137 - mae: 654381.0625 - val_loss: 952105006515.4943 - val_mae: 637473.0625\n",
      "Epoch 23/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2206689394688.0000 - mae: 932088.50 - 0s 190us/sample - loss: 1012948995389.7931 - mae: 651791.7500 - val_loss: 952793009469.7931 - val_mae: 624700.3750\n",
      "Epoch 24/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 598827728896.0000 - mae: 620096.375 - 0s 182us/sample - loss: 998168851514.8506 - mae: 647886.0000 - val_loss: 939937192759.9081 - val_mae: 620968.8125\n",
      "Epoch 25/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1507727900672.0000 - mae: 818049.75 - 0s 174us/sample - loss: 1030537200180.9656 - mae: 658031.7500 - val_loss: 931078655105.4713 - val_mae: 614896.6250\n",
      "Epoch 26/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1374741069824.0000 - mae: 676686.18 - 0s 174us/sample - loss: 1012073397142.0690 - mae: 652222.3750 - val_loss: 919123547477.3334 - val_mae: 610124.1250\n",
      "Epoch 27/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 566201286656.0000 - mae: 529660.375 - 0s 170us/sample - loss: 999879370493.0575 - mae: 649299.0625 - val_loss: 905507017127.7240 - val_mae: 607587.6875\n",
      "Epoch 28/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1251881517056.0000 - mae: 734784.12 - 0s 185us/sample - loss: 951459735293.0575 - mae: 631968.8125 - val_loss: 883907291630.3448 - val_mae: 609730.8750\n",
      "Epoch 29/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1205747974144.0000 - mae: 772965.75 - 0s 177us/sample - loss: 971074569698.5748 - mae: 646503.9375 - val_loss: 885675029939.4943 - val_mae: 597347.1875\n",
      "Epoch 30/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 660421607424.0000 - mae: 559856.875 - 0s 151us/sample - loss: 960898080862.1609 - mae: 622791.6250 - val_loss: 867306270672.9196 - val_mae: 599548.6250\n",
      "Epoch 31/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 450114879488.0000 - mae: 526723.875 - 0s 171us/sample - loss: 965456582726.6207 - mae: 624478.0000 - val_loss: 850301615751.3563 - val_mae: 602531.0625\n",
      "Epoch 32/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 422263160832.0000 - mae: 509338.500 - 0s 212us/sample - loss: 908166149743.8162 - mae: 613683.4375 - val_loss: 841020314777.0115 - val_mae: 598443.3750\n",
      "Epoch 33/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 401724637184.0000 - mae: 548655.187 - 0s 207us/sample - loss: 921121038430.1609 - mae: 604112.7500 - val_loss: 829890121586.7587 - val_mae: 595736.3750\n",
      "Epoch 34/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2122621124608.0000 - mae: 945319.68 - 0s 175us/sample - loss: 932733673754.4827 - mae: 644853.5000 - val_loss: 834333779144.0919 - val_mae: 579369.1250\n",
      "Epoch 35/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 441793904640.0000 - mae: 529446.750 - 0s 184us/sample - loss: 959091910962.0229 - mae: 628350.7500 - val_loss: 832045103833.7471 - val_mae: 570695.6250\n",
      "Epoch 36/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1024277872640.0000 - mae: 566427.00 - 0s 177us/sample - loss: 952956934214.6207 - mae: 593885.5000 - val_loss: 823222277649.6552 - val_mae: 566949.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2698821238784.0000 - mae: 757398.81 - 0s 152us/sample - loss: 914417402797.6093 - mae: 599110.3750 - val_loss: 815398562851.3103 - val_mae: 563242.3750\n",
      "Epoch 38/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 409587089408.0000 - mae: 527023.312 - 0s 169us/sample - loss: 900293372586.6667 - mae: 601417.6250 - val_loss: 814663408628.2299 - val_mae: 554859.3125\n",
      "Epoch 39/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 633855410176.0000 - mae: 466662.875 - 0s 169us/sample - loss: 847517073972.9656 - mae: 567278.0000 - val_loss: 789838558266.8506 - val_mae: 561671.9375\n",
      "Epoch 40/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 366995243008.0000 - mae: 476726.875 - 0s 176us/sample - loss: 876932815295.2643 - mae: 589918.5000 - val_loss: 783386263740.3219 - val_mae: 557097.1250\n",
      "Epoch 41/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 3061412003840.0000 - mae: 929320.81 - 0s 198us/sample - loss: 851761523558.9885 - mae: 594882.0000 - val_loss: 785205181899.0344 - val_mae: 547365.6875\n",
      "Epoch 42/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 598178267136.0000 - mae: 491460.500 - 0s 175us/sample - loss: 868914276281.3793 - mae: 593392.9375 - val_loss: 773000434087.7240 - val_mae: 546660.0000\n",
      "Epoch 43/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 477686988800.0000 - mae: 483688.531 - 0s 185us/sample - loss: 894597278755.3103 - mae: 594451.6875 - val_loss: 764924313788.3219 - val_mae: 543189.1250\n",
      "Epoch 44/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1347447685120.0000 - mae: 734735.37 - 0s 179us/sample - loss: 846530417063.7241 - mae: 589672.8750 - val_loss: 765848934093.9772 - val_mae: 535458.3750\n",
      "Epoch 45/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 388758470656.0000 - mae: 467991.312 - 0s 145us/sample - loss: 887425008745.9310 - mae: 593398.0625 - val_loss: 767691132975.0804 - val_mae: 528128.6250\n",
      "Epoch 46/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 396811337728.0000 - mae: 464572.281 - 0s 192us/sample - loss: 879010474478.3447 - mae: 576684.4375 - val_loss: 749565957461.3334 - val_mae: 530783.8125\n",
      "Epoch 47/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 293650759680.0000 - mae: 395924.875 - 0s 159us/sample - loss: 917258702518.4369 - mae: 577552.6875 - val_loss: 744749641869.2413 - val_mae: 526321.6875\n",
      "Epoch 48/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1056407486464.0000 - mae: 696868.31 - 0s 200us/sample - loss: 818162851369.1954 - mae: 566708.6875 - val_loss: 724972473661.7931 - val_mae: 533464.0000\n",
      "Epoch 49/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 485651415040.0000 - mae: 552959.750 - 0s 178us/sample - loss: 907986216512.7357 - mae: 581883.7500 - val_loss: 716937855352.6437 - val_mae: 533760.5625\n",
      "Epoch 50/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2414964113408.0000 - mae: 762274.93 - 0s 196us/sample - loss: 869666493216.3678 - mae: 586955.3750 - val_loss: 714055695065.7471 - val_mae: 527096.8125\n",
      "Epoch 51/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 552284913664.0000 - mae: 499408.000 - 0s 168us/sample - loss: 868447712314.8506 - mae: 564604.8125 - val_loss: 720575059661.9772 - val_mae: 514343.9062\n",
      "Epoch 52/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 401022124032.0000 - mae: 464788.375 - 0s 195us/sample - loss: 822065862161.6553 - mae: 544262.1875 - val_loss: 704516975839.6322 - val_mae: 521128.3750\n",
      "Epoch 53/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 306479169536.0000 - mae: 347548.875 - 0s 191us/sample - loss: 832206278326.4369 - mae: 562453.3125 - val_loss: 706164910950.9885 - val_mae: 513129.5938\n",
      "Epoch 54/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1104349626368.0000 - mae: 704589.31 - 0s 198us/sample - loss: 867528939402.2988 - mae: 559838.9375 - val_loss: 698659879265.1034 - val_mae: 513658.1562\n",
      "Epoch 55/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 868329652224.0000 - mae: 555655.562 - 0s 180us/sample - loss: 806694427259.5863 - mae: 559394.8750 - val_loss: 690911451959.9081 - val_mae: 514379.7812\n",
      "Epoch 56/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 3210286202880.0000 - mae: 804688.75 - 0s 193us/sample - loss: 805007248136.8276 - mae: 538883.3125 - val_loss: 688479583879.3563 - val_mae: 510428.7812\n",
      "Epoch 57/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 568891080704.0000 - mae: 427770.687 - 0s 196us/sample - loss: 839391513541.1494 - mae: 546189.8750 - val_loss: 682634464950.4368 - val_mae: 509848.9688\n",
      "Epoch 58/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 517657460736.0000 - mae: 476441.312 - 0s 168us/sample - loss: 846883601490.3907 - mae: 535029.8125 - val_loss: 677485794268.6897 - val_mae: 508793.1562\n",
      "Epoch 59/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2006207299584.0000 - mae: 625687.31 - 0s 170us/sample - loss: 729939216219.2184 - mae: 525094.6250 - val_loss: 668265765311.2644 - val_mae: 515140.7500\n",
      "Epoch 60/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 582324322304.0000 - mae: 523044.968 - 0s 170us/sample - loss: 847201910972.3218 - mae: 553550.1875 - val_loss: 671833517844.5977 - val_mae: 501844.8750\n",
      "Epoch 61/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 343320461312.0000 - mae: 442981.406 - 0s 190us/sample - loss: 798701138437.8850 - mae: 548835.5625 - val_loss: 681099363245.6093 - val_mae: 490835.2500\n",
      "Epoch 62/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 296978776064.0000 - mae: 435844.937 - 0s 192us/sample - loss: 751623828162.2069 - mae: 513704.1875 - val_loss: 672723546629.8850 - val_mae: 492317.4688\n",
      "Epoch 63/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 622567751680.0000 - mae: 474325.937 - 0s 162us/sample - loss: 785058617037.9771 - mae: 530353.5625 - val_loss: 668286398605.2413 - val_mae: 491588.4688\n",
      "Epoch 64/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 469211643904.0000 - mae: 500329.093 - 0s 195us/sample - loss: 782512969798.6207 - mae: 528996.4375 - val_loss: 661528904244.9656 - val_mae: 492902.5625\n",
      "Epoch 65/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 696870961152.0000 - mae: 558991.375 - 0s 183us/sample - loss: 753314180131.3103 - mae: 541819.3750 - val_loss: 668444035848.8275 - val_mae: 484412.6875\n",
      "Epoch 66/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2810264682496.0000 - mae: 791127.25 - 0s 199us/sample - loss: 747063400330.2988 - mae: 516156.2812 - val_loss: 669174996144.5518 - val_mae: 482237.0000\n",
      "Epoch 67/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 401687281664.0000 - mae: 501100.500 - 0s 189us/sample - loss: 777954536483.3103 - mae: 521849.5312 - val_loss: 651508476398.3448 - val_mae: 490265.5938\n",
      "Epoch 68/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 261983895552.0000 - mae: 443866.312 - 0s 186us/sample - loss: 779371960390.6207 - mae: 528458.1250 - val_loss: 639069425522.7587 - val_mae: 501072.0938\n",
      "Epoch 69/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 717271203840.0000 - mae: 607799.937 - 0s 163us/sample - loss: 695950969797.1494 - mae: 526966.2500 - val_loss: 641615853921.1034 - val_mae: 490437.5625\n",
      "Epoch 70/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2523017773056.0000 - mae: 796548.37 - 0s 186us/sample - loss: 829624645043.4941 - mae: 555328.1875 - val_loss: 643259864217.0115 - val_mae: 484919.0000\n",
      "Epoch 71/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 895035047936.0000 - mae: 601012.812 - 0s 201us/sample - loss: 751490900062.1609 - mae: 517766.2188 - val_loss: 632827005139.8621 - val_mae: 491394.4062\n",
      "Epoch 72/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 317152362496.0000 - mae: 405213.812 - 0s 192us/sample - loss: 787191888790.0690 - mae: 538107.1875 - val_loss: 636055668571.2184 - val_mae: 482641.6875\n",
      "Epoch 73/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 140667650048.0000 - mae: 296473.312 - 0s 191us/sample - loss: 771617830511.8162 - mae: 519876.8750 - val_loss: 635134653687.1725 - val_mae: 479809.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 420598054912.0000 - mae: 433849.062 - 0s 181us/sample - loss: 726389120553.1954 - mae: 511331.4062 - val_loss: 625158786554.1150 - val_mae: 486479.3438\n",
      "Epoch 75/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 879319973888.0000 - mae: 641625.500 - 0s 186us/sample - loss: 798958956308.5978 - mae: 534488.5000 - val_loss: 633788483607.5403 - val_mae: 475496.9062\n",
      "Epoch 76/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 616024571904.0000 - mae: 578994.500 - 0s 185us/sample - loss: 795065810567.3563 - mae: 545788.3125 - val_loss: 635186992092.6897 - val_mae: 472577.1562\n",
      "Epoch 77/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 559621210112.0000 - mae: 424256.250 - 0s 174us/sample - loss: 747939679738.1150 - mae: 498732.7812 - val_loss: 620417144714.2988 - val_mae: 480903.1875\n",
      "Epoch 78/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 287217221632.0000 - mae: 456916.187 - 0s 195us/sample - loss: 784656709608.4597 - mae: 529919.5625 - val_loss: 617919705982.5287 - val_mae: 479268.1250\n",
      "Epoch 79/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 485105336320.0000 - mae: 549912.375 - 0s 181us/sample - loss: 777790720211.8621 - mae: 522181.4688 - val_loss: 616544357317.1494 - val_mae: 477350.6562\n",
      "Epoch 80/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 398327611392.0000 - mae: 464236.875 - 0s 166us/sample - loss: 855042005250.9425 - mae: 547722.1875 - val_loss: 624599168435.4943 - val_mae: 468571.1250\n",
      "Epoch 81/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 3507418824704.0000 - mae: 1023654.500 - 0s 155us/sample - loss: 780080461423.8162 - mae: 514945.0938 - val_loss: 634989460562.3907 - val_mae: 462562.8438\n",
      "Epoch 82/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 523080171520.0000 - mae: 525049.812 - 0s 177us/sample - loss: 788347195439.0804 - mae: 512939.5312 - val_loss: 619318022756.0460 - val_mae: 466966.4062\n",
      "Epoch 83/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 585531260928.0000 - mae: 590791.312 - 0s 188us/sample - loss: 783054444802.9425 - mae: 502112.7812 - val_loss: 605854259458.9425 - val_mae: 475140.0312\n",
      "Epoch 84/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 631962992640.0000 - mae: 493193.562 - 0s 190us/sample - loss: 797641502484.5978 - mae: 528563.1875 - val_loss: 614580307238.2529 - val_mae: 465168.0000\n",
      "Epoch 85/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1252217323520.0000 - mae: 632108.00 - 0s 143us/sample - loss: 813898270555.2184 - mae: 525786.1250 - val_loss: 628308486826.6666 - val_mae: 458487.6875\n",
      "Epoch 86/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 396574851072.0000 - mae: 381884.843 - 0s 184us/sample - loss: 763405916607.2643 - mae: 511843.0000 - val_loss: 617811756326.2529 - val_mae: 460023.0312\n",
      "Epoch 87/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 464559603712.0000 - mae: 485446.312 - 0s 164us/sample - loss: 751641519115.7701 - mae: 500195.2188 - val_loss: 604784534539.7701 - val_mae: 465228.4062\n",
      "Epoch 88/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 285263921152.0000 - mae: 428365.312 - 0s 158us/sample - loss: 752820817967.0804 - mae: 501276.5000 - val_loss: 606314823115.0344 - val_mae: 462146.0625\n",
      "Epoch 89/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 427135696896.0000 - mae: 432650.656 - 0s 191us/sample - loss: 803734797771.0344 - mae: 517855.5000 - val_loss: 606670934063.0804 - val_mae: 460244.5938\n",
      "Epoch 90/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 578802679808.0000 - mae: 522446.750 - 0s 180us/sample - loss: 778961657573.5173 - mae: 504221.5312 - val_loss: 598312411606.8046 - val_mae: 463381.8750\n",
      "Epoch 91/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 468876001280.0000 - mae: 456880.125 - 0s 180us/sample - loss: 736312582426.4828 - mae: 505472.4062 - val_loss: 593019444400.5518 - val_mae: 465267.5000\n",
      "Epoch 92/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 274073616384.0000 - mae: 431061.250 - 0s 192us/sample - loss: 762940164307.8621 - mae: 497286.9375 - val_loss: 588532075837.7931 - val_mae: 467376.2812\n",
      "Epoch 93/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 392396079104.0000 - mae: 476573.125 - 0s 157us/sample - loss: 740907612324.7816 - mae: 516403.1875 - val_loss: 590386749910.8046 - val_mae: 462296.6562\n",
      "Epoch 94/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 740758585344.0000 - mae: 549868.125 - 0s 158us/sample - loss: 717856120926.1609 - mae: 496671.1250 - val_loss: 586707885420.8735 - val_mae: 463004.4688\n",
      "Epoch 95/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 183943364608.0000 - mae: 361916.187 - 0s 207us/sample - loss: 680612241125.5172 - mae: 499660.9062 - val_loss: 585760662728.0919 - val_mae: 461881.1562\n",
      "Epoch 96/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 343308337152.0000 - mae: 396906.531 - 0s 187us/sample - loss: 779631742834.7587 - mae: 505279.7812 - val_loss: 594843137647.8160 - val_mae: 454380.0312\n",
      "Epoch 97/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 508373827584.0000 - mae: 420168.437 - 0s 156us/sample - loss: 758956633876.5978 - mae: 497659.4062 - val_loss: 580979811151.4482 - val_mae: 461881.3750\n",
      "Epoch 98/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 254474240000.0000 - mae: 397088.125 - 0s 166us/sample - loss: 828826405264.1838 - mae: 519509.1875 - val_loss: 580411408948.9656 - val_mae: 459165.8750\n",
      "Epoch 99/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 581608144896.0000 - mae: 491953.437 - 0s 169us/sample - loss: 709043901404.6897 - mae: 499513.0000 - val_loss: 579012118174.8966 - val_mae: 457935.6250\n",
      "Epoch 100/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 395975655424.0000 - mae: 461740.593 - 0s 181us/sample - loss: 683182816715.0345 - mae: 477777.1875 - val_loss: 580618269189.8850 - val_mae: 454227.5938\n",
      "Epoch 101/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 878908735488.0000 - mae: 618036.875 - 0s 194us/sample - loss: 846312857882.4827 - mae: 519951.5312 - val_loss: 586827390881.8391 - val_mae: 449159.4375\n",
      "Epoch 102/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 192839942144.0000 - mae: 363666.250 - 0s 200us/sample - loss: 723457774180.0460 - mae: 497821.6875 - val_loss: 579658404640.3678 - val_mae: 450920.6562\n",
      "Epoch 103/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 433011752960.0000 - mae: 476459.656 - 0s 172us/sample - loss: 689940818414.3448 - mae: 483687.7188 - val_loss: 575491154579.1265 - val_mae: 451527.1875\n",
      "Epoch 104/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 203459428352.0000 - mae: 346267.375 - 0s 199us/sample - loss: 791282162487.9081 - mae: 508833.6875 - val_loss: 580775809259.4023 - val_mae: 446697.0625\n",
      "Epoch 105/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 509487284224.0000 - mae: 419650.687 - 0s 171us/sample - loss: 742275868789.7012 - mae: 497567.6875 - val_loss: 582255867692.1379 - val_mae: 445010.5312\n",
      "Epoch 106/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 823833198592.0000 - mae: 517359.406 - 0s 148us/sample - loss: 751720106949.1494 - mae: 514126.0312 - val_loss: 576616234443.0344 - val_mae: 446286.2188\n",
      "Epoch 107/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 316969582592.0000 - mae: 413052.687 - 0s 184us/sample - loss: 823567874212.7816 - mae: 500559.9688 - val_loss: 577160850655.6322 - val_mae: 444884.6875\n",
      "Epoch 108/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 364521848832.0000 - mae: 376239.687 - 0s 175us/sample - loss: 737069138296.6437 - mae: 486660.6875 - val_loss: 569877474527.6322 - val_mae: 447253.0000\n",
      "Epoch 109/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 956621258752.0000 - mae: 606502.750 - 0s 157us/sample - loss: 754258945224.0919 - mae: 505168.1875 - val_loss: 569066497789.0575 - val_mae: 446233.1875\n",
      "Epoch 110/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 345335595008.0000 - mae: 454328.718 - 0s 208us/sample - loss: 748926087309.2413 - mae: 497878.1250 - val_loss: 564355146575.4482 - val_mae: 448133.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 806146670592.0000 - mae: 510025.750 - 0s 173us/sample - loss: 722805433544.0919 - mae: 517851.0312 - val_loss: 570093981743.0804 - val_mae: 442690.1562\n",
      "Epoch 112/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 733367631872.0000 - mae: 522832.312 - 0s 183us/sample - loss: 711457466109.0575 - mae: 489965.2500 - val_loss: 570539920701.7931 - val_mae: 441617.3438\n",
      "Epoch 113/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 283312586752.0000 - mae: 322398.125 - 0s 160us/sample - loss: 731033161963.4022 - mae: 488769.7500 - val_loss: 562695178263.5403 - val_mae: 444931.1250\n",
      "Epoch 114/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 833295351808.0000 - mae: 512657.625 - 0s 165us/sample - loss: 728131518652.3218 - mae: 494952.1875 - val_loss: 563385444093.0575 - val_mae: 443456.0000\n",
      "Epoch 115/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1005279379456.0000 - mae: 603537.00 - 0s 190us/sample - loss: 683150647955.1265 - mae: 488362.6562 - val_loss: 561025976908.5057 - val_mae: 443268.5625\n",
      "Epoch 116/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 387171516416.0000 - mae: 407079.531 - 0s 175us/sample - loss: 762200480944.5516 - mae: 494622.6562 - val_loss: 564670976800.3678 - val_mae: 439497.8438\n",
      "Epoch 117/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2752163348480.0000 - mae: 838116.87 - 0s 164us/sample - loss: 740483691202.2069 - mae: 493375.3125 - val_loss: 567037437693.0575 - val_mae: 437299.0000\n",
      "Epoch 118/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 764386607104.0000 - mae: 542139.375 - 0s 166us/sample - loss: 743359422934.8046 - mae: 493947.1250 - val_loss: 566741663932.3219 - val_mae: 436117.8438\n",
      "Epoch 119/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 648065056768.0000 - mae: 470797.750 - 0s 156us/sample - loss: 717432550082.2069 - mae: 478277.0625 - val_loss: 556105744195.6781 - val_mae: 440879.6875\n",
      "Epoch 120/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 251054866432.0000 - mae: 385536.625 - 0s 179us/sample - loss: 711146576507.5862 - mae: 496824.8125 - val_loss: 558328249991.3563 - val_mae: 438011.3438\n",
      "Epoch 121/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 376804933632.0000 - mae: 438883.937 - 0s 178us/sample - loss: 716354907924.5978 - mae: 482902.6250 - val_loss: 555843942753.1034 - val_mae: 438349.9688\n",
      "Epoch 122/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 401013833728.0000 - mae: 393542.062 - 0s 162us/sample - loss: 733312859218.3907 - mae: 498310.3438 - val_loss: 566489151841.1034 - val_mae: 433427.2188\n",
      "Epoch 123/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 468305936384.0000 - mae: 475364.187 - 0s 181us/sample - loss: 796344474423.9081 - mae: 512251.0938 - val_loss: 564625749615.8160 - val_mae: 433076.4688\n",
      "Epoch 124/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 558214676480.0000 - mae: 413675.562 - 0s 182us/sample - loss: 646198680375.9081 - mae: 463025.1875 - val_loss: 551931432630.4368 - val_mae: 438468.0938\n",
      "Epoch 125/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 477821796352.0000 - mae: 465993.250 - 0s 165us/sample - loss: 755225904422.2528 - mae: 494963.7812 - val_loss: 563294538128.1840 - val_mae: 431837.3750\n",
      "Epoch 126/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 649370075136.0000 - mae: 491487.906 - 0s 167us/sample - loss: 730412774082.2069 - mae: 496846.2188 - val_loss: 563286482096.5518 - val_mae: 431597.5938\n",
      "Epoch 127/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2457576407040.0000 - mae: 724821.00 - 0s 175us/sample - loss: 766628283309.6093 - mae: 492429.2500 - val_loss: 563411311792.5518 - val_mae: 430895.7188\n",
      "Epoch 128/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 493897351168.0000 - mae: 475822.500 - 0s 187us/sample - loss: 671271767169.4712 - mae: 464785.8750 - val_loss: 550859777035.7701 - val_mae: 435596.1875\n",
      "Epoch 129/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 590667972608.0000 - mae: 491049.437 - 0s 197us/sample - loss: 693452911486.5288 - mae: 507451.5000 - val_loss: 555065093155.3103 - val_mae: 431613.3438\n",
      "Epoch 130/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 490361585664.0000 - mae: 408881.250 - 0s 191us/sample - loss: 735193411348.5978 - mae: 494710.6250 - val_loss: 555056698144.3678 - val_mae: 431522.1562\n",
      "Epoch 131/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 254865932288.0000 - mae: 332944.781 - 0s 182us/sample - loss: 708259678419.8621 - mae: 481617.2500 - val_loss: 548207041147.5862 - val_mae: 434790.5312\n",
      "Epoch 132/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2654058840064.0000 - mae: 707030.25 - 0s 182us/sample - loss: 767365328283.9540 - mae: 481928.6562 - val_loss: 550315216189.7931 - val_mae: 432111.2500\n",
      "Epoch 133/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 193719435264.0000 - mae: 274894.468 - 0s 167us/sample - loss: 694953178924.1379 - mae: 469466.7188 - val_loss: 547568529549.2414 - val_mae: 432753.2500\n",
      "Epoch 134/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1352765407232.0000 - mae: 746318.00 - 0s 160us/sample - loss: 811311599663.0804 - mae: 499753.3750 - val_loss: 545454986016.3678 - val_mae: 433037.7500\n",
      "Epoch 135/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1066848616448.0000 - mae: 677084.43 - 0s 173us/sample - loss: 724714694585.3793 - mae: 490036.2188 - val_loss: 544667374556.6896 - val_mae: 432353.5625\n",
      "Epoch 136/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 415722438656.0000 - mae: 487326.250 - 0s 194us/sample - loss: 778487050004.5978 - mae: 499781.4375 - val_loss: 546761527154.7586 - val_mae: 430795.7188\n",
      "Epoch 137/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 249747636224.0000 - mae: 343844.562 - 0s 176us/sample - loss: 731839654758.9885 - mae: 494062.0312 - val_loss: 546262126827.4023 - val_mae: 429916.2812\n",
      "Epoch 138/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 592827973632.0000 - mae: 382838.750 - 0s 171us/sample - loss: 747589747252.9655 - mae: 471845.7500 - val_loss: 549838197348.0460 - val_mae: 426840.6562\n",
      "Epoch 139/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 256890109952.0000 - mae: 382828.500 - 0s 211us/sample - loss: 697471704052.2299 - mae: 483366.4375 - val_loss: 545899930635.7701 - val_mae: 427734.4688\n",
      "Epoch 140/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 217219858432.0000 - mae: 341038.906 - 0s 195us/sample - loss: 768253429971.8621 - mae: 484112.7812 - val_loss: 536810082162.7586 - val_mae: 434840.3750\n",
      "Epoch 141/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 755693518848.0000 - mae: 524476.687 - 0s 177us/sample - loss: 709201231118.7126 - mae: 486049.9688 - val_loss: 540940096476.6896 - val_mae: 429511.2188\n",
      "Epoch 142/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 389483954176.0000 - mae: 375159.375 - 0s 197us/sample - loss: 723814191151.0804 - mae: 469513.1562 - val_loss: 541265007015.7241 - val_mae: 428794.3438\n",
      "Epoch 143/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 222390779904.0000 - mae: 359013.656 - 0s 175us/sample - loss: 748875560936.4597 - mae: 498261.1875 - val_loss: 545106187228.6896 - val_mae: 424734.1250\n",
      "Epoch 144/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 505014878208.0000 - mae: 501756.625 - 0s 163us/sample - loss: 705998284164.4138 - mae: 473779.7188 - val_loss: 542763541327.4483 - val_mae: 425324.4688\n",
      "Epoch 145/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 264914501632.0000 - mae: 347649.437 - 0s 212us/sample - loss: 703987812563.8621 - mae: 470694.4688 - val_loss: 539233461824.7357 - val_mae: 427311.0312\n",
      "Epoch 146/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 333088686080.0000 - mae: 354119.000 - 0s 195us/sample - loss: 729214139474.3907 - mae: 500300.1250 - val_loss: 544197465617.6552 - val_mae: 424017.5625\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - ETA: 0s - loss: 274044157952.0000 - mae: 376322.343 - 0s 175us/sample - loss: 707389159965.4253 - mae: 471777.2500 - val_loss: 541174945674.2988 - val_mae: 424801.3750\n",
      "Epoch 148/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 230046826496.0000 - mae: 353862.500 - 0s 180us/sample - loss: 700915704090.4828 - mae: 486268.6562 - val_loss: 544089442139.2184 - val_mae: 423515.1875\n",
      "Epoch 149/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 598290202624.0000 - mae: 503796.906 - 0s 171us/sample - loss: 678931457035.7701 - mae: 485508.7500 - val_loss: 539179652437.3333 - val_mae: 425812.3750\n",
      "Epoch 150/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2188290555904.0000 - mae: 668124.75 - 0s 169us/sample - loss: 703785996711.7241 - mae: 470366.1562 - val_loss: 539597732381.4253 - val_mae: 424673.6875\n",
      "Epoch 151/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2547160186880.0000 - mae: 759349.50 - 0s 178us/sample - loss: 766077402571.0344 - mae: 511808.3750 - val_loss: 543467403993.7471 - val_mae: 422084.8750\n",
      "Epoch 152/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 164164632576.0000 - mae: 298160.687 - 0s 189us/sample - loss: 746744489913.3793 - mae: 469607.5312 - val_loss: 536999258606.3448 - val_mae: 425216.5938\n",
      "Epoch 153/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2943451398144.0000 - mae: 697686.31 - 0s 178us/sample - loss: 731228831744.0000 - mae: 469111.1250 - val_loss: 535978211116.1380 - val_mae: 425475.0938\n",
      "Epoch 154/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 299003445248.0000 - mae: 372237.812 - 0s 207us/sample - loss: 789997638997.3333 - mae: 497013.1875 - val_loss: 541854311059.1265 - val_mae: 422003.2500\n",
      "Epoch 155/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 339644579840.0000 - mae: 383472.062 - 0s 173us/sample - loss: 756245704468.5978 - mae: 496335.0938 - val_loss: 534168053501.0575 - val_mae: 425246.7188\n",
      "Epoch 156/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 666296975360.0000 - mae: 493952.062 - 0s 178us/sample - loss: 732719275937.8391 - mae: 481441.8750 - val_loss: 533561316881.6552 - val_mae: 425189.7812\n",
      "Epoch 157/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 688712450048.0000 - mae: 558480.312 - 0s 178us/sample - loss: 705624515030.8046 - mae: 474550.4062 - val_loss: 533584720954.8506 - val_mae: 424825.0000\n",
      "Epoch 158/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 561179983872.0000 - mae: 476104.562 - 0s 187us/sample - loss: 724248439090.0229 - mae: 476968.5000 - val_loss: 538447751285.7012 - val_mae: 421183.8750\n",
      "Epoch 159/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 974475624448.0000 - mae: 523295.937 - 0s 163us/sample - loss: 760391655329.8391 - mae: 470735.1875 - val_loss: 538764818408.4598 - val_mae: 420782.4688\n",
      "Epoch 160/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 673002684416.0000 - mae: 434183.781 - 0s 245us/sample - loss: 740790313807.4484 - mae: 478575.7188 - val_loss: 541239196942.7126 - val_mae: 420031.4375\n",
      "Epoch 161/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 497588961280.0000 - mae: 427919.937 - 0s 207us/sample - loss: 736220853495.1724 - mae: 479790.8125 - val_loss: 541817309207.5402 - val_mae: 419490.7188\n",
      "Epoch 162/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 920447746048.0000 - mae: 616283.500 - 0s 182us/sample - loss: 733886592623.8160 - mae: 471603.1875 - val_loss: 534661227343.4483 - val_mae: 421997.5312\n",
      "Epoch 163/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 836382294016.0000 - mae: 491729.500 - 0s 188us/sample - loss: 737371744373.7012 - mae: 474206.4688 - val_loss: 536160002142.1609 - val_mae: 420831.5000\n",
      "Epoch 164/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 438504587264.0000 - mae: 478449.125 - 0s 201us/sample - loss: 687953772732.3218 - mae: 452932.6875 - val_loss: 531170100518.2529 - val_mae: 423458.2188\n",
      "Epoch 165/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 237393199104.0000 - mae: 385653.062 - 0s 175us/sample - loss: 693312369157.8850 - mae: 468656.0938 - val_loss: 530357282933.7012 - val_mae: 423440.6562\n",
      "Epoch 166/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 485275959296.0000 - mae: 462642.875 - 0s 166us/sample - loss: 741158724054.8046 - mae: 488564.4688 - val_loss: 533086989723.9540 - val_mae: 421530.5625\n",
      "Epoch 167/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 481729445888.0000 - mae: 519493.312 - 0s 191us/sample - loss: 736686186590.1609 - mae: 485548.3125 - val_loss: 536199360841.5632 - val_mae: 419710.1250\n",
      "Epoch 168/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 591376351232.0000 - mae: 477126.375 - 0s 211us/sample - loss: 759485666457.0115 - mae: 491415.2500 - val_loss: 540091296426.6667 - val_mae: 419065.5625\n",
      "Epoch 169/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 354008956928.0000 - mae: 379136.500 - 0s 200us/sample - loss: 716636602556.3218 - mae: 472552.5000 - val_loss: 533301664379.5862 - val_mae: 420058.0625\n",
      "Epoch 170/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 257191444480.0000 - mae: 359265.562 - 0s 207us/sample - loss: 672344068613.8850 - mae: 461574.6562 - val_loss: 534758155840.7357 - val_mae: 419162.7188\n",
      "Epoch 171/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 410153713664.0000 - mae: 438860.250 - 0s 190us/sample - loss: 751224863073.1034 - mae: 486669.5625 - val_loss: 534630855927.1724 - val_mae: 418839.3438\n",
      "Epoch 172/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 292918919168.0000 - mae: 346855.187 - 0s 166us/sample - loss: 777930850868.9656 - mae: 497172.5938 - val_loss: 535199673190.9886 - val_mae: 418196.4688\n",
      "Epoch 173/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 262719078400.0000 - mae: 368308.687 - 0s 170us/sample - loss: 754143517872.5516 - mae: 477837.3750 - val_loss: 535917271675.5862 - val_mae: 417713.4688\n",
      "Epoch 174/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 334853636096.0000 - mae: 437293.250 - 0s 190us/sample - loss: 778571449826.5748 - mae: 491517.3438 - val_loss: 534623767869.7931 - val_mae: 417190.4688\n",
      "Epoch 175/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 655627780096.0000 - mae: 452020.593 - 0s 188us/sample - loss: 724248673362.3907 - mae: 481461.0625 - val_loss: 525815657907.4943 - val_mae: 421470.7500\n",
      "Epoch 176/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 442976829440.0000 - mae: 461254.781 - 0s 169us/sample - loss: 673232256459.0345 - mae: 459266.7500 - val_loss: 528001109686.4368 - val_mae: 418905.5312\n",
      "Epoch 177/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2228947255296.0000 - mae: 668978.81 - 0s 172us/sample - loss: 688142854073.3793 - mae: 493890.5625 - val_loss: 531092097612.5057 - val_mae: 417813.8750\n",
      "Epoch 178/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 319381110784.0000 - mae: 409090.875 - 0s 179us/sample - loss: 687722904281.7471 - mae: 476092.9688 - val_loss: 529984696272.9196 - val_mae: 418133.3750\n",
      "Epoch 179/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 838027182080.0000 - mae: 452661.125 - 0s 191us/sample - loss: 720379656168.4597 - mae: 482195.2500 - val_loss: 529275356265.9310 - val_mae: 417996.3750\n",
      "Epoch 180/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 499675627520.0000 - mae: 402872.718 - 0s 178us/sample - loss: 770884574572.8735 - mae: 496393.5938 - val_loss: 532605330820.4138 - val_mae: 416657.5312\n",
      "Epoch 181/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 449508900864.0000 - mae: 486920.281 - 0s 188us/sample - loss: 643558567888.9196 - mae: 451653.5312 - val_loss: 525592393704.4598 - val_mae: 418471.4062\n",
      "Epoch 182/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2312918532096.0000 - mae: 675283.50 - 0s 167us/sample - loss: 683738021134.7126 - mae: 458891.1875 - val_loss: 528937502178.5747 - val_mae: 417355.8125\n",
      "Epoch 183/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - ETA: 0s - loss: 344719228928.0000 - mae: 446068.031 - 0s 196us/sample - loss: 706415662609.6552 - mae: 488346.5625 - val_loss: 525793811820.8735 - val_mae: 418552.9688\n",
      "Epoch 184/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 281912082432.0000 - mae: 406659.562 - 0s 175us/sample - loss: 729889260649.9310 - mae: 495363.5938 - val_loss: 531459159663.8161 - val_mae: 416727.7188\n",
      "Epoch 185/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2263939547136.0000 - mae: 643691.75 - 0s 179us/sample - loss: 689101351218.0229 - mae: 459939.0312 - val_loss: 526101527069.4253 - val_mae: 417671.3438\n",
      "Epoch 186/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 562968395776.0000 - mae: 429464.625 - 0s 178us/sample - loss: 710599199873.4712 - mae: 483004.0938 - val_loss: 525208746337.1035 - val_mae: 418577.0000\n",
      "Epoch 187/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 435364364288.0000 - mae: 418127.437 - 0s 192us/sample - loss: 654295482744.6437 - mae: 446108.3125 - val_loss: 521919542707.4943 - val_mae: 419925.3438\n",
      "Epoch 188/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 863106760704.0000 - mae: 533130.750 - 0s 256us/sample - loss: 658192893787.2184 - mae: 461092.6562 - val_loss: 527232258848.3678 - val_mae: 417065.0000\n",
      "Epoch 189/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 661671641088.0000 - mae: 475116.437 - 0s 179us/sample - loss: 731464892039.3563 - mae: 483646.9062 - val_loss: 528901075650.2069 - val_mae: 416718.7500\n",
      "Epoch 190/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 384701497344.0000 - mae: 417864.125 - 0s 181us/sample - loss: 730537520728.2759 - mae: 480535.2500 - val_loss: 533341864312.6437 - val_mae: 415783.5938\n",
      "Epoch 191/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 691910475776.0000 - mae: 489894.343 - 0s 179us/sample - loss: 730062916172.5057 - mae: 475412.5938 - val_loss: 525702287595.4023 - val_mae: 416751.3125\n",
      "Epoch 192/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1465988153344.0000 - mae: 734667.93 - 0s 170us/sample - loss: 733174969591.1724 - mae: 482270.2188 - val_loss: 529208569243.9540 - val_mae: 416004.3125\n",
      "Epoch 193/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 601650102272.0000 - mae: 496143.093 - 0s 179us/sample - loss: 642329319682.9425 - mae: 455983.1250 - val_loss: 530763343048.0920 - val_mae: 415701.5312\n",
      "Epoch 194/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 690703368192.0000 - mae: 554486.937 - 0s 165us/sample - loss: 623563680803.3103 - mae: 447103.5000 - val_loss: 527489974083.6782 - val_mae: 415792.0000\n",
      "Epoch 195/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 3347833159680.0000 - mae: 967768.25 - 0s 177us/sample - loss: 723162608133.8850 - mae: 475449.0938 - val_loss: 529708948656.5517 - val_mae: 415790.3438\n",
      "Epoch 196/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1222387695616.0000 - mae: 629660.12 - 0s 174us/sample - loss: 729748846568.4597 - mae: 477523.2188 - val_loss: 525181767350.4368 - val_mae: 416353.9375\n",
      "Epoch 197/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1280946601984.0000 - mae: 553653.93 - 0s 160us/sample - loss: 733363982194.7587 - mae: 479586.1250 - val_loss: 519869442165.7012 - val_mae: 418926.0312\n",
      "Epoch 198/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2418741608448.0000 - mae: 732416.68 - 0s 171us/sample - loss: 748764478923.0344 - mae: 498839.4375 - val_loss: 527814799877.8851 - val_mae: 415686.3438\n",
      "Epoch 199/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 396782338048.0000 - mae: 441243.281 - 0s 172us/sample - loss: 748749070053.5173 - mae: 481347.6875 - val_loss: 527544073110.0690 - val_mae: 414912.2812\n",
      "Epoch 200/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 534198419456.0000 - mae: 499770.218 - 0s 166us/sample - loss: 703721508534.4368 - mae: 469263.6250 - val_loss: 521346671439.4483 - val_mae: 416913.1875\n",
      "Epoch 201/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 181427765248.0000 - mae: 276458.656 - 0s 251us/sample - loss: 697256039388.6897 - mae: 468359.1250 - val_loss: 519769760968.0920 - val_mae: 417830.2188\n",
      "Epoch 202/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 373373403136.0000 - mae: 437223.000 - 0s 219us/sample - loss: 727906898155.4022 - mae: 483746.2500 - val_loss: 526749218227.4943 - val_mae: 415069.9688\n",
      "Epoch 203/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 689141645312.0000 - mae: 552494.875 - 0s 186us/sample - loss: 664330048665.0115 - mae: 456254.5312 - val_loss: 527935536963.6782 - val_mae: 415163.0312\n",
      "Epoch 204/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 349562863616.0000 - mae: 449749.312 - 0s 179us/sample - loss: 735474572087.9081 - mae: 503542.1562 - val_loss: 531596722364.3218 - val_mae: 415089.1875\n",
      "Epoch 205/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 812842811392.0000 - mae: 549517.500 - 0s 177us/sample - loss: 737653420361.5632 - mae: 476277.5312 - val_loss: 528675502056.4598 - val_mae: 415463.1875\n",
      "Epoch 206/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 678391840768.0000 - mae: 529248.875 - 0s 191us/sample - loss: 698399633549.2413 - mae: 468094.9375 - val_loss: 519430808587.7701 - val_mae: 417984.5625\n",
      "Epoch 207/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 963870851072.0000 - mae: 578533.250 - 0s 182us/sample - loss: 687058040431.8160 - mae: 468017.0938 - val_loss: 519244808380.3218 - val_mae: 417945.3438\n",
      "Epoch 208/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 527914565632.0000 - mae: 470248.281 - 0s 167us/sample - loss: 700135165434.1150 - mae: 478796.8750 - val_loss: 518862371357.4253 - val_mae: 417605.5625\n",
      "Epoch 209/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 646038749184.0000 - mae: 521684.656 - 0s 173us/sample - loss: 704594811892.2299 - mae: 484589.6562 - val_loss: 520916547372.1380 - val_mae: 416197.8750\n",
      "Epoch 210/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 594942230528.0000 - mae: 475631.125 - 0s 173us/sample - loss: 645028310616.2759 - mae: 456407.5000 - val_loss: 519797264242.7586 - val_mae: 417009.9688\n",
      "Epoch 211/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 287082348544.0000 - mae: 362043.218 - 0s 196us/sample - loss: 699158426682.8506 - mae: 472790.6562 - val_loss: 521975502541.9770 - val_mae: 415838.4375\n",
      "Epoch 212/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 252884746240.0000 - mae: 328773.312 - 0s 157us/sample - loss: 683261533360.5516 - mae: 477514.9375 - val_loss: 528077982472.8276 - val_mae: 415147.2188\n",
      "Epoch 213/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 146658197504.0000 - mae: 261554.406 - 0s 170us/sample - loss: 692110843056.5516 - mae: 468988.9688 - val_loss: 519426908254.1609 - val_mae: 416649.5312\n",
      "Epoch 214/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 974556102656.0000 - mae: 573818.125 - 0s 202us/sample - loss: 754521811061.7012 - mae: 492484.3750 - val_loss: 523782132324.0460 - val_mae: 415716.1250\n",
      "Epoch 215/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 473155043328.0000 - mae: 421694.437 - 0s 204us/sample - loss: 728382917008.1840 - mae: 470558.1250 - val_loss: 520201068638.1609 - val_mae: 416277.9688\n",
      "Epoch 216/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2208323862528.0000 - mae: 615375.87 - 0s 179us/sample - loss: 669499056787.1265 - mae: 469577.1562 - val_loss: 524551565076.5977 - val_mae: 414923.5312\n",
      "Epoch 217/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 825771556864.0000 - mae: 529661.000 - 0s 189us/sample - loss: 700681505921.4712 - mae: 460208.9062 - val_loss: 521043653502.5287 - val_mae: 415754.9375\n",
      "Epoch 218/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 491022385152.0000 - mae: 430889.000 - 0s 186us/sample - loss: 681165070430.1609 - mae: 450960.4688 - val_loss: 520827386962.3908 - val_mae: 415477.4688\n",
      "Epoch 219/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - ETA: 0s - loss: 546671362048.0000 - mae: 416112.531 - 0s 186us/sample - loss: 726479789232.5516 - mae: 479447.0312 - val_loss: 521137211038.8965 - val_mae: 414929.5625\n",
      "Epoch 220/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2464339722240.0000 - mae: 707457.81 - 0s 175us/sample - loss: 695395358225.6552 - mae: 485734.5312 - val_loss: 523527574869.3333 - val_mae: 414098.8125\n",
      "Epoch 221/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 682193453056.0000 - mae: 561613.500 - 0s 163us/sample - loss: 732107613384.0919 - mae: 493142.9062 - val_loss: 527183560139.0345 - val_mae: 413669.0938\n",
      "Epoch 222/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 468557365248.0000 - mae: 349715.343 - 0s 155us/sample - loss: 679378640048.5516 - mae: 451516.1875 - val_loss: 518696978267.2184 - val_mae: 415599.5312\n",
      "Epoch 223/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 942441431040.0000 - mae: 585128.312 - 0s 186us/sample - loss: 727486939700.9655 - mae: 482057.3438 - val_loss: 517960884694.8046 - val_mae: 415968.5000\n",
      "Epoch 224/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 599276519424.0000 - mae: 506730.031 - 0s 162us/sample - loss: 687628227925.3334 - mae: 482408.2812 - val_loss: 520983651716.4138 - val_mae: 414246.6562\n",
      "Epoch 225/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 191009357824.0000 - mae: 299898.750 - 0s 175us/sample - loss: 666036775971.3103 - mae: 454470.5625 - val_loss: 517752908458.6667 - val_mae: 415479.2500\n",
      "Epoch 226/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 333187088384.0000 - mae: 444195.218 - 0s 191us/sample - loss: 695110107889.2874 - mae: 495158.0312 - val_loss: 518280490584.2759 - val_mae: 415244.0000\n",
      "Epoch 227/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 399066169344.0000 - mae: 446455.437 - 0s 177us/sample - loss: 713308472802.5747 - mae: 471001.5312 - val_loss: 519889872825.3793 - val_mae: 414601.7500\n",
      "Epoch 228/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 509696212992.0000 - mae: 460690.375 - 0s 184us/sample - loss: 646774496997.5172 - mae: 458469.6562 - val_loss: 518229531636.2299 - val_mae: 414848.5938\n",
      "Epoch 229/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 477181607936.0000 - mae: 455574.562 - 0s 160us/sample - loss: 624535476106.2988 - mae: 467809.0938 - val_loss: 519368322648.2759 - val_mae: 414189.7812\n",
      "Epoch 230/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 601545375744.0000 - mae: 446605.562 - 0s 170us/sample - loss: 768980200106.6667 - mae: 488171.8125 - val_loss: 517908861728.3678 - val_mae: 413872.0938\n",
      "Epoch 231/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 890849460224.0000 - mae: 628261.812 - 0s 187us/sample - loss: 724478230528.0000 - mae: 464147.3125 - val_loss: 517265882571.0345 - val_mae: 414098.4375\n",
      "Epoch 232/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 326555598848.0000 - mae: 425196.250 - 0s 179us/sample - loss: 724950383886.7126 - mae: 488366.5625 - val_loss: 517622731740.6896 - val_mae: 413544.5938\n",
      "Epoch 233/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 757455781888.0000 - mae: 544771.437 - 0s 182us/sample - loss: 673162218248.8276 - mae: 447038.9062 - val_loss: 518325138302.5287 - val_mae: 412862.6250\n",
      "Epoch 234/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 599835738112.0000 - mae: 450470.187 - 0s 172us/sample - loss: 723530915557.5172 - mae: 479445.0938 - val_loss: 518469699230.8965 - val_mae: 413431.5000\n",
      "Epoch 235/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 983538073600.0000 - mae: 683332.875 - 0s 172us/sample - loss: 683546439444.5978 - mae: 468262.4688 - val_loss: 524465056991.6322 - val_mae: 412093.8438\n",
      "Epoch 236/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 289541521408.0000 - mae: 400570.468 - 0s 179us/sample - loss: 689668975462.9885 - mae: 467218.3125 - val_loss: 522554778259.1265 - val_mae: 411556.1875\n",
      "Epoch 237/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 191152144384.0000 - mae: 327744.937 - 0s 167us/sample - loss: 623952692329.9310 - mae: 467944.1875 - val_loss: 516162658021.5172 - val_mae: 412640.7812\n",
      "Epoch 238/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 960592216064.0000 - mae: 602202.875 - 0s 171us/sample - loss: 715762129154.9425 - mae: 472376.0938 - val_loss: 518569204535.9080 - val_mae: 411591.3438\n",
      "Epoch 239/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2609737105408.0000 - mae: 714293.75 - 0s 169us/sample - loss: 721556827359.6322 - mae: 479355.7812 - val_loss: 514975577900.1380 - val_mae: 412294.3438\n",
      "Epoch 240/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 503604576256.0000 - mae: 468939.156 - 0s 169us/sample - loss: 668618072158.1609 - mae: 445038.0312 - val_loss: 512134270646.4368 - val_mae: 415187.3438\n",
      "Epoch 241/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 726297477120.0000 - mae: 462572.500 - 0s 210us/sample - loss: 734191873247.6322 - mae: 478149.3438 - val_loss: 512706087312.1839 - val_mae: 414575.6250\n",
      "Epoch 242/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 903887323136.0000 - mae: 541638.000 - 0s 195us/sample - loss: 734455410452.5978 - mae: 487271.8750 - val_loss: 517160915932.6896 - val_mae: 412532.1250\n",
      "Epoch 243/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 979566592000.0000 - mae: 554234.437 - 0s 182us/sample - loss: 661988178461.4253 - mae: 466495.2500 - val_loss: 513021724507.2184 - val_mae: 414277.3750\n",
      "Epoch 244/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 266624860160.0000 - mae: 397176.156 - 0s 192us/sample - loss: 721924130274.5747 - mae: 480096.8125 - val_loss: 515939657280.7357 - val_mae: 413465.9375\n",
      "Epoch 245/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 402280873984.0000 - mae: 447406.062 - 0s 187us/sample - loss: 659625896053.7012 - mae: 451652.8750 - val_loss: 511622129593.3793 - val_mae: 417335.9062\n",
      "Epoch 246/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 737203585024.0000 - mae: 546991.375 - 0s 175us/sample - loss: 682962713117.4253 - mae: 461986.4688 - val_loss: 512148608341.3333 - val_mae: 415421.8750\n",
      "Epoch 247/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 337550475264.0000 - mae: 431737.968 - 0s 165us/sample - loss: 636052873310.1609 - mae: 468083.1250 - val_loss: 512473675187.4943 - val_mae: 414679.0312\n",
      "Epoch 248/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 336520282112.0000 - mae: 389629.968 - 0s 187us/sample - loss: 741778532281.3793 - mae: 489903.7188 - val_loss: 512498074541.6092 - val_mae: 415014.4375\n",
      "Epoch 249/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 452850581504.0000 - mae: 498333.468 - 0s 170us/sample - loss: 658871288937.9310 - mae: 480951.0000 - val_loss: 516432748261.5172 - val_mae: 412618.5625\n",
      "Epoch 250/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 624535273472.0000 - mae: 573939.625 - 0s 157us/sample - loss: 681875173328.9196 - mae: 472209.8438 - val_loss: 517301541122.9425 - val_mae: 412194.9375\n",
      "Epoch 251/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 331860770816.0000 - mae: 359533.375 - 0s 162us/sample - loss: 677159368868.7816 - mae: 458757.2812 - val_loss: 513364794921.1954 - val_mae: 413123.0938\n",
      "Epoch 252/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 513050574848.0000 - mae: 503218.250 - 0s 171us/sample - loss: 741450365281.1035 - mae: 478716.3125 - val_loss: 517100961603.6782 - val_mae: 411849.6562\n",
      "Epoch 253/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 212758511616.0000 - mae: 301651.593 - 0s 169us/sample - loss: 681337215988.2299 - mae: 453791.7188 - val_loss: 513927998875.9540 - val_mae: 412313.8438\n",
      "Epoch 254/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 260047798272.0000 - mae: 386921.125 - 0s 161us/sample - loss: 744109186412.8735 - mae: 480120.3750 - val_loss: 514876482948.4138 - val_mae: 411790.7188\n",
      "Epoch 255/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - ETA: 0s - loss: 99460677632.0000 - mae: 235865.53 - 0s 164us/sample - loss: 682929027990.0690 - mae: 476401.9375 - val_loss: 521214761407.2643 - val_mae: 411367.8750\n",
      "Epoch 256/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 709108432896.0000 - mae: 522221.250 - 0s 200us/sample - loss: 688981330696.8276 - mae: 473606.2188 - val_loss: 511404641221.1494 - val_mae: 412521.1875\n",
      "Epoch 257/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 211686686720.0000 - mae: 333992.531 - 0s 176us/sample - loss: 671915445565.7931 - mae: 460204.6562 - val_loss: 510854554259.1265 - val_mae: 412629.4375\n",
      "Epoch 258/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 641503592448.0000 - mae: 536519.500 - 0s 159us/sample - loss: 705531404193.8391 - mae: 464825.2500 - val_loss: 512667959107.6782 - val_mae: 412315.7812\n",
      "Epoch 259/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 520418918400.0000 - mae: 394834.437 - 0s 161us/sample - loss: 692855448587.7701 - mae: 463096.5938 - val_loss: 514069582812.6896 - val_mae: 411722.1250\n",
      "Epoch 260/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 564933820416.0000 - mae: 473021.156 - 0s 172us/sample - loss: 708126220005.5172 - mae: 472360.7812 - val_loss: 513225342034.3908 - val_mae: 412013.9688\n",
      "Epoch 261/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 199549124608.0000 - mae: 282297.250 - 0s 180us/sample - loss: 646844740290.2069 - mae: 453042.4688 - val_loss: 509022762607.8161 - val_mae: 414059.5938\n",
      "Epoch 262/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 874462183424.0000 - mae: 603942.375 - 0s 194us/sample - loss: 680747667503.0804 - mae: 475090.5625 - val_loss: 513819471071.6322 - val_mae: 411440.9062\n",
      "Epoch 263/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 541120856064.0000 - mae: 467790.812 - 0s 193us/sample - loss: 746305525642.2988 - mae: 476815.0938 - val_loss: 520817031521.1035 - val_mae: 409886.9062\n",
      "Epoch 264/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 524918423552.0000 - mae: 469668.500 - 0s 171us/sample - loss: 777209189529.0115 - mae: 476278.4688 - val_loss: 513155959937.4713 - val_mae: 410884.0938\n",
      "Epoch 265/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 200446525440.0000 - mae: 319154.656 - 0s 197us/sample - loss: 651384585486.7126 - mae: 440571.7188 - val_loss: 515595109293.6092 - val_mae: 410633.9688\n",
      "Epoch 266/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 530249089024.0000 - mae: 509493.406 - 0s 179us/sample - loss: 779589020895.6322 - mae: 496054.0312 - val_loss: 516396763347.8620 - val_mae: 410392.7500\n",
      "Epoch 267/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 232483323904.0000 - mae: 332397.875 - 0s 183us/sample - loss: 690153773879.9081 - mae: 448919.6875 - val_loss: 513737224709.8851 - val_mae: 410462.3125\n",
      "Epoch 268/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 286758141952.0000 - mae: 386195.312 - 0s 184us/sample - loss: 694179811563.4022 - mae: 461765.9688 - val_loss: 514085345538.9425 - val_mae: 410358.0625\n",
      "Epoch 269/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 597219016704.0000 - mae: 487113.656 - 0s 194us/sample - loss: 620699413539.3103 - mae: 467153.5625 - val_loss: 510786511883.7701 - val_mae: 411781.0000\n",
      "Epoch 270/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 233818914816.0000 - mae: 357333.812 - 0s 205us/sample - loss: 708236343084.1379 - mae: 477309.7812 - val_loss: 509756986521.0114 - val_mae: 413116.4062\n",
      "Epoch 271/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 213888204800.0000 - mae: 316355.750 - 0s 190us/sample - loss: 672825405957.8850 - mae: 471644.7500 - val_loss: 512607335294.5287 - val_mae: 411791.5312\n",
      "Epoch 272/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 344832278528.0000 - mae: 434185.312 - 0s 161us/sample - loss: 655646080835.6782 - mae: 451750.0625 - val_loss: 509332893837.2414 - val_mae: 413700.0938\n",
      "Epoch 273/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 681791193088.0000 - mae: 478224.750 - 0s 161us/sample - loss: 650698849550.7126 - mae: 452878.7188 - val_loss: 512023339667.1265 - val_mae: 411514.5625\n",
      "Epoch 274/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 650647830528.0000 - mae: 447209.468 - 0s 149us/sample - loss: 710070648267.0345 - mae: 467145.5938 - val_loss: 514741006665.5632 - val_mae: 411035.3438\n",
      "Epoch 275/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 729729925120.0000 - mae: 462558.937 - 0s 158us/sample - loss: 754174701709.2413 - mae: 483844.4062 - val_loss: 513486857227.7701 - val_mae: 411182.9062\n",
      "Epoch 276/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 649126608896.0000 - mae: 448709.218 - 0s 159us/sample - loss: 740683877693.7931 - mae: 479662.3125 - val_loss: 511779651584.0000 - val_mae: 411684.5000\n",
      "Epoch 277/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 373010989056.0000 - mae: 416639.500 - 0s 177us/sample - loss: 656610833655.1724 - mae: 450554.4062 - val_loss: 509947891194.1149 - val_mae: 412586.1562\n",
      "Epoch 278/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 294166822912.0000 - mae: 390299.500 - 0s 166us/sample - loss: 655037744987.2184 - mae: 478450.3125 - val_loss: 513224176887.1724 - val_mae: 411187.1250\n",
      "Epoch 279/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 693989998592.0000 - mae: 444492.125 - 0s 192us/sample - loss: 721571480681.9310 - mae: 471290.6562 - val_loss: 519243424214.8046 - val_mae: 410023.8125\n",
      "Epoch 280/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 2452669595648.0000 - mae: 667174.00 - 0s 163us/sample - loss: 681783236866.9425 - mae: 463169.3750 - val_loss: 512372030546.3908 - val_mae: 410905.9688\n",
      "Epoch 281/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 225495269376.0000 - mae: 340608.125 - 0s 196us/sample - loss: 694311542313.1954 - mae: 440868.0938 - val_loss: 508796905778.0230 - val_mae: 413174.6250\n",
      "Epoch 282/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 130073354240.0000 - mae: 256707.406 - 0s 179us/sample - loss: 660463529371.9540 - mae: 458038.1250 - val_loss: 509630824824.6437 - val_mae: 411487.2188\n",
      "Epoch 283/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 253477306368.0000 - mae: 362661.562 - 0s 164us/sample - loss: 733797346150.9885 - mae: 481709.3750 - val_loss: 513102942631.7241 - val_mae: 410293.3750\n",
      "Epoch 284/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 456375336960.0000 - mae: 494137.156 - 0s 160us/sample - loss: 632563972449.1035 - mae: 446776.8125 - val_loss: 517536888996.7816 - val_mae: 408868.7812\n",
      "Epoch 285/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 298958323712.0000 - mae: 365900.187 - 0s 183us/sample - loss: 683011528397.9771 - mae: 464683.7812 - val_loss: 514363700094.5287 - val_mae: 409164.2812\n",
      "Epoch 286/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 314997145600.0000 - mae: 391102.812 - 0s 178us/sample - loss: 713744553866.2988 - mae: 469053.0938 - val_loss: 511764146293.7012 - val_mae: 409844.1250\n",
      "Epoch 287/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 649425453056.0000 - mae: 550553.937 - 0s 163us/sample - loss: 747929702447.0804 - mae: 476531.3125 - val_loss: 510431642541.6092 - val_mae: 410893.8438\n",
      "Epoch 288/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 192321929216.0000 - mae: 308562.781 - 0s 171us/sample - loss: 690050237357.6093 - mae: 467702.5625 - val_loss: 512003706550.4368 - val_mae: 409942.8438\n",
      "Epoch 289/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 251359985664.0000 - mae: 366422.843 - 0s 174us/sample - loss: 721256875113.9310 - mae: 479164.8125 - val_loss: 511032242882.2069 - val_mae: 410505.8750\n",
      "Epoch 290/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 502302179328.0000 - mae: 438396.000 - 0s 193us/sample - loss: 744874094733.2413 - mae: 480085.7500 - val_loss: 515397326165.3333 - val_mae: 409028.9688\n",
      "Epoch 291/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - ETA: 0s - loss: 449379958784.0000 - mae: 371803.218 - 0s 182us/sample - loss: 694361695055.4484 - mae: 458248.8125 - val_loss: 512100033547.7701 - val_mae: 409779.0000\n",
      "Epoch 292/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 712112930816.0000 - mae: 565655.625 - 0s 175us/sample - loss: 704118260088.6437 - mae: 469432.6875 - val_loss: 520249275639.1724 - val_mae: 409230.2500\n",
      "Epoch 293/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 383441240064.0000 - mae: 384675.031 - 0s 188us/sample - loss: 715545660922.1150 - mae: 476444.3750 - val_loss: 527128780705.8391 - val_mae: 409659.9062\n",
      "Epoch 294/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 100205895680.0000 - mae: 236200.703 - 0s 177us/sample - loss: 683351299789.9771 - mae: 450746.4062 - val_loss: 511122871260.6896 - val_mae: 410033.2812\n",
      "Epoch 295/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 425516892160.0000 - mae: 382408.343 - 0s 195us/sample - loss: 701454960286.8965 - mae: 456509.9688 - val_loss: 520349087155.4943 - val_mae: 408459.6250\n",
      "Epoch 296/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 219018559488.0000 - mae: 339043.906 - 0s 181us/sample - loss: 706708692144.5516 - mae: 473046.1250 - val_loss: 520217172038.6207 - val_mae: 408152.4688\n",
      "Epoch 297/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 205360726016.0000 - mae: 297196.250 - 0s 184us/sample - loss: 681028354801.2874 - mae: 458346.1562 - val_loss: 512727604777.1954 - val_mae: 408187.9062\n",
      "Epoch 298/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 522240000000.0000 - mae: 479384.125 - 0s 182us/sample - loss: 711684824052.2299 - mae: 466950.9062 - val_loss: 509831710366.8965 - val_mae: 409374.0625\n",
      "Epoch 299/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 358300712960.0000 - mae: 445118.750 - 0s 174us/sample - loss: 662733674001.6552 - mae: 460583.7188 - val_loss: 509562003738.4828 - val_mae: 409531.5938\n",
      "Epoch 300/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 3023936159744.0000 - mae: 770990.68 - 0s 185us/sample - loss: 703287436111.4484 - mae: 466736.0312 - val_loss: 512598319763.1265 - val_mae: 409178.8438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13146), started 3 days, 1:00:53 ago. (Use '!kill 13146' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3b8e9fb3d2c49acd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3b8e9fb3d2c49acd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Best Model hyperparams\n",
    "#|-learning_rate: 0.01\n",
    "#|-num_layers: 3\n",
    "#|-units_0: 80\n",
    "#|-units_1: 80\n",
    "#|-units_2: 64\n",
    "#|-units_3: 80\n",
    "#|-units_4: 64\n",
    "!rm -rf ./logs\n",
    "tuned_model = keras.Sequential()\n",
    "tuned_model.add(layers.Dense(80, activation = 'relu', input_shape = (15,)))\n",
    "tuned_model.add(layers.Dropout(0.5))\n",
    "tuned_model.add(layers.Dense(80, activation = 'relu'))\n",
    "tuned_model.add(layers.Dropout(0.4))\n",
    "tuned_model.add(layers.Dense(64, activation = 'relu'))\n",
    "tuned_model.add(layers.Dropout(0.3))\n",
    "tuned_model.add(layers.Dense(80, activation = 'relu'))\n",
    "tuned_model.add(layers.Dropout(0.2))\n",
    "tuned_model.add(layers.Dense(64, activation = 'relu'))\n",
    "tuned_model.add(layers.Dropout(0.2))\n",
    "tuned_model.add(layers.Dense(1))\n",
    "tuned_model.compile(learning_rate = 0.01, loss = 'mean_squared_error', metrics = ['mae'])\n",
    "tuned_model.fit(x=train_x, y=train_y, epochs=300, validation_split=0.5, callbacks = [tensorboard_callback])\n",
    "# Clear any logs from previous runs\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test_small_provinces_modified.csv')\n",
    "test_id = test_df.id\n",
    "test_df = test_df.drop(columns=['id', 'usosmultiples'])\n",
    "#test_df.antiguedad = scaler.fit_transform(np.array(test_df.antiguedad).reshape(-1, 1))\n",
    "#test_df.metroscubiertos = scaler.fit_transform(np.array(test_df.metroscubiertos).reshape(-1, 1))\n",
    "#test_df.metrostotales = scaler.fit_transform(np.array(test_df.metrostotales).reshape(-1, 1))\n",
    "#test_df.habitaciones = scaler.fit_transform(np.array(test_df.habitaciones).reshape(-1, 1))\n",
    "#test_df.garages = scaler.fit_transform(np.array(test_df.garages).reshape(-1, 1))\n",
    "#test_df.banos = scaler.fit_transform(np.array(test_df.banos).reshape(-1, 1))\n",
    "test_df = test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (15,) but got array with shape (16,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-17bc54991bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuned_model_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuned_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    572\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (15,) but got array with shape (16,)"
     ]
    }
   ],
   "source": [
    "tuned_model_prediction = tuned_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tuned_model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1471112.2 ],\n",
       "       [1525055.6 ],\n",
       "       [ 563851.8 ],\n",
       "       [1242546.4 ],\n",
       "       [1325111.  ],\n",
       "       [1921076.9 ],\n",
       "       [ 446258.1 ],\n",
       "       [ 902801.56],\n",
       "       [ 740866.4 ],\n",
       "       [1416326.9 ],\n",
       "       [ 644496.25],\n",
       "       [2480247.5 ],\n",
       "       [2288037.5 ],\n",
       "       [ 315806.9 ],\n",
       "       [1755442.  ],\n",
       "       [1155330.8 ],\n",
       "       [1079785.8 ],\n",
       "       [ 379200.8 ],\n",
       "       [2224810.5 ],\n",
       "       [ 430990.4 ],\n",
       "       [ 355249.9 ],\n",
       "       [ 349252.5 ],\n",
       "       [2689153.8 ],\n",
       "       [ 601380.  ],\n",
       "       [ 404526.66],\n",
       "       [ 418033.7 ],\n",
       "       [2520592.  ],\n",
       "       [ 328255.34],\n",
       "       [ 708386.1 ],\n",
       "       [ 553520.56],\n",
       "       [1134750.4 ],\n",
       "       [ 356281.44],\n",
       "       [ 589321.2 ],\n",
       "       [1886140.8 ],\n",
       "       [ 743205.3 ],\n",
       "       [1421487.6 ],\n",
       "       [ 633412.94],\n",
       "       [2790108.2 ],\n",
       "       [ 686965.8 ],\n",
       "       [ 518003.8 ],\n",
       "       [1912045.6 ],\n",
       "       [ 299813.28],\n",
       "       [ 428579.78],\n",
       "       [ 409579.9 ],\n",
       "       [1989099.8 ],\n",
       "       [ 642730.56],\n",
       "       [1279524.4 ],\n",
       "       [2591228.  ],\n",
       "       [ 388148.56],\n",
       "       [1143834.5 ],\n",
       "       [ 536044.9 ],\n",
       "       [ 756750.6 ],\n",
       "       [1440773.2 ],\n",
       "       [1110297.6 ],\n",
       "       [ 322161.53],\n",
       "       [ 915029.  ],\n",
       "       [1039483.7 ],\n",
       "       [1306622.5 ],\n",
       "       [ 272971.12],\n",
       "       [2187937.  ],\n",
       "       [2363000.5 ],\n",
       "       [ 598283.06],\n",
       "       [ 647794.2 ],\n",
       "       [ 821199.56],\n",
       "       [1447491.1 ],\n",
       "       [1337074.9 ],\n",
       "       [1224285.4 ],\n",
       "       [ 374453.  ],\n",
       "       [ 989934.4 ],\n",
       "       [1705437.8 ],\n",
       "       [ 334902.3 ],\n",
       "       [1224034.4 ],\n",
       "       [1646412.8 ],\n",
       "       [1611253.9 ],\n",
       "       [ 655322.2 ],\n",
       "       [1138242.4 ],\n",
       "       [ 900434.06],\n",
       "       [1749113.9 ],\n",
       "       [2074337.  ],\n",
       "       [ 346857.66],\n",
       "       [ 567047.7 ],\n",
       "       [ 807203.06],\n",
       "       [2518303.2 ],\n",
       "       [1589894.6 ],\n",
       "       [1797809.1 ],\n",
       "       [ 444998.3 ],\n",
       "       [1506318.5 ],\n",
       "       [1245415.1 ],\n",
       "       [ 356993.53],\n",
       "       [2638087.  ],\n",
       "       [ 579674.8 ],\n",
       "       [1063905.8 ],\n",
       "       [ 890871.25],\n",
       "       [1020001.75],\n",
       "       [ 658029.56],\n",
       "       [ 508368.94],\n",
       "       [1148987.6 ],\n",
       "       [1350042.5 ],\n",
       "       [ 436746.06],\n",
       "       [2780163.8 ],\n",
       "       [ 967651.6 ],\n",
       "       [ 523114.6 ],\n",
       "       [ 382620.4 ],\n",
       "       [ 539753.7 ],\n",
       "       [3208428.2 ],\n",
       "       [2703378.2 ],\n",
       "       [2006405.1 ],\n",
       "       [ 504848.03],\n",
       "       [1083061.  ],\n",
       "       [ 404946.34],\n",
       "       [ 955039.4 ],\n",
       "       [1852580.8 ],\n",
       "       [1691286.4 ],\n",
       "       [2192134.2 ],\n",
       "       [ 911056.94],\n",
       "       [ 548235.94],\n",
       "       [ 835344.2 ],\n",
       "       [1664478.4 ],\n",
       "       [1431558.6 ],\n",
       "       [ 560950.6 ],\n",
       "       [ 695359.  ],\n",
       "       [ 484120.5 ],\n",
       "       [ 545307.3 ],\n",
       "       [ 518082.62],\n",
       "       [ 671386.  ],\n",
       "       [2343911.5 ],\n",
       "       [ 954767.5 ],\n",
       "       [1918450.1 ],\n",
       "       [ 373986.75],\n",
       "       [1765330.9 ],\n",
       "       [1262650.  ],\n",
       "       [ 514913.94],\n",
       "       [2227984.  ],\n",
       "       [2745963.8 ],\n",
       "       [ 239170.47],\n",
       "       [ 729945.  ],\n",
       "       [ 467374.97],\n",
       "       [ 862283.6 ],\n",
       "       [ 963655.3 ],\n",
       "       [2189775.8 ],\n",
       "       [3118098.5 ],\n",
       "       [1379910.5 ],\n",
       "       [1377251.9 ],\n",
       "       [1398330.  ],\n",
       "       [2311456.  ],\n",
       "       [2855501.  ],\n",
       "       [ 442467.56],\n",
       "       [1382781.8 ],\n",
       "       [3592553.8 ],\n",
       "       [1966750.  ],\n",
       "       [ 838297.6 ],\n",
       "       [ 816778.44],\n",
       "       [ 413868.  ],\n",
       "       [ 730773.8 ],\n",
       "       [2273665.  ],\n",
       "       [1338116.2 ],\n",
       "       [1792090.9 ],\n",
       "       [1704242.9 ],\n",
       "       [2403213.5 ],\n",
       "       [1879607.5 ],\n",
       "       [ 987042.  ],\n",
       "       [ 361897.2 ],\n",
       "       [ 982500.06],\n",
       "       [1749311.4 ],\n",
       "       [ 278067.  ],\n",
       "       [2819329.8 ],\n",
       "       [1241776.  ],\n",
       "       [1732951.5 ],\n",
       "       [1168691.8 ],\n",
       "       [1143727.5 ],\n",
       "       [2237362.  ],\n",
       "       [1352454.4 ],\n",
       "       [ 489944.6 ],\n",
       "       [2056347.2 ],\n",
       "       [ 360334.9 ],\n",
       "       [1444591.1 ],\n",
       "       [1509659.8 ],\n",
       "       [ 398511.66],\n",
       "       [ 400213.75],\n",
       "       [ 903288.56],\n",
       "       [ 286950.4 ],\n",
       "       [2452542.5 ],\n",
       "       [ 765195.6 ],\n",
       "       [ 523634.22],\n",
       "       [ 592598.1 ],\n",
       "       [ 555988.9 ],\n",
       "       [ 926732.1 ],\n",
       "       [ 785960.94],\n",
       "       [ 538550.8 ],\n",
       "       [ 257751.12],\n",
       "       [1581775.5 ],\n",
       "       [ 333890.94],\n",
       "       [ 578175.3 ],\n",
       "       [ 438363.75],\n",
       "       [2387062.8 ],\n",
       "       [ 254582.12],\n",
       "       [1270224.1 ],\n",
       "       [2871010.5 ],\n",
       "       [ 492195.7 ],\n",
       "       [ 400868.16],\n",
       "       [ 504845.84],\n",
       "       [ 378276.75],\n",
       "       [ 489432.34],\n",
       "       [ 387422.38],\n",
       "       [1250924.6 ],\n",
       "       [1109692.9 ],\n",
       "       [ 681004.4 ],\n",
       "       [ 716444.25],\n",
       "       [2969016.5 ],\n",
       "       [1257827.  ],\n",
       "       [ 393434.34],\n",
       "       [ 293785.4 ],\n",
       "       [ 481605.56],\n",
       "       [ 420119.1 ],\n",
       "       [1789638.6 ],\n",
       "       [1949295.2 ],\n",
       "       [ 943050.5 ],\n",
       "       [ 250276.81],\n",
       "       [ 992036.8 ],\n",
       "       [ 521157.66],\n",
       "       [1320456.4 ],\n",
       "       [ 916494.  ],\n",
       "       [ 627038.7 ],\n",
       "       [1774167.9 ],\n",
       "       [2292194.8 ],\n",
       "       [1004743.2 ],\n",
       "       [2095411.6 ],\n",
       "       [ 428456.5 ],\n",
       "       [3036062.  ],\n",
       "       [1334962.8 ],\n",
       "       [ 502854.3 ],\n",
       "       [1176214.9 ],\n",
       "       [ 571847.5 ],\n",
       "       [ 802177.44],\n",
       "       [1652967.6 ],\n",
       "       [ 981605.56],\n",
       "       [1078696.1 ],\n",
       "       [1466789.8 ],\n",
       "       [ 448927.28],\n",
       "       [ 756111.3 ],\n",
       "       [ 458700.8 ],\n",
       "       [ 861695.8 ],\n",
       "       [1440577.1 ],\n",
       "       [ 497811.  ],\n",
       "       [1323875.1 ],\n",
       "       [1602177.8 ],\n",
       "       [2740126.  ],\n",
       "       [1677486.6 ],\n",
       "       [ 810794.44],\n",
       "       [ 409245.44],\n",
       "       [ 567589.56],\n",
       "       [1638039.4 ],\n",
       "       [1824902.4 ],\n",
       "       [2839602.  ],\n",
       "       [ 287104.44],\n",
       "       [2361668.8 ],\n",
       "       [ 932302.8 ],\n",
       "       [1237929.8 ],\n",
       "       [1646111.4 ],\n",
       "       [ 466046.66],\n",
       "       [ 865980.5 ],\n",
       "       [1918579.5 ],\n",
       "       [ 904042.75],\n",
       "       [ 667352.06],\n",
       "       [1536182.5 ],\n",
       "       [ 491022.5 ],\n",
       "       [1404234.9 ],\n",
       "       [1671453.9 ],\n",
       "       [ 517077.97],\n",
       "       [1479815.  ],\n",
       "       [ 510746.06],\n",
       "       [2251735.8 ],\n",
       "       [ 315943.53],\n",
       "       [1185464.4 ],\n",
       "       [ 870178.56],\n",
       "       [1431801.6 ],\n",
       "       [ 549695.3 ],\n",
       "       [3199625.2 ],\n",
       "       [1214248.5 ],\n",
       "       [1539608.1 ],\n",
       "       [1310085.4 ],\n",
       "       [ 539664.7 ],\n",
       "       [1824492.8 ],\n",
       "       [1632147.6 ],\n",
       "       [2231472.2 ],\n",
       "       [1352102.4 ],\n",
       "       [1250081.5 ],\n",
       "       [1249221.9 ],\n",
       "       [1010405.7 ],\n",
       "       [1256907.8 ],\n",
       "       [ 433336.34],\n",
       "       [ 716444.25],\n",
       "       [1132827.  ],\n",
       "       [1043923.8 ],\n",
       "       [ 600408.06],\n",
       "       [ 700708.94],\n",
       "       [2584357.8 ],\n",
       "       [ 715104.2 ],\n",
       "       [ 811165.06],\n",
       "       [ 968869.75],\n",
       "       [ 349022.25],\n",
       "       [1085121.1 ],\n",
       "       [3437912.2 ],\n",
       "       [2733810.5 ],\n",
       "       [1403939.1 ],\n",
       "       [1802227.4 ],\n",
       "       [ 649457.5 ],\n",
       "       [ 294510.4 ],\n",
       "       [ 693105.  ],\n",
       "       [ 913005.7 ],\n",
       "       [ 899985.44],\n",
       "       [ 664171.56],\n",
       "       [2776307.8 ],\n",
       "       [ 563957.94],\n",
       "       [ 564283.7 ],\n",
       "       [ 453406.6 ],\n",
       "       [ 486368.75],\n",
       "       [2037530.9 ],\n",
       "       [ 306574.12],\n",
       "       [1135820.2 ],\n",
       "       [ 704699.44],\n",
       "       [ 504097.75],\n",
       "       [1312422.9 ],\n",
       "       [1023030.  ],\n",
       "       [ 211348.61],\n",
       "       [1205733.9 ],\n",
       "       [1321370.2 ],\n",
       "       [ 384509.06],\n",
       "       [ 743248.6 ],\n",
       "       [1066115.6 ],\n",
       "       [ 371348.72],\n",
       "       [1227978.1 ],\n",
       "       [1329317.1 ],\n",
       "       [1720869.  ],\n",
       "       [1367876.4 ],\n",
       "       [1549183.5 ],\n",
       "       [1235564.  ],\n",
       "       [1908234.1 ],\n",
       "       [ 349811.12],\n",
       "       [ 642486.  ],\n",
       "       [ 843490.94],\n",
       "       [1821107.4 ],\n",
       "       [1736591.6 ],\n",
       "       [ 631357.8 ],\n",
       "       [1409064.4 ],\n",
       "       [ 489708.84],\n",
       "       [ 826007.3 ],\n",
       "       [1152924.5 ],\n",
       "       [ 469189.53],\n",
       "       [1570921.5 ],\n",
       "       [ 417057.22],\n",
       "       [1060405.2 ],\n",
       "       [ 476514.66],\n",
       "       [ 529688.3 ],\n",
       "       [ 368754.4 ],\n",
       "       [ 683309.25],\n",
       "       [ 867783.5 ],\n",
       "       [3439715.8 ],\n",
       "       [1666307.6 ],\n",
       "       [1416513.8 ],\n",
       "       [1256718.4 ],\n",
       "       [ 415365.28],\n",
       "       [ 848329.1 ],\n",
       "       [ 301871.78],\n",
       "       [ 679383.8 ],\n",
       "       [ 330616.66],\n",
       "       [4588770.  ],\n",
       "       [ 450953.84],\n",
       "       [ 554293.5 ],\n",
       "       [4048111.8 ],\n",
       "       [1184904.9 ],\n",
       "       [1087666.4 ],\n",
       "       [1223852.5 ],\n",
       "       [ 522566.56],\n",
       "       [ 887418.44],\n",
       "       [1368336.8 ],\n",
       "       [ 267179.25],\n",
       "       [ 850238.5 ],\n",
       "       [3638261.2 ],\n",
       "       [1350085.4 ],\n",
       "       [ 414500.9 ],\n",
       "       [1075723.8 ],\n",
       "       [1764857.2 ],\n",
       "       [ 614447.3 ],\n",
       "       [ 622673.8 ],\n",
       "       [1018146.7 ],\n",
       "       [ 653011.2 ],\n",
       "       [ 778694.7 ],\n",
       "       [2299072.8 ],\n",
       "       [3290718.2 ],\n",
       "       [1024915.94],\n",
       "       [ 268977.  ],\n",
       "       [1533312.6 ],\n",
       "       [ 749980.1 ],\n",
       "       [ 291495.25],\n",
       "       [ 462976.22],\n",
       "       [ 892638.9 ],\n",
       "       [ 915753.25],\n",
       "       [ 948769.6 ],\n",
       "       [3049196.2 ],\n",
       "       [2320822.8 ],\n",
       "       [ 455312.8 ],\n",
       "       [1253884.4 ],\n",
       "       [1603458.1 ],\n",
       "       [2939220.  ],\n",
       "       [1027900.8 ],\n",
       "       [ 808541.1 ],\n",
       "       [1513607.9 ],\n",
       "       [1142722.1 ],\n",
       "       [1607341.8 ],\n",
       "       [1800859.2 ],\n",
       "       [2947002.5 ],\n",
       "       [2297961.2 ],\n",
       "       [1871190.4 ],\n",
       "       [1378046.6 ],\n",
       "       [ 511305.25],\n",
       "       [1121770.  ],\n",
       "       [2408294.5 ],\n",
       "       [2188743.  ],\n",
       "       [1168451.6 ],\n",
       "       [ 687660.9 ],\n",
       "       [ 829644.56],\n",
       "       [ 362989.03],\n",
       "       [1307234.9 ],\n",
       "       [2268760.8 ],\n",
       "       [ 353319.1 ],\n",
       "       [2284304.5 ],\n",
       "       [1853840.  ],\n",
       "       [ 511451.53],\n",
       "       [ 656016.8 ],\n",
       "       [ 743499.25],\n",
       "       [ 692348.2 ],\n",
       "       [ 210205.98],\n",
       "       [1882980.4 ],\n",
       "       [2178015.8 ],\n",
       "       [ 719944.94],\n",
       "       [ 380882.2 ],\n",
       "       [ 411649.9 ],\n",
       "       [1767423.8 ],\n",
       "       [ 686888.5 ],\n",
       "       [1743572.6 ],\n",
       "       [ 333835.4 ],\n",
       "       [2175206.8 ],\n",
       "       [1551933.9 ],\n",
       "       [3219839.8 ],\n",
       "       [1470292.6 ],\n",
       "       [ 683026.5 ],\n",
       "       [2201160.5 ],\n",
       "       [2367024.5 ],\n",
       "       [2084127.2 ],\n",
       "       [2272982.8 ],\n",
       "       [ 537389.5 ],\n",
       "       [1547558.4 ],\n",
       "       [1006276.5 ],\n",
       "       [ 817678.5 ],\n",
       "       [1949523.  ],\n",
       "       [1610760.9 ],\n",
       "       [ 490050.72],\n",
       "       [ 589024.8 ],\n",
       "       [ 172246.67],\n",
       "       [ 655135.1 ],\n",
       "       [ 425850.25],\n",
       "       [1995525.  ],\n",
       "       [1079995.6 ],\n",
       "       [ 617769.2 ],\n",
       "       [ 831853.8 ],\n",
       "       [ 511019.16],\n",
       "       [ 790518.44],\n",
       "       [ 927327.75],\n",
       "       [1368605.9 ],\n",
       "       [ 723526.75],\n",
       "       [1144133.9 ],\n",
       "       [2256675.2 ],\n",
       "       [ 768349.25],\n",
       "       [2082002.5 ],\n",
       "       [1485300.  ],\n",
       "       [1343617.2 ],\n",
       "       [ 436259.88],\n",
       "       [ 465826.88],\n",
       "       [ 801992.6 ],\n",
       "       [ 368933.28],\n",
       "       [1587185.9 ],\n",
       "       [ 385795.38],\n",
       "       [1478915.6 ],\n",
       "       [1522484.9 ],\n",
       "       [ 456838.06],\n",
       "       [ 515645.84],\n",
       "       [ 464780.22],\n",
       "       [ 308967.47],\n",
       "       [2910326.2 ],\n",
       "       [2477623.  ],\n",
       "       [1198534.  ],\n",
       "       [ 395652.7 ],\n",
       "       [ 415735.28],\n",
       "       [2889151.  ],\n",
       "       [1782176.9 ],\n",
       "       [2220510.2 ],\n",
       "       [ 345867.4 ],\n",
       "       [1370460.2 ],\n",
       "       [ 635442.5 ],\n",
       "       [ 905428.06],\n",
       "       [1731814.  ],\n",
       "       [1196966.4 ],\n",
       "       [ 402835.44],\n",
       "       [1232107.1 ],\n",
       "       [1363177.6 ],\n",
       "       [1056794.1 ],\n",
       "       [1537752.4 ],\n",
       "       [ 228855.61],\n",
       "       [3060731.  ],\n",
       "       [ 372871.  ],\n",
       "       [2378590.8 ],\n",
       "       [2443763.5 ],\n",
       "       [ 534011.3 ],\n",
       "       [1702505.2 ],\n",
       "       [1710702.1 ],\n",
       "       [ 879688.94],\n",
       "       [ 539473.4 ],\n",
       "       [1046757.25],\n",
       "       [1969879.6 ],\n",
       "       [ 637564.9 ],\n",
       "       [ 454271.9 ],\n",
       "       [2576053.2 ],\n",
       "       [2177282.5 ],\n",
       "       [1382482.2 ],\n",
       "       [ 711288.3 ],\n",
       "       [ 269903.47],\n",
       "       [ 556489.1 ],\n",
       "       [1778641.9 ],\n",
       "       [1487694.6 ],\n",
       "       [3410509.  ],\n",
       "       [ 297966.25],\n",
       "       [2949184.  ],\n",
       "       [ 641052.06],\n",
       "       [ 576487.7 ],\n",
       "       [2078185.6 ],\n",
       "       [1581073.2 ],\n",
       "       [1882110.5 ],\n",
       "       [ 801972.  ],\n",
       "       [2313193.8 ],\n",
       "       [2046636.5 ],\n",
       "       [ 808545.2 ],\n",
       "       [ 258253.55],\n",
       "       [ 755051.6 ],\n",
       "       [1303575.9 ],\n",
       "       [ 451270.12],\n",
       "       [2392824.5 ],\n",
       "       [ 660799.2 ],\n",
       "       [ 949558.56],\n",
       "       [2385655.5 ],\n",
       "       [ 815426.6 ],\n",
       "       [1768970.8 ],\n",
       "       [1736851.2 ],\n",
       "       [2010369.4 ],\n",
       "       [ 892696.25],\n",
       "       [2877977.5 ],\n",
       "       [ 592686.25],\n",
       "       [1142032.  ],\n",
       "       [1138806.6 ],\n",
       "       [1366272.9 ],\n",
       "       [4304880.  ],\n",
       "       [1229837.4 ],\n",
       "       [1103542.5 ],\n",
       "       [2164528.  ],\n",
       "       [1139321.4 ],\n",
       "       [ 502869.75],\n",
       "       [2391547.  ],\n",
       "       [2428848.  ],\n",
       "       [1541087.2 ],\n",
       "       [ 888914.94],\n",
       "       [ 769965.3 ],\n",
       "       [2567685.  ],\n",
       "       [ 660440.4 ],\n",
       "       [ 524740.7 ],\n",
       "       [1729542.8 ],\n",
       "       [ 278457.2 ],\n",
       "       [2014316.4 ],\n",
       "       [2157225.  ],\n",
       "       [ 575101.3 ],\n",
       "       [1730161.1 ],\n",
       "       [ 241082.05],\n",
       "       [2190461.5 ],\n",
       "       [1195733.1 ],\n",
       "       [ 889015.8 ],\n",
       "       [1324937.9 ],\n",
       "       [ 533194.6 ],\n",
       "       [ 935754.8 ],\n",
       "       [ 551213.25],\n",
       "       [1247435.  ],\n",
       "       [2527737.5 ],\n",
       "       [ 291569.03],\n",
       "       [ 722837.2 ],\n",
       "       [ 792512.4 ],\n",
       "       [ 358646.97],\n",
       "       [ 368933.28],\n",
       "       [ 974789.44],\n",
       "       [ 970595.06],\n",
       "       [1741837.8 ],\n",
       "       [ 890663.56],\n",
       "       [1251125.6 ],\n",
       "       [1645696.  ],\n",
       "       [ 412770.78],\n",
       "       [ 692143.1 ],\n",
       "       [1115027.1 ],\n",
       "       [2074662.1 ],\n",
       "       [2910290.2 ],\n",
       "       [1979875.5 ],\n",
       "       [3053221.2 ],\n",
       "       [ 426657.7 ],\n",
       "       [ 413576.  ],\n",
       "       [ 733669.06],\n",
       "       [1279071.8 ],\n",
       "       [ 261138.05],\n",
       "       [4169706.8 ],\n",
       "       [ 716514.3 ],\n",
       "       [2485046.2 ],\n",
       "       [ 236055.67],\n",
       "       [1367518.6 ],\n",
       "       [2475507.  ],\n",
       "       [ 621002.5 ],\n",
       "       [ 470648.78],\n",
       "       [ 300953.25],\n",
       "       [1700152.8 ],\n",
       "       [1392705.8 ],\n",
       "       [1163226.2 ],\n",
       "       [ 384053.03],\n",
       "       [1983062.1 ],\n",
       "       [ 384922.3 ],\n",
       "       [1604520.1 ],\n",
       "       [ 595393.2 ],\n",
       "       [ 685332.25],\n",
       "       [ 700460.94],\n",
       "       [1997047.8 ],\n",
       "       [ 289785.06],\n",
       "       [1879670.8 ],\n",
       "       [ 277859.38],\n",
       "       [ 612429.8 ],\n",
       "       [ 653101.  ],\n",
       "       [1988587.6 ],\n",
       "       [ 668252.1 ],\n",
       "       [ 872508.8 ],\n",
       "       [1961778.4 ],\n",
       "       [ 436613.72],\n",
       "       [1115826.5 ],\n",
       "       [ 948293.2 ],\n",
       "       [ 518070.25],\n",
       "       [1035270.  ],\n",
       "       [1639330.1 ],\n",
       "       [ 489524.88],\n",
       "       [ 534492.44],\n",
       "       [ 392272.12],\n",
       "       [ 645601.8 ],\n",
       "       [ 896644.44],\n",
       "       [ 866530.  ],\n",
       "       [ 273190.03],\n",
       "       [1177324.9 ],\n",
       "       [1262650.  ],\n",
       "       [1435760.6 ],\n",
       "       [ 318357.7 ],\n",
       "       [ 615931.4 ],\n",
       "       [1709048.9 ],\n",
       "       [1102493.5 ],\n",
       "       [3476109.  ],\n",
       "       [ 672971.9 ],\n",
       "       [ 404526.66],\n",
       "       [2192659.2 ],\n",
       "       [ 951095.4 ],\n",
       "       [2214146.  ],\n",
       "       [ 311148.53],\n",
       "       [ 394295.94],\n",
       "       [ 898895.9 ],\n",
       "       [1436883.8 ],\n",
       "       [ 593142.94],\n",
       "       [2105529.  ],\n",
       "       [ 769174.8 ],\n",
       "       [ 454899.62],\n",
       "       [2542760.5 ],\n",
       "       [1337585.9 ],\n",
       "       [ 918788.  ],\n",
       "       [1974612.9 ],\n",
       "       [1792876.8 ],\n",
       "       [ 349012.34],\n",
       "       [1336012.1 ],\n",
       "       [1357515.2 ],\n",
       "       [3711321.5 ],\n",
       "       [ 349022.25],\n",
       "       [ 373587.28],\n",
       "       [ 417076.22],\n",
       "       [ 781153.7 ],\n",
       "       [1242634.4 ],\n",
       "       [1569638.2 ],\n",
       "       [1784398.8 ],\n",
       "       [ 372757.2 ],\n",
       "       [ 349321.2 ],\n",
       "       [ 395967.1 ],\n",
       "       [2071940.4 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -321112.25   ,   208887.75   ,  -991112.25   , ...,\n",
       "        -1011112.25   , -1081112.25   ,  -711112.25   ],\n",
       "       [ -375055.625  ,   154944.375  , -1045055.625  , ...,\n",
       "        -1065055.625  , -1135055.625  ,  -765055.625  ],\n",
       "       [  586148.1875 ,  1116148.1875 ,   -83851.8125 , ...,\n",
       "         -103851.8125 ,  -173851.8125 ,   196148.1875 ],\n",
       "       ...,\n",
       "       [  800678.8125 ,  1330678.8125 ,   130678.8125 , ...,\n",
       "          110678.8125 ,    40678.8125 ,   410678.8125 ],\n",
       "       [  754032.90625,  1284032.90625,    84032.90625, ...,\n",
       "           64032.90625,    -5967.09375,   364032.90625],\n",
       "       [ -921940.375  ,  -391940.375  , -1591940.375  , ...,\n",
       "        -1611940.375  , -1681940.375  , -1311940.375  ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train_y - prediction\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413828.2098823635"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(prediction, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = tuned_model.to_json()\n",
    "with open(\"tuned_model_small_provinces.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "tuned_model.save_weights(\"model_small_provinces.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
